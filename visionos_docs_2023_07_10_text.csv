id,text
0,"Creating your first visionOS app  Overview See Also       Create your Xcode project Modify the existing window Handle events in your views Build and run your app App construction                        If you’re new to visionOS, start with a new Xcode project to learn about the platform features, and to familiarize yourself with visionOS content and techniques. When you build an app for visionOS, SwiftUI is an excellent choice because it gives you full access to visionOS features. Although you can also use UIKit to build portions of your app, you need to use SwiftUI for many features that are unique to the platform. In any SwiftUI app, you place content onscreen using scenes. A scene contains the views and controls to display onscreen. Scenes also define the appearance of those views and controls when they appear onscreen. In visionOS, you can include both 2D and 3D views in the same scene, and you can present those views in a window or as part of the person’s surroundings. Scene with a window Scene with a window and 3D objects Start with a new Xcode project and add features to familiarize yourself with visionOS content and techniques. Run your app in Simulator to verify your content looks like you expect, and run it on device to see your 3D content come to life. Organize your content around one or more scenes, which manage your app’s interface. Each scene contains the views and controls you want to display, and the scene type determines whether your content adopts a 2D or 3D appearance. SwiftUI adds 3D scene types specifically for visionOS, and also adds 3D elements and layout options for all scene types. Create a new project in Xcode by choosing File > New > Project. Navigate to the visionOS section of the template chooser, and choose the App template. When prompted, specify a name for your project along with other options. When creating a new visionOS app, you can configure your app’s initial scene types from the configuration dialog. To display primarily 2D content in your initial scene, choose a Window as your initial scene type. For primarily 3D content, choose a Volume. You can also add an immersive scene to place your content in the person’s surroundings.  Include a Reality Composer Pro project file when you want to create 3D assets or scenes to display from your app. Use this project file to build content from primitive shapes and existing USDZ assets. You can also use it to build and test custom RealityKit animations and behaviors for your content. Build your initial interface using standard SwiftUI views. Views provide the basic content for your interface, and you customize the appearance and behavior of them using SwiftUI modifiers. For example, the .background modifier adds a partially transparent tint color behind your content: To learn more about how to create and configure interfaces using SwiftUI, see SwiftUI Essentials. Many SwiftUI views handle interactions automatically — all you do is provide code to run when the interactions occur. You can also add SwiftUI gesture recognizers to a view to handle tap, long-press, drag, rotate, and zoom gestures. The system automatically maps the following types of input to your SwiftUI event-handling code:  Indirect input. The person’s eyes indicate the target of an interaction. To start the interaction, the person touches their thumb and forefinger together on one or both hands. Additional finger and hand movements define the gesture type.  Direct input. When a person’s finger occupies the same space as an onscreen item, the system reports an interaction. Additional finger and hand movements define the gesture type.  Keyboard input. People can use a connected mouse, trackpad, or keyboard to interact with items, trigger menu commands, and perform gestures. For more information about handling interactions in SwiftUI views, see Handling User Input in the SwiftUI Essentials tutorial. Build and run your app in Simulator to see how it looks. Simulator for visionOS has a virtual background as the backdrop for your app’s content. Use your keyboard and your mouse or trackpad to navigate around the environment and interact with your app. Tap and drag the window bar below your app’s content to reposition the window in the environment. Move the pointer over the circle next to the window bar to reveal the window’s close button. Move the cursor to one of the window’s corners to turn the window bar into a resizing control. Note Apps don’t control the placement of windows in the space. The system places each window in its initial position, and updates that position based on further interactions with the app. For additional information about how to interact with your app in Simulator, see Interacting with your app in the visionOS simulator.       .background @main
struct MyApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
               .background(.black.opacity(0.8))
        }


        ImmersiveSpace(id: ""Immersive"") {
            ImmersiveView()
        }
    }
} "
1,"Adding 3D content to your app  Overview See Also       Add depth to traditional 2D windows Display dynamic 3D scenes using RealityKit Respond to interactions with RealityKit content Display 3D content in a volume Display 3D content in a person’s surroundings App construction                        A device with a stereoscopic display lets people experience 3D content in a way that feels more real. Content appears to have real depth, and people can view it from different angles, making it seem like it’s there in front of them. When building an app for visionOS, think about ways you might add depth to your app’s interface. The system provides several ways to display 3D content, including in your existing windows, in a volume, and in an immersive space. Choose the options that work best for your app and the content you offer. Window Volume Immersive space Windows are an important part of your app’s interface. With visionOS, apps automatically get materials with the visionOS look and feel, fully resizable windows with spacing tuned for eyes and hands input, and access to highlighting adjustments for your custom controls. Incorporate depth effects into your custom views as needed, and use 3D layout options to arrange views in your windows. Apply a shadow(color:radius:x:y:) or visualEffect(_:) modifier to the view. Lift or highlight the view when someone looks at it using a hoverEffect(_:isEnabled:) modifier. Lay out views using a ZStack. Animate view-related changes with doc://com.apple.documentation/documentation/swiftui/view/transform3deffect(_:). Rotate the view using a rotation3DEffect(_:axis:anchor:anchorZ:perspective:) modifier. In addition to giving 2D views more depth, you can also add static 3D models to your 2D windows. The Model3D view loads a USDZ file or other asset type and displays it at its intrinsic size in your window. Use this in places where you already have the model data in your app, or can download it from the network. For example, a shopping app might use this type of view to display a 3D version of a product. RealityKit is Apple’s technology for building 3D models and scenes that you update dynamically onscreen. In visionOS, use RealityKit and SwiftUI together to seamlessly couple your app’s 2D and 3D content. Load existing USDZ assets or create scenes in Reality Composer Pro that incorporate animation, physics, lighting, sounds, and custom behaviors for your content. To use a Reality Composer Pro project in your app, add the Swift package to your Xcode project and import its module in your Swift file. For for information, see Managing files and folders in your Xcode project.  When you’re ready to display 3D content in your interface, use a RealityView. This SwiftUI view serves as a container for your RealityKit content, and lets you update that content using familiar SwiftUI techniques. The following example shows a view that uses a RealityView to display a 3D sphere. The code in the view’s closure creates a RealityKit entity for the sphere, applies a texture to the surface of the sphere, and adds the sphere to the view’s content. When SwiftUI displays your RealityView, it executes your code once to create the entities and other content. Because creating entities is relatively expensive, the view runs your creation code only once. When you want to update the state of your entities, change the state of your view and use an update closure to apply those changes to your content. The following example uses an update closure to change the size of the sphere when the value in the scale property changes: For information about how to create content using RealityKit, see RealityKit. To handle interactions with the entities of your RealityKit scenes: Attach a gesture recognizer to your RealityView and add the doc://com.apple.documentation/documentation/swiftui/gesture/targetedtoanyentity() modifier to it. Attach an InputTargetComponent to the entity or one of its parent entities. Add collision shapes to the RealityKit entities that support interactions. The doc://com.apple.documentation/documentation/swiftui/gesture/targetedtoanyentity() modifier provides a bridge between the gesture recognizer and your RealityKit content. For example, to recognize when someone drags an entity, specify a DragGesture and add the modifier to it. When the specified gesture occurs on an entity, SwiftUI executes the provided closure. The following example adds a tap gesture recognizer to the sphere view from the previous example. The code also adds InputTargetComponent and CollisionComponent components to the shape to allow the interactions to occur. If you omit these components, the view doesn’t detect the interactions with your entity. A volume is a type of window that grows in three dimensions to match the size of the content it contains. Windows and volumes both accommodate 2D and 3D content, and are alike in many ways. However, windows clip 3D content that extends too far from the window’s surface, so volumes are the better choice for content that is primarily 3D. To create a volume, add a WindowGroup scene to your app and set its style to doc://com.apple.documentation/documentation/swiftui/windowstyle/volumetric. This style tells SwiftUI to create a window for 3D content. Include any 2D or 3D views you want in your volume. You can also add a RealityView to build your content using RealityKit. The following example creates a volume with a static 3D model of some balloons stored in the app’s bundle: Windows and volumes are a convenient way to display bounded 2D and 3D content, but your app doesn’t control the placement of that content in the person’s surroundings. The system sets the initial position of each window and volume at display time. The system also adds a window bar to allow someone to reposition the window or resize it.  For more information about when to use volumes, see Human Interface Guidelines > Windows. When you need more control over the placement of your app’s content, add that content to an doc://com.apple.documentation/documentation/swiftui/immersivespace. An immersive space offers an unbounded area for your content, and you control the size and placement of content within the space. After receiving permission from the user, you can also use ARKit with an immersive space to integrate content into their surroundings. For example, you can use ARKit scene reconstruction to obtain a mesh of furniture and nearby objects and have your content interact with that mesh. An doc://com.apple.documentation/documentation/swiftui/immersivespace is a scene type that you create alongside your app’s other scenes. The following example shows an app that contains an immersive space and a window: If you don’t add a style modifier to your doc://com.apple.documentation/documentation/swiftui/immersivespace declaration, the system creates that space using the doc://com.apple.documentation/documentation/swiftui/immersionstyle/mixed style. This style displays your content together with the passthrough content that shows the person’s surroundings. Other styles let you hide passthrough to varying degrees. Use the doc://com.apple.documentation/documentation/swiftui/scene/immersionstyle(selection:in:) modifier to specify which styles your space supports. If you specify more than one style, you can toggle between the styles using the selection parameter of the modifier. Warning Be mindful of how much content you include in immersive scenes that use the doc://com.apple.documentation/documentation/swiftui/immersionstyle/mixed style. Content that fills a significant portion of the screen, even if that content is partially transparent, can prevent the person from seeing potential hazards in their surroundings. If you want to immerse the person in your content, configure your space with the doc://com.apple.documentation/documentation/swiftui/immersionstyle/full style. For more information, see, Creating fully immersive experiences in your app. Remember to set the position of items you place in an doc://com.apple.documentation/documentation/swiftui/immersivespace. Position SwiftUI views using modifiers, and position a RealityKit entity using its transform component. SwiftUI places the origin of a space at a person’s feet initially, but can change this origin in response to other events. For example, the system might shift the origin to accommodate a SharePlay activity that displays your content with Spatial Personas. If you need to position SwiftUI views and RealityKit entities relative to one another, perform any needed coordinate conversions using the methods in the content parameter of RealityView. To display your doc://com.apple.documentation/documentation/swiftui/immersivespace scene, open it using the doc://com.apple.documentation/documentation/swiftui/environmentvalues/openimmersivespace action, which you obtain from the SwiftUI environment. This action runs asynchronously and uses the provided information to find and initialize your scene. The following example shows a button that opens the space with the solarSystem identifier: When an app presents an doc://com.apple.documentation/documentation/swiftui/immersivespace, the system hides the content of other apps to prevent visual conflicts. The other apps remain hidden while your space is visible but return when you dismiss it. If your app defines multiple spaces, you must dismiss the currently visible space before displaying a different space. If you don’t dismiss the visible space, the system issues a runtime warning when you try to open the other space.       Apply a shadow(color:radius:x:y:) or visualEffect(_:) modifier to the view.
Lift or highlight the view when someone looks at it using a hoverEffect(_:isEnabled:) modifier.
Lay out views using a ZStack.
Animate view-related changes with doc://com.apple.documentation/documentation/swiftui/view/transform3deffect(_:).
Rotate the view using a rotation3DEffect(_:axis:anchor:anchorZ:perspective:) modifier. Attach a gesture recognizer to your RealityView and add the doc://com.apple.documentation/documentation/swiftui/gesture/targetedtoanyentity() modifier to it.
Attach an InputTargetComponent to the entity or one of its parent entities.
Add collision shapes to the RealityKit entities that support interactions. shadow(color:radius:x:y:) visualEffect(_:) hoverEffect(_:isEnabled:) ZStack rotation3DEffect(_:axis:anchor:anchorZ:perspective:) Model3D RealityView RealityView  struct SphereView: View {
    var body: some View {
        RealityView { content in
            let model = ModelEntity(
                         mesh: .generateSphere(radius: 0.1),
                         materials: [SimpleMaterial(color: .white, isMetallic: true)])
            content.add(model)
        }
    }
} RealityView scale struct SphereView: View {


    @State var scale = false


    var body: some View {
        RealityView { content in
            let model = ModelEntity(
                         mesh: .generateSphere(radius: 0.1),
                         materials: [SimpleMaterial(color: .white, isMetallic: true)])
            content.add(model)
        } update: { content in
            if let model = content.entities.first {
                model.transform.scale = scale ? [1.2, 1.2, 1.2] : [1.0, 1.0, 1.0]
            }
        }


    }
} RealityView InputTargetComponent DragGesture InputTargetComponent CollisionComponent struct SphereView: View {


    @State var scale = false


    var body: some View {
        RealityView { content in
            let model = ModelEntity(
                mesh: .generateSphere(radius: 0.1),
                materials: [SimpleMaterial(color: .white, isMetallic: true)])


            // Enable interactions on the entity.
            model.components.set(InputTargetComponent())
            model.components.set(CollisionComponent(shapes: [.generateSphere(radius: 0.1)]))
            content.add(model)
        } update: { content in
            if let model = content.entities.first {
                model.transform.scale = scale ? [1.2, 1.2, 1.2] : [1.0, 1.0, 1.0]
            }
        }
        .gesture(TapGesture().onEnded.targetedToAnyEntity() { _ in
            scale.toggle()
        })
    }
} WindowGroup RealityView struct MyApp: App {
    var body: some Scene {
        WindowGroup {
            Model3D(""balloons"")
        }.windowStyle(style: .volumetric)
    }
} @main
struct MyImmersiveApp: App {
    var body: some Scene {
        WindowGroup() {
            ContentView()
        }


        ImmersiveSpace(id: ""solarSystem"") {
            SolarSystemView()
        }
    }
} selection content RealityView solarSystem Button(""Show Solar System"") {
    Task {
        let result = await openImmersiveSpace(id: ""solarSystem"")
        if case .error = result {
            print(""An error occurred"")
        }
    }
} "
2,"Creating fully immersive experiences in your app  Overview See Also       Prepare someone for your app’s transitions Open an immersive space Draw your content using RealityKit Draw your content using Metal App construction                        A fully immersive experience replaces everything the person sees with custom content you create. You might use this type of experience to: Offer a temporary transitional experience Create a distraction-free space for your content Implement a virtual reality (VR) game Present a virtual world to explore With a fully immersive experience, you’re responsible for everything that appears onscreen. The system hides passthrough video and displays the content you provide, showing the person’s hands only when they come into view. To achieve the best performance, use RealityKit or Metal to create and animate your content. Typically, you combine a fully immersive experience with other types of experiences and provide transitions between them. When you display a window first and then offer controls to enter your immersive experience, you give people time to prepare for the transition. It also gives them the option to skip the experience if they prefer to use your app’s windows instead. Give people control over when they enter or exit fully immersive experiences, and provide clear transitions to and from those experiences. Clear visual transitions make it easier to adjust to such a large change. Sudden transitions might be disorienting, unpleasant, or make the person think something went wrong. At launch time, display windows or other content that allows the person to see their surroundings. Add controls to that content to initiate the transition to the fully immersive experience, and provide a clear indication of what the controls do. Inside your experience, provide clear controls and instructions on how to exit the experience. Warning When you start a fully immersive experience, visionOS defines a system boundary that extends 1.5 meters from the initial position of the person’s head. If their head moves outside of that zone, the system automatically stops the immersive experience and turns on the external video again. This feature is an assistant to help prevent someone from colliding with objects. For guidelines on how to design fully immersive experiences, see Human Interface Guidelines. To create a fully immersive experience, open an doc://com.apple.documentation/documentation/swiftui/immersivespace and set its style to doc://com.apple.documentation/documentation/swiftui/immersionstyle/full. An immersive space is a type of SwiftUI scene that lets you place content anywhere in the person’s surroundings. Applying the doc://com.apple.documentation/documentation/swiftui/immersionstyle/full style to the scene tells the system to hide passthrough video and display only your app’s content. Declare spaces in the body property of your app object, or anywhere you manage SwiftUI scenes. The following example shows an app with a main window and a fully immersive space. At launch time, the app displays the window. To display an doc://com.apple.documentation/documentation/swiftui/immersivespace, open it using the doc://com.apple.documentation/documentation/swiftui/environmentvalues/openimmersivespace action, which you obtain from the SwiftUI environment. This action runs asynchronously and uses the provided information to find and initialize your scene. The following example shows a button that opens the space with the solarSystem identifier: An app can display only one space at a time, and it’s an error for you to try to open a space while another space is visible. To dismiss an open space, use the doc://com.apple.documentation/documentation/swiftui/environmentvalues/dismissimmersivespace action. For more information about displaying spaces, see the doc://com.apple.documentation/documentation/swiftui/immersivespace type. RealityKit works well when your content consists of primitive shapes or existing content in USD files. Organize the contents of your scene using RealityKit entities, and animate that content using components and systems. Use Reality Composer Pro to assemble your content visually, and to attach dynamic shaders, animations, audio, and other behaviors to your content. Display the contents of your RealityKit scene in a RealityView in your scene. To load a Reality Composer Pro scene at runtime, fetch the URL of your Reality Composer Pro package file, and load the root entity of your scene. The following example shows how to create the entity for a package located in the app’s bundle: For more information about how to display content in a RealityView and manage interactions with your content, see Adding 3D content to your app. Another option for creating fully immersive scenes is to draw everything yourself using Metal. When using Metal to draw your content, use the Compositor Services framework to place that content onscreen. Compositor Services provides the code you need to set up your Metal rendering engine and start drawing. For details on how to render content using Metal and Compositor Services, and manage interactions with your content, see Drawing fully immersive content using Metal.       Offer a temporary transitional experience
Create a distraction-free space for your content
Implement a virtual reality (VR) game
Present a virtual world to explore body @main
struct MyImmersiveApp: App {


    var body: some Scene {
        WindowGroup() {
            ContentView()
        }


        // Display a fully immersive space.
        ImmersiveSpace(id: ""solarSystem"") {
            SolarSystemView()
        }.immersionStyle([.full])
    }
} solarSystem Button(""Show Solar System"") {
    Task {
        let result = await openImmersiveSpace(id: ""solarSystem"")
        if case .error = result {
            print(""An error occurred"")
        }
    }
} RealityView import MyRealityBundle


let url = MyRealityBundle.bundle.url(forResource:
         ""MyRealityBundle"", withExtension: ""reality"")
let scene = try await Entity(contentsOf: url) RealityView "
3,"Drawing sharp layer-based content in visionOS  Overview See Also       Request dynamic scaling for custom layers Draw the layer’s content dynamically Modify layer hierarchies to improve performance App construction                        If your app uses Core Animation layers directly, update your layer code to draw a high-resolution version of your content when appropriate. SwiftUI and UIKit views use Core Animation layers to manage interface content efficiently. When a view draws its content, the underlying layer captures that content and caches it to improve subsequent render operations. Core Animation on most Apple platforms rasterizes your layer at the same resolution as the screen, but Core Animation on visionOS can rasterize at different resolutions to maximize both content clarity and performance. The system follows the person’s eyes and renders content immediately in front of them at the highest possible resolution. Outside of this focal area, the system renders content at progressively lower resolutions to reduce GPU workloads. Because the content is in the person’s peripheral vision, these lower resolutions don’t impact the content’s clarity. As the person’s eyes move around, the system redraws content at different resolutions to match the change in focus. A figure at 2x resolution A figure at 8x resolution If you deliver content using custom CALayer objects, you can configure your custom layers to support drawing at different resolutions. If you don’t perform this extra configuration step, each layer rasterizes its content at a @2x scale factor, which is good enough for most content and matches what the layer provides on a Retina display. However, if you opt in to drawing at different resolutions, the layer rasterizes its content at up to @8x scale factor in visionOS, which adds significant detail to text and vector-based content. Dynamic content scaling is off by default for all Core Animation layers, and frameworks or apps must turn on this support explicitly. If your interface uses only SwiftUI or UIKit views, you don’t need to do anything to support this feature. SwiftUI and UIKit enable it automatically for views that benefit from the added detail, such as text views and image views with SF Symbols or other vector-based artwork. However, the frameworks don’t enable the feature for all views, including UIView and View. If your visionOS interface includes custom Core Animation layers, you can enable the wantsDynamicContentScaling property of any CALayer objects that contain vector-based content. Setting this property to true tells the system that you support rendering your layer’s content at different resolutions. However, the setting is not a guarantee that the system applies dynamic content scaling to your content. The system can disable the feature if your layer draws using incompatible functions or techniques. The following example shows how to enable this feature for a CATextLayer object. After configuring the layer, set the wantsDynamicContentScaling property to true and add the layer to your layer hierarchy. Dynamic content scaling works best when the layer contains text or vector-based content. Don’t enable the feature if you do any of the following in your layer: You set the layer’s content using the contents property. You draw primarily bitmap-based content. You redraw your layer’s contents repeatedly over a short time period. The CAShapeLayer class ignores the value of the wantsDynamicContentScaling property and always enables dynamic content scaling. For other Core Animation layers, you must enable the feature explicitly to take advantage of it. Dynamic content scaling requires you to draw your layer’s contents using one of the prescribed methods. If you define a custom subclass of CALayer, draw your layer’s content in the draw(in:) method. If you use a CALayerDelegate object to draw the layer’s content, use the delgate’s drawLayer:inContext: method instead. When you enable dynamic content scaling for a layer, the system captures your app’s drawing commands for playback later. As the person’s eyes move, the system draws the layer at higher resolutions when someone looks directly at it, or at lower resolutions otherwise. Because the redraw operations implicitly communicate what the person is looking at, the system performs them outside of your app’s process. Letting the system handle these operations maintains the person’s privacy while still giving your app the benefits of high-resolution drawing. Some Core Graphics routines are incompatible with dynamic content scaling. Even if you enable dynamic content scaling for your layer, the system automatically disables the feature if your layer uses any of the following: Core Graphics shaders. APIs that set intent, quality, or other bitmap-related properties. For example, don’t call CGContextSetInterpolationQuality. A CGBitmapContext to draw content. If your app creates timer-based animations, don’t animate layer changes using your drawing method. Calling setNeedsDisplay() on your layer repeatedly in a short time causes the system to draw the layer multiple times in quick succession. Because visionOS needs a little extra time to draw a layer at high resolution, each redraw request forces it to throw away work. A better option is to animate layer-based properties to achieve the same effect, or use a CAShapeLayer to animate paths when needed. The backing store for a layer consumes more memory at higher resolutions than at lower resolutions. Measure your app’s memory usage before and after you enable dynamic content scaling to make sure the increased memory cost is worth the benefit. If your app’s memory usage increases too much, limit which layers adopt dynamic content scaling. You can also reduce the amount of memory each layer uses in the following ways: Make your layer the smallest size possible. Larger layers require significantly more memory, especially at higher resolutions. Make the size of the layer match the size of your content by eliminating padding or extra space. Separate complex content into different layers. Instead of drawing everything in a single layer, build your content from multiple layers and arrange them hierarchically to achieve the same result. Enable dynamic content scaling only in the layers that actually need it. Apply special effects using layer properties whenever possible. Applying effects during drawing might require you to increase the layer’s size. For example, apply scale and rotation effects to the layer’s transform property, instead of during drawing. Don’t draw your layer’s content at different resolutions in advance and cache the images. Maintaining multiple images requires more memory. If you do cache images, draw them only at @2x scale factor. Don’t use your drawing code to draw a single image. If your layer’s content consists of an image, assign that image to the layer’s contents property directly. Complex drawing code can also lead to performance issues. A layer with many strokes can render quickly at lower scale factors, but might be computationally too complex to render at larger scales. If a complex layer doesn’t render correctly at higher resolutions, turn off dynamic content scaling and measure the render times again.       You set the layer’s content using the contents property.
You draw primarily bitmap-based content.
You redraw your layer’s contents repeatedly over a short time period. Core Graphics shaders.
APIs that set intent, quality, or other bitmap-related properties. For example, don’t call CGContextSetInterpolationQuality.
A CGBitmapContext to draw content. Make your layer the smallest size possible. Larger layers require significantly more memory, especially at higher resolutions. Make the size of the layer match the size of your content by eliminating padding or extra space.
Separate complex content into different layers. Instead of drawing everything in a single layer, build your content from multiple layers and arrange them hierarchically to achieve the same result. Enable dynamic content scaling only in the layers that actually need it.
Apply special effects using layer properties whenever possible. Applying effects during drawing might require you to increase the layer’s size. For example, apply scale and rotation effects to the layer’s transform property, instead of during drawing.
Don’t draw your layer’s content at different resolutions in advance and cache the images. Maintaining multiple images requires more memory. If you do cache images, draw them only at @2x scale factor.
Don’t use your drawing code to draw a single image. If your layer’s content consists of an image, assign that image to the layer’s contents property directly. CALayer UIView View wantsDynamicContentScaling CALayer true CATextLayer wantsDynamicContentScaling true     let layer = CATextLayer()
    
    layer.string = ""Hello, World!""
    layer.foregroundColor = UIColor.black.cgColor
    layer.frame = parentLayer.bounds
    
    // Setting this property to true enables content scaling 
    // and calls setNeedsDisplay to redraw the layer's content.
    layer.wantsDynamicContentScaling = true
    
    parentLayer.addSublayer(layer) contents CAShapeLayer wantsDynamicContentScaling CALayer draw(in:) CALayerDelegate drawLayer:inContext: CGContextSetInterpolationQuality setNeedsDisplay() CAShapeLayer transform contents "
4,"Designing for visionOS  Best practices Resources Change log  Related Developer documentation Videos         June 21, 2023  As you begin designing your app or game for visionOS, start by understanding the fundamental device characteristics and patterns that distinguish the platform. Use these characteristics and patterns to inform your design decisions and help you create immersive and engaging experiences. Space. Apple Vision Pro offers a limitless canvas where people can view virtual content like windows, volumes, and 3D objects, and choose to enter deeply immersive experiences that can transport them to different places. Immersion. In a visionOS app, people can fluidly transition between different levels of immersion. By default, an app launches in the Shared Space where multiple apps can run side-by-side and people can open, close, and relocate windows. People can also choose to transition an app to a Full Space, where it’s the only app running. While in a Full Space app, people can view 3D content blended with their surroundings, open a portal to view another place, or enter a different world. Passthrough. Passthrough provides live video from the device’s external cameras, and helps people interact with virtual content while also seeing their actual surroundings. When people want to see more or less of their surroundings, they use the Digital Crown to control the amount of passthrough. Spatial Audio. Vision Pro combines acoustic and visual-sensing technologies to model the sonic characteristics of a person’s surroundings, automatically making audio sound natural in their space. When an app receives a person’s permission to access information about their surroundings, it can fine-tune Spatial Audio to bring custom experiences to life. Focus and gestures. In general, people interact with Vision Pro using their eyes and hands. People perform most actions by looking at a virtual object to bring focus to it and making an indirect gesture, like a tap, to activate it. People can also use a direct gesture to interact with a virtual object by touching it with a finger. Ergonomics. While wearing Vision Pro, people rely entirely on the device’s cameras for everything they see, both real and virtual, so maintaining visual comfort is paramount. The system helps maintain comfort by automatically placing content so it’s relative to the wearer’s head, regardless of the person’s height or whether they’re sitting, standing, or lying down. Because visionOS brings content to people — instead of making people move to reach the content — people can remain at rest while engaging with apps and games. Accessibility. Apple Vision Pro supports accessibility technologies like VoiceOver, Switch Control, Dwell Control, Guided Access, Head Pointer, and many more, so people can use the interactions that work for them. In visionOS, as in all platforms, system-provided UI components build in accessibility support by default, while system frameworks give you ways to enhance the accessibility of your app or game. Great visionOS apps and games are approachable and familiar, while offering extraordinary experiences that can surround people with beautiful content, expanded capabilities, and captivating adventures. Embrace the unique features of Apple Vision Pro. Take advantage of space, Spatial Audio, and immersion to bring life to your experiences, while integrating passthrough, focus, and gestures in ways that feel at home on the device. Consider the entire spectrum of immersion as you design ways to present your app’s most distinctive moments. You can present experiences in a windowed, UI-centric context, a fully immersive context, or something in between. For each key moment in your app, find the minimum level of immersion that suits it best — don’t assume that every moment needs to be fully immersive. Use windows for contained, UI-centric experiences. To help people perform standard tasks, prefer standard windows that appear as planes in space and contain familiar controls. In visionOS, people can relocate windows anywhere they want, and the system’s dynamic scaling helps keep window content legible whether it’s near or far. Prioritize comfort. To help people stay comfortable and physically relaxed as they interact with your app or game, keep the following fundamentals in mind. Display content within a person’s field of view, positioning it relative to their head. Avoid placing content in places where people have to turn their head or change their position to interact with it. Avoid displaying motion that’s overwhelming, jarring, too fast, or missing a stationary frame of reference. Support indirect gestures that let people interact with apps while their hands rest in their lap or at their sides. If you support direct gestures, make sure the interactive content isn’t too far away and that people don’t need to interact with it for extended periods. Avoid encouraging people to move too much while they’re in a fully immersive experience. Help people share activities with others. When you use SharePlay to support shared activities, people can view the Spatial Personas of other participants, making it feel like everyone is together in the same space. Apple Design Resources Creating your first visionOS app Date Changes June 21, 2023 New page.     Display content within a person’s field of view, positioning it relative to their head. Avoid placing content in places where people have to turn their head or change their position to interact with it.
Avoid displaying motion that’s overwhelming, jarring, too fast, or missing a stationary frame of reference.
Support indirect gestures that let people interact with apps while their hands rest in their lap or at their sides.
If you support direct gestures, make sure the interactive content isn’t too far away and that people don’t need to interact with it for extended periods.
Avoid encouraging people to move too much while they’re in a fully immersive experience. Designing for visionOS
Best practices
Resources
Change log "
5,"Adopting best practices for privacy and user preferences  Overview See Also       Adopt the system-provided input mechanisms Provide clear messaging around privacy-sensitive features Design                        To protect user privacy, the system handles camera and sensor inputs without passing the information to apps directly. Instead, the system enables your app to seamlessly interact with a user’s surroundings and to automatically receive input from the user. For example, the system handles the eye- and hand-position data needed to detect interactions with your app’s content. Similarly, the system provides a way to automatically alter a view’s appearance when someone looks at it, without your app ever knowing what the user is looking at. In the few cases where you actually need access to hand position or information about the user’s surroundings, the system requires you to obtain authorization from the user first.  Important It’s your responsibility to protect any data your app collects, and to use it in responsible and privacy-preserving ways. Don’t ask for data that you don’t need, be transparent about how you use the data you acquire, and respect the choices of the person whose data it is. For general information about privacy, see Protecting the User’s Privacy. On Apple Vision Pro, people use their eyes and hands to interact with the items they see in front of them. Where they look determines where the system applies focus, and a tap gesture with either hand generates a touch event on that focused item. The system can also detect when someone’s fingers interact with virtual items in the person’s field of vision. When you adopt the standard UIKit and SwiftUI event-handling mechanisms, you get all of these interactions automatically. For most apps, the system-provided gesture recognizers are sufficient for responding to interactions. Although you can get the position of someone’s hands with ARKit, doing so isn’t necessary for most apps. Collect hand-position data only when the system doesn’t offer what you need. For example, you might use hand-position data to attach 3D content to the person’s hands. Some other things to remember about hand-position data: People can deny your request for access to hand-position data. Be prepared to handle situations where the data isn’t available. You must present an immersive space to access hand data. When you open an immersive space, the system hides other apps. For information about how to handle the standard-system events, see the SwiftUI and UIKit documentation. The following ARKit features require you to provide a usage description string in your app’s Info.plist file: World-tracking data Hand-tracking data Other privacy-sensitive technologies in visionOS also require you to supply usage description strings. For example, you provide usage descriptions for the Core Location features you adopt. These strings communicate why your app needs the data, and how you plan to use the data to help the person using your app. The first time you request authorization to use the technology, the system prompts the person to grant or deny access to your app. The system includes your usage-description string in the dialog it displays. For information about requesting access to ARKit data, see ARKit. For guidance on how to craft good messages around privacy-friendly features, see Human Interface Guidelines.       People can deny your request for access to hand-position data. Be prepared to handle situations where the data isn’t available.
You must present an immersive space to access hand data. When you open an immersive space, the system hides other apps. World-tracking data
Hand-tracking data Info.plist "
6,"Improving accessibility support in your visionOS app  Overview See Also       Add accessibility traits to RealityKit entities Add support for Direct Gesture mode Provide alternatives to input that involves physical movement Avoid head-locked content Limit motion effects in your content Include captions for audio content Design                        visionOS is an immersive platform that supports people of all abilities. Even though experiences incorporate stunning visual content and hand- and eye-tracking technologies, people can engage with content in other ways. In fact, the platform supports people in many different situations, including those who are blind, have low vision, have limited mobility, or have limb differences. With the help of assistive technologies, people can interact with all of your app’s content. During development, enable VoiceOver and other assistive features and test your app’s accessibility support. Make sure people can navigate your app’s interface intuitively, and that all of the necessary elements are present. Improve the descriptive information for those elements to communicate their intended purpose. And make sure your app adapts to changing conditions, such as changes to the Dynamic Type setting while your app is running. Default font size Increased font size For general information about supporting accessibility, see Accessibility. For design guidance, see Human Interface Guidelines > Accessibility. VoiceOver and other assistive technologies rely on the accessibility information that your app’s views and content provide. SwiftUI and UIKit provide default information for the standard system views, but RealityKit doesn’t provide default information for the entities in your scenes. To configure the accessibility information for a RealityKit entity, add an instance of AccessibilityComponent to the entity. Use this component to specify the same values you specify for the rest of your app’s views. The following example shows how to create this component and add it to an entity: People can use VoiceOver to initiate specific types of actions on your entities. Assign a value to the systemActions property of your component if your entity supports the incrementing or decrementing of its value, or supports activation with a gesture other than a standard tap. You don’t need to set a system action if you let people interact with the entity using a standard single-tap gesture. The following example uses the content of a RealityView to determine when activation events occur on the view’s entities. After subscribing to the view’s activation events, the code sets up an asynchronous task to handle incoming events. When a new event occurs, the task executes the custom code to handle a collision. When VoiceOver is active in visionOS, people use hand gestures to navigate your app’s interface and inspect elements. To prevent your app’s code from interfering with VoiceOver interactions, the system doesn’t deliver hand input to your app during this time. However, a person can perform a special VoiceOver gesture to enable Direct Gesture mode, which leaves VoiceOver enabled but restores hand input to your app. Add VoiceOver announcements to your code to communicate the results of meaningful events. VoiceOver speaks these announcements at all times, but they are particularly useful when Direct Gesture mode is on. The following example posts an announcement when a custom gesture causes an interaction with a game piece: Reduced mobility can affect a person’s ability to interact with your app’s content. When designing your app’s input model, avoid experiences that require specific body movements or positions. For example, if your app supports custom hand gestures, add menu commands for each gesture so someone can enter them using a keyboard or assistive device. Some assistive technologies let people interact with your app using only their eyes. Using these technologies they can select, scroll, long press, or drag items in your interface. Even if you support other types of interactions, give people a way to access all of your app’s behavior using only these interactions. Some assistive technologies allow people to navigate or view your app’s interface using head movements. As the person’s head moves, the assistive technology focuses on the item directly in front of them. Content that follows the movements of the person’s head interferes with the behavior of these assistive technologies. When designing your interface, place content in windows or anchor it to locations other than the virtual camera. If you do need head-locked content, provide an alternative solution when relevant assistive technologies are in use. For example, you might move head-locked content to an anchor point that doesn’t follow the person’s head movements. To determine when to change the anchoring approach for your content, check the doc://com.apple.documentation/documentation/swiftui/environmentvalues/accessibilitypreferscameraanchoralternative environment variable in SwiftUI, or call the AXPrefersCameraAnchorAlternative() function. This environment variable is true when an assistive technology is in use that conflicts with head-locked content. Adapt your content to use alternate anchoring mechanisms at that time. Motion effects on any immersive device can be jarring, even for people who aren’t sensitive to motion. Limit the use of motion effects that incorporate rapid movement, bouncing or wave-like movement, zooming animations, multi-axis movement, spinning, or rotations. When the person wearing the device is sensitive to motion effects, eliminate the use of these effects altogether. The Reduce Motion system setting lets you know when to provide alternatives for all of your app’s motion effects. Access this setting using the accessibilityReduceMotion environment variable in SwiftUI or with the isReduceMotionEnabled property in UIKit. When the setting is true, provide suitable alternatives for motion effects or eliminate them altogether. For example, show a static snapshot of the ocean instead of a video. For more information, see Human Interface Guidelines > Motion. For people who are deaf or hard of hearing, provide high-quality captions for your app’s content. Captions are a necessity to some, but are practical for everyone in certain situations. For example, captions are useful to someone watching a video in a noisy environment. Remember to include captions not just for text and dialogue, but also for music and sound effects in your content. For Spatial Audio content, include information in your captions that indicates the direction of various sounds. AVKit and AVFoundation provide built-in support for displaying captioned content. These frameworks configure the font, size, color, and style of the captions automatically according to the person’s accessibility settings. For example, the frameworks adopt the current Dynamic Type setting when displaying text. If you have a custom video engine, check the isClosedCaptioningEnabled accessibility setting to determine when to display captions. To get the correct appearance information for your captioned content, adopt Media Accessibility in your project. This framework provides you with the optimal font, color, and opacity information to apply to captioned text and images.       AccessibilityComponent var accessibilityComponent = AccessibilityComponent()
accessibilityComponent.isAccessibilityElement = true
accessibilityComponent.traits = [.button]
accessibilityComponent.label = ""Sports car""
accessibilityComponent.value = ""Parked""
accessibilityComponent.systemActions = [.activate]
myEntity.components[AccessibilityComponent.self] = accessibilityComponent systemActions RealityView activationSubscription = content.subscribe(to: AccessibilityEvents.Activate.self, 
                            on: nil, componentType: nil) { activation in
    Task {
        try handleCollision(for: activation.entity, gameModel: gameModel)
    }
} AccessibilityNotification.Announcement(""Game piece hit"").post() AXPrefersCameraAnchorAlternative() true accessibilityReduceMotion isReduceMotionEnabled true isClosedCaptioningEnabled "
7,"Hello World  Overview See Also        Create an entry point into the app Present different modules using a navigation stack Display an interactive globe in a new scene Declare a volumetric window for the globe Open and dismiss the globe window Display objects that orbit the Earth Show Earth’s relationship to its satellites in an immersive space View the solar system from space using full immersion Related samples Related articles Related videos                                You can use visionOS scene types and styles to share information in fun and compelling ways. Features like volumes and immersive spaces let you put interactive virtual objects into people’s environments, or put people into a virtual environment. Hello World uses these tools to teach people about the Earth — the planet we call home. The app shows how the Earth’s tilt creates the seasons, how objects move as they orbit the Earth, and how Earth appears from space. The app uses SwiftUI to define its interface, including both 2D and 3D elements. To create, customize, and manage 3D models and effects, it also relies on the RealityKit framework and Reality Composer Pro. Hello World constructs the scene that it displays at launch — the first scene that appears in the WorldApp structure — using a WindowGroup: Like other platforms — for example, macOS and iOS — visionOS displays a window group as a familiar-looking window. In visionOS, people can resize and move windows around the Shared Space. Even if your app offers a sophisticated 3D experience, a window is a great starting point for an app because it eases people into the experience. It’s also a good place to provide instructions or controls. Tip This particular window group uses the doc://com.apple.documentation/documentation/SwiftUI/WindowStyle/plain window style to maintain control over the glass background effect that visionOS would otherwise automatically add. After you watch a brief introductory animation that shows the text Hello World typing in, the Modules view that defines the primary scene’s content presents options to explore different aspects of the world. This view contains a table of contents at the root of a NavigationStack: A visionOS navigation stack has the same behavior that it has in other platforms. When it first appears, the stack displays its root view. When someone chooses an embedded NavigationLink, the stack draws a new view and displays a back button in the toolbar. When someone taps the back button, the stack restores the previous view.  The trailing closure of the navigationDestination(for:destination:) view modifier in the code above displays a view when someone activates a link based on a module input that comes from the corresponding link’s initializer: The possible module values come from a custom Module enumeration: The globe module opens with a few facts about the Earth in the main window next to a decorative, flat image that supports the content. To help people understand even more, the module includes a button titled View Globe that opens a 3D interactive globe in a new window.  To be able to open multiple scene types, Hello World includes the UIApplicationSceneManifest key in its Information Property List file. The value for this key is a dictionary that includes the UIApplicationSupportsMultipleScenes key with a value of true: With the key in place, the app makes use of a second WindowGroup in its App declaration. This new window group uses the Globe view as its content: This window group creates a window that has arbitrary depth — great for displaying a 3D model in a bounded region that behaves like a transparent box — because Hello World uses the doc://com.apple.documentation/documentation/SwiftUI/WindowStyle/volumetric window style scene modifier. People can move this box around the Shared Space like any other window, and the content remains fixed inside. The doc://com.apple.documentation/documentation/SwiftUI/Scene/defaultSize(width:height:depth:in:) modifier specifies a size for the window in meters, including a depth dimension. The Globe view contains 3D content, but is still just a SwiftUI view. It contains two elements in a ZStack: a subview that draws a model of the Earth, and another that provides a control panel that people can use to configure the model’s appearance. The globe module presents a View Globe button that people can tap to display the volumetric window or dismiss the window, depending on the current state. Hello World achieves this behavior by creating a Toggle with the button style, and embedding it in a custom, reusable WindowToggle view.  When someone taps the toggle, the isShowing state changes, and the onChange(of:initial:_:) modifier calls the openWindow or dismissWindow action to open or dismiss the window, respectively. The view gets these actions from the environment and uses an identifier that matches the window’s identifier. You use windows in visionOS the same way you do in other platforms. But windows in visionOS provide a small amount of depth you can use to create 3D effects — like elements that appear in front of other elements. Hello World takes advantage of this depth to present small models inline with 2D content. The app’s second module, Objects in Orbit, provides information about objects that go around the Earth, like the Moon and artificial satellites. To give a sense of what these objects look like, the module displays 3D models of these items directly inside the window.  Hello World loads these models from the asset bundle using a Model3D structure inside a custom ItemView. The view scales and positions the model to fit the available space, and applies optional orientation adjustments: The app uses this ItemView once for each model, placing each in an overlay that only becomes visible based on the current selection. For example, the following overlay displays the satellite model with a small amount of tilt in the x-axis and z-axis: The VStack that contains the models also contains a Picker that people use to select a model to view: When you add 3D effects to a 2D window, keep this guidance in mind: Don’t overdo it. These kinds of effects add interest, but can unintentionally obscure important controls or information as people view the window from different directions. Ensure that elements don’t exceed the available depth. Excess depth causes elements to clip. Account for any position or orientation changes that might occur after initial placement. Avoid models intersecting with the backing glass. Again, account for potential movement after initial placement. People can visualize how satellites move around the Earth because the app’s orbit module displays the Earth, the Moon, and a communications satellite together as a single system. People can move the system anywhere in their environment or resize it using standard gestures. They can also move themselves around the system to get different perspectives.  Note To learn about designing with gestures in visionOS, read Gestures in Human Interface Guidelines. To create this visualization, the app displays the Orbit view — which contains a single RealityView that models the entire system — in an doc://com.apple.documentation/documentation/SwiftUI/ImmersiveSpace scene with the doc://com.apple.documentation/documentation/SwiftUI/ImmersionStyle/mixed immersion style: As with any secondary scene in a visionOS app, this scene depends on having the UIApplicationSupportsMultipleScenes key in the Information Property List file. The app also opens and closes the space using a generalized toggle view that resembles the one used for windows: There are a few key differences from the window equivalent of this toggle that appears in the section Open and dismiss the globe window: SpaceToggle uses doc://com.apple.documentation/documentation/SwiftUI/EnvironmentValues/openImmersiveSpace and doc://com.apple.documentation/documentation/SwiftUI/EnvironmentValues/dismissImmersiveSpace from the environment, rather than the window equivalents. The dismiss action in this case doesn’t require an identifier, because people can only open one space at a time, even across apps. The open and dismiss actions for spaces operate asynchronously, and so they appear inside a Task. The app’s final module gives people a sense of the Earth’s place in the solar system. Like other modules, this one includes information and a decorative image next to a button that leads to another visualization — in this case so people can experience Earth from space. When a person taps the button, the app takes over the entire display and shows stars in all directions, which you can see in the video at the right. The Earth appears directly in front, the Moon to the right, and the Sun to the left. The main window also shows a small control panel that people can use to exit the fully immersive experience.  Tip People can exit full immersion by pressing the device’s Digital Crown, but it’s typically useful when you provide a built-in mechanism to maintain control of the experience within your app. The app uses another immersive space scene for this module, but here with the doc://com.apple.documentation/documentation/SwiftUI/ImmersionStyle/full immersion style that turns off the passthrough video: This scene depends on the same UIApplicationSupportsMultipleScenes key that other secondary scenes do, and is activated by the same custom SpaceToggle that the previous section describes, but uses the Module.solar.id scene identifier in this case. To reuse the main window for the solar system controls, Hello World places both the navigation stack and the controls in a ZStack, and then sets the opacity of each to ensure that only one appears at a time:       Don’t overdo it. These kinds of effects add interest, but can unintentionally obscure important controls or information as people view the window from different directions.
Ensure that elements don’t exceed the available depth. Excess depth causes elements to clip. Account for any position or orientation changes that might occur after initial placement.
Avoid models intersecting with the backing glass. Again, account for potential movement after initial placement. SpaceToggle uses doc://com.apple.documentation/documentation/SwiftUI/EnvironmentValues/openImmersiveSpace and doc://com.apple.documentation/documentation/SwiftUI/EnvironmentValues/dismissImmersiveSpace from the environment, rather than the window equivalents.
The dismiss action in this case doesn’t require an identifier, because people can only open one space at a time, even across apps.
The open and dismiss actions for spaces operate asynchronously, and so they appear inside a Task. WorldApp WindowGroup WindowGroup(""Hello World"", id: ""modules"") {
    Modules()
        .environment(model)
}
.windowStyle(.plain) Modules NavigationStack NavigationStack(path: $model.navigationPath) {
    TableOfContents()
        .navigationDestination(for: Module.self) { module in
            ModuleDetail(module: module)
                .navigationTitle(module.eyebrow)
        }
} NavigationLink navigationDestination(for:destination:) module NavigationLink(value: module) { /* The link's label. */ } module Module enum Module: String, Identifiable, CaseIterable, Equatable {
    case globe, orbit, solar
    // ...
} globe UIApplicationSupportsMultipleScenes true <key>UIApplicationSceneManifest</key>
<dict>
    <key>UIApplicationSupportsMultipleScenes</key>
    <true/>
    <key>UISceneConfigurations</key>
    <dict/>
</dict> WindowGroup App Globe WindowGroup(id: Module.globe.name) {
    Globe()
        .environment(model)
}
.windowStyle(.volumetric)
.defaultSize(width: 0.6, height: 0.6, depth: 0.6, in: .meters) Globe ZStack Toggle WindowToggle private struct WindowToggle: View {
    var title: String
    var id: String
    @Binding var isShowing: Bool


    @Environment(\.openWindow) private var openWindow
    @Environment(\.dismissWindow) private var dismissWindow


    var body: some View {
        Toggle(title, isOn: $isShowing)
            .onChange(of: isShowing) { wasShowing, isShowing in
                if isShowing {
                    openWindow(id: id)
                } else {
                    dismissWindow(id: id)
                }
            }
            .toggleStyle(.button)
    }
} isShowing onChange(of:initial:_:) openWindow dismissWindow Model3D ItemView private struct ItemView: View {
    var item: Item
    var orientation: SIMD3<Double> = .zero


    var body: some View {
        Model3D(named: item.name, bundle: worldAssetsBundle) { model in
            model.resizable()
                .scaledToFit()
                .rotation3DEffect(
                    Rotation3D(
                        eulerAngles: .init(angles: orientation, order: .xyz)
                    )
                )
                .frame(depth: modelDepth)
                .offset(z: -modelDepth / 2)
        } placeholder: {
            ProgressView()
                .offset(z: -modelDepth * 0.75)
        }
    }
} ItemView .overlay {
    ItemView(item: .satellite, orientation: [0.15, 0, 0.15])
        .opacity(selection == .satellite ? 1 : 0)
} VStack Picker Picker(""Satellite"", selection: $selection) {
    ForEach(Item.allCases) { item in
        Text(item.name)
    }
}
.pickerStyle(.segmented) Orbit RealityView ImmersiveSpace(id: Module.orbit.name) {
    Orbit()
        .environment(model)
}
.immersionStyle(selection: $orbitImmersionStyle, in: .mixed) UIApplicationSupportsMultipleScenes private struct SpaceToggle: View {
    var title: String
    var id: String
    @Binding var isShowing: Bool


    @Environment(\.openImmersiveSpace) private var openImmersiveSpace
    @Environment(\.dismissImmersiveSpace) private var dismissImmersiveSpace


    var body: some View {
        Toggle(title, isOn: $isShowing)
            .onChange(of: isShowing) { wasShowing, isShowing in
                Task {
                    if isShowing {
                        await openImmersiveSpace(id: id)
                    } else {
                        await dismissImmersiveSpace()
                    }
                }
            }
            .toggleStyle(.button)
    }
} SpaceToggle Task ImmersiveSpace(id: Module.solar.name) {
    SolarSystem()
        .environment(model)
}
.immersionStyle(selection: $solarImmersionStyle, in: .full) UIApplicationSupportsMultipleScenes SpaceToggle Module.solar.id ZStack ZStack {
    SolarSystemControls()
        .opacity(model.isShowingSolar ? 1 : 0)


    NavigationStack(path: $model.navigationPath) {
        // ...
    }
    .opacity(model.isShowingSolar ? 0 : 1)
}
.animation(.default, value: model.isShowingSolar) "
8,"Presenting windows and spaces  Overview See Also        Check for multiple-scene support Enable multiple simultaneous scenes Open windows programmatically Open a space programmatically Designate a space as your app’s main interface Close windows programmatically Close spaces programmatically SwiftUI                                An app’s scenes, which contain views that people interact with, can take different forms. For example, a scene can fill a window, a tab in a window, or an entire screen. Some scenes can even place views throughout a person’s surroundings. How a scene appears depends on its type, the platform, and the context. When someone launches your app, SwiftUI looks for the first WindowGroup, Window, or DocumentGroup in your app declaration and opens a scene of that type, typically filling a new window or the entire screen, depending on the platform. For example, the following app running in macOS presents a window that contains a MailViewer view: In visionOS, you can alternatively configure your app to open the first doc://com.apple.documentation/documentation/SwiftUI/ImmersiveSpace that the app declares. In any case, specific platforms and configurations enable you to open more than one scene at a time. Under those conditions, you can use actions that appear in the environment to programmatically open and close the scenes in your app. If you share code among different platforms and need to find out at runtime whether the current system supports displaying multiple scenes, read the supportsMultipleWindows environment value. The following code creates a button that’s hidden unless the app supports multiple windows: The value that you read depends on both the platform and how you configure your app: In macOS, this property returns true for any app that uses the SwiftUI app lifecycle. In iPadOS and visionOS, this property returns true for any app that uses the SwiftUI app lifecycle and has the Information Property List key UIApplicationSupportsMultipleScenes set to true, and false otherwise. For all other platforms and configurations, the value returns false. If your app only ever runs in one of these situations, you can assume the associated behavior and don’t need to check the value. You can always present multiple scenes in macOS. To enable an iPadOS or visionOS app to simultaneously display multiple scenes — including doc://com.apple.documentation/documentation/SwiftUI/ImmersiveSpace scenes in visionOS — add the UIApplicationSupportsMultipleScenes key with a value of true in the UIApplicationSceneManifest dictionary of your app’s Information Property List. Use the Info tab in Xcode for your app’s target to add this key:  Apps on other platforms can display only one scene during their lifetime. Some platforms provide built-in controls that enable people to open instances of the window-style scenes that your app defines. For example, in macOS people can choose File > New Window from the menu bar to open a new window. SwiftUI also provides ways for you to open new windows programmatically. To do this, get the openWindow action from the environment and call it with an identifier, a value, or both to indicate what kind of window to open and optionally what data to open it with. The following view opens a new instance of the previously defined mail viewer window when someone clicks or taps the button: When the action runs on a system that supports multiple scenes, SwiftUI looks for a window in the app declaration that has a matching identifier and creates a new scene of that type. Important If supportsMultipleWindows is false and you try to open a new window, SwiftUI ignores the action and logs a runtime error. In addition to opening more instances of an app’s main window, as in the above example, you can also open other window types that your app’s body declares. For example, you can open an instance of the Window that displays connectivity information: In visionOS, you open an immersive space — a scene that you can use to present unbounded content in a person’s surroundings — in much the same way that you open a window, except that you use the doc://com.apple.documentation/documentation/SwiftUI/EnvironmentValues/openImmersiveSpace action. The action runs asynchronously, so you use the await keyword when you call it, and typically do so from inside a Task: Because your app operates in a Full Space when you open an doc://com.apple.documentation/documentation/SwiftUI/ImmersiveSpace scene, you can only open one scene of this type at a time. If you try to open a space when one is already open, the system logs a runtime error. Your app can display any number of windows together with an immersive space. However, when you open a space from your app, the system hides all windows that belong to other apps. After you dismiss your space, the other apps’ windows reappear. Similarly, the system hides your app’s windows if another app opens an immersive space. When visionOS launches an app, it opens the first window group, window, or document scene that the app’s body declares, just like on other platforms. This is true even if you first declare a space. However, if you want to open your app into an immersive space directly, specify a space as the default scene for your app by adding the UIApplicationPreferredDefaultSceneSessionRole key to your app’s information property list and setting its value to UISceneSessionRoleImmersiveSpaceApplication. In that case, visionOS opens the first space that it finds in your app declaration. Important Be careful not to overwhelm people when starting your app with an immersive space. For design guidance, see Immersive experiences. People can close windows using system controls, like the close button built into the frame around a macOS window. You can also close windows programmatically. Get the dismissWindow action from the environment, and call it using the identifier of the window that you want to dismiss: To close a space, call the doc://com.apple.documentation/documentation/SwiftUI/EnvironmentValues/dismissImmersiveSpace action. Like the corresponding open space action, the close action operates asynchronously and requires the await keyword: You don’t need to specify an identifier for this action, because there can only ever be one space open at a time.       In macOS, this property returns true for any app that uses the SwiftUI app lifecycle.
In iPadOS and visionOS, this property returns true for any app that uses the SwiftUI app lifecycle and has the Information Property List key UIApplicationSupportsMultipleScenes set to true, and false otherwise.
For all other platforms and configurations, the value returns false. WindowGroup Window DocumentGroup MailViewer @main
struct MailReader: App {
    var body: some Scene {
        WindowGroup(id: ""mail-viewer"") {
            MailViewer()
        }


        Window(""Connection Status"", id: ""connection"") {
            ConnectionStatus()
        }
    }
} supportsMultipleWindows struct NewWindowButton: View {
    @Environment(\.supportsMultipleWindows) private var supportsMultipleWindows
    @Environment(\.openWindow) private var openWindow


    var body: some View {
        Button(""Open New Window"") {
            openWindow(id: ""mail-viewer"")
        }
        .opacity(supportsMultipleWindows ? 1 : 0)
    }
} true true UIApplicationSupportsMultipleScenes true false false UIApplicationSupportsMultipleScenes true openWindow struct NewViewerButton: View {
    @Environment(\.openWindow) private var openWindow


    var body: some View {
        Button(""New Mail Viewer"") {
            openWindow(id: ""mail-viewer"")
        }
    }
} supportsMultipleWindows false Window Button(""Connection Status"") {
    openWindow(id: ""connection"")
} await Task struct NewSpaceButton: View {
    @Environment(\.openImmersiveSpace) private var openImmersiveSpace


    var body: some View {
        Button(""View Orbits"") {
            Task {
                await openImmersiveSpace(id: ""orbits"")
            }
        }
    }
} UIApplicationPreferredDefaultSceneSessionRole UISceneSessionRoleImmersiveSpaceApplication dismissWindow private struct ContentView: View {
    @Environment(\.dismissWindow) private var dismissWindow


    var body: some View {
        Button(""Done"") {
            dismissWindow(id: ""connection"")
        }
    }
} await private struct ContentView: View {
    @Environment(\.dismissImmersiveSpace) private var dismissImmersiveSpace


    var body: some View {
        Button(""Done"") {
            Task {
                await dismissImmersiveSpace()
            }
        }
    }
} "
9,"Positioning and sizing windows  Overview See Also        Specify initial window position Specify initial window size Specify window resizability Specify a volume size SwiftUI                                visionOS and macOS enable people to move and resize windows. In some cases, your app can use scene modifiers to influence a window’s initial geometry on these platforms, as well as to specify the strategy that the system employs to place minimum and maximum size limitations on a window. This kind of configuration affects both windows and volumes, which are windows with the doc://com.apple.documentation/documentation/SwiftUI/WindowStyle/volumetric window style. Your ability to configure window size and position is subject to the following constraints: The system might be unable to fulfill your request. For example, if you specify a default size that’s outside the range of the window’s resizability, the system clamps the affected dimension to keep it in range. Although you can change the window’s content, you can’t directly manipulate window position or size after the window appears. This ensures that people have full control over their workspace. During state restoration, the system restores windows to their previous position and size. Note Windows in iPadOS occupy the full screen, or share the screen with another window in Slide Over or Split View. You can’t programmatically affect window geometry on that platform. In macOS, the first time your app opens a window from a particular scene declaration, the system places the window at the center of the screen by default. For scene types that support multiple simultaneous windows, the system offsets each additional window by a small amount to avoid fully obscuring existing windows. You can override the default placement of the first window by applying the defaultPosition(_:) scene modifier to indicate where to place the window relative to the screen bounds. For example, you can request that the system place a new window in the bottom trailing corner of the screen: The system aligns the point in the window that corresponds to the specified UnitPoint with the point in the screen that corresponds to the same unit point. You can use a built-in unit point, like bottomTrailing in the above example, or define a custom one. In visionOS, the system places new windows directly in front of people, where they happen to be gazing at the moment the window opens. This helps to ensure that people become aware of newly opened windows. In macOS, you can indicate a default initial size for a new window that the system creates from a Scene declaration by applying one of the default size scene modifiers, like defaultSize(width:height:). For example, you can request that new windows that a WindowGroup generates occupy 600 points in the x-dimension and 400 points in the y-dimension: The system might clamp the actual size of the window depending on both the window’s content and resizability settings. In visionOS, all windows open with the same initial dimensions. Both macOS and visionOS provide interface controls that enable people to resize windows, within certain limits. For example, people can use the control that appears when they look at the corner of a visionOS window to resize a window on that platform. In macOS, you can specify how the system limits window resizability. The default resizability for all scenes is automatic. With that strategy, Settings windows use the contentSize strategy, where both the minimum and maximum window size match the respective minimum and maximum sizes of the content that the window contains. Other scene types use contentMinSize by default, which retains the minimum size restriction, but doesn’t limit the maximium size. You can specify one of these resizability strategies explicitly by adding the windowResizability(_:) scene modifier to a scene. For example, people can resize windows from the following window group to between 100 and 400 points in both dimensions because the frame modifier imposes those bounds on the content view: You can take this even further and enforce a specific size for a window with content that has a fixed size. In visionOS, the system enforces a standard minimum and maximum size for all windows, regardless of the content they contain. When you create a volume, which is a window with the doc://com.apple.documentation/documentation/SwiftUI/WindowStyle/volumetric style, you can specify the volume’s size using one of the three-dimensional default size modifiers, like doc://com.apple.documentation/documentation/SwiftUI/Scene/defaultSize(width:height:depth:in:). The following code creates a volume that’s one meter on a side: The volume maintains this size for its entire lifetime. People can’t change the size of a volume at runtime. Although you can specify a volume’s size in points, it’s typically better to use physical units, like the above code which specifies a size in meters. This is because the system renders a volume with fixed scaling rather than dynamic scaling, unlike a regular window, which means the volume appears more like a physical object than a user interface. For information about the different kinds of scaling, see Spatial layout.       The system might be unable to fulfill your request. For example, if you specify a default size that’s outside the range of the window’s resizability, the system clamps the affected dimension to keep it in range.
Although you can change the window’s content, you can’t directly manipulate window position or size after the window appears. This ensures that people have full control over their workspace.
During state restoration, the system restores windows to their previous position and size. defaultPosition(_:) @main
struct MyApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
        .defaultPosition(.bottomTrailing)
    }
} UnitPoint bottomTrailing Scene defaultSize(width:height:) WindowGroup @main
struct MyApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
        .defaultSize(CGSize(width: 600, height: 400))
    }
} automatic Settings contentSize contentMinSize windowResizability(_:) @main
struct MyApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
                .frame(
                    minWidth: 100, maxWidth: 400,
                    minHeight: 100, maxHeight: 400)
        }
        .windowResizability(.contentSize)
    }
} WindowGroup(id: ""globe"") {
    Globe()
}
.windowStyle(.volumetric)
.defaultSize(width: 1, height: 1, depth: 1, in: .meters) "
10,"Diorama  Overview See Also       Import assets for building the scene Create scenes containing the app’s entities Add assets to your scenes Add components to entities Use transforms to mark locations Load a scene at runtime Create the floating view Create attachments for points of interest Display point of interest attachments Create custom materials with Shader Graph Update the Shader Graph material at runtime Related samples Related articles Related videos                                Use Reality Composer Pro to compose, edit, and preview RealityKit content for your visionOS app. In your Reality Composer Pro project, you can create one or more scenes, each of which contains a hierarchy of virtual objects called entities that your app can efficiently load and display. In addition to helping you compose entity hierarchies, Reality Composer Pro also gives you the ability to add and configure components — even custom components that you’ve written — to the entities in your scenes. You can also design the visual appearance of entities using Shader Graph, a node-based visual tool for creating RealityKit materials. Shader Graph gives you a tremendous amount of control over the surface details and shape of entities. You can even create animated materials and dynamic materials that change based on the state of your app or user input. Diorama demonstrates many of RealityKit and Reality Composer Pro’s features. It displays an interactive, virtual topographical trail map, much like the real-world dioramas you find at trailheads and ranger stations in national parks. This virtual map has points of interest you can tap to bring up more detailed information. You can also smoothly transition between two trail maps: Yosemite and Catalina Island. Your Reality Composer Pro project must contain assets, which you use to compose scenes for your app. Diorama’s project has several assets, including 3D models like the diorama table, trail map, some birds and clouds that fly over the map, and a number of sounds and images. Reality Composer Pro provides a library of 3D models you can use. Access the library by clicking the Add (+) button on the right side of the toolbar. Selecting objects from the library imports them into your project.  Diorama uses custom assets instead of the available library assets. To use custom assets in your own Reality Composer Pro scenes, import them into your project in one of three ways: by dragging them to Reality Composer Pro’s project browser, using File > Import from the File menu, or copying the assets into the .rkassets bundle inside your project’s Swift package.  Note Although you can still load USDZ files and other assets directly in visionOS, RealityKit compiles assets in your Reality Composer Pro project into a binary format that loads considerably faster than loading from individual files. A single Reality Composer Pro project can have multiple scenes. A scene is an entity hierarchy stored in the project as a .usda file that you can load and display in a RealityView. You can use Reality Composer’s scenes to build an entire RealityKit scene, or to store reusable entity hierarchies that you can use as building block for composing scenes at runtime — the approach Diorama uses. You can add as many different scenes to your project as you need by selecting File > New > Scene, or pressing ⌘N. At the top of the Reality Composer Pro window, there’s a separate tab for every scene that’s currently open. To open a scene, double-click the scene’s .usda file in the project browser. To edit a scene, select its tab, and make changes using the hierarchy viewer, the 3D view, and the inspector.  RealityKit can only include entities in a scene, but it can’t use every type of asset that Reality Composer Pro supports as an entity. Reality Composer Pro automatically turns some assets, like 3D models, into an entity when you place them in a scene. It uses other assets indirectly. It uses image files, for example, primarily to define the surface details of model entities. Diorama uses multiple scenes to group assets together and then, at runtime, combines those scenes into a single immersive experience. For example, the diorama table has its own scene that includes the table, the map surface, and the trail lines. There are separate scenes for the birds that flock over the table, and for the clouds that float above it.  To add entities to a scene, drag assets from the project browser to the scene’s hierarchy view or 3D view. If the asset you drag is a type that can be represented as an entity, Reality Composer Pro adds it to your scene. You can select any asset in the scene hierarchy or the 3D view and change its location, rotation, and scale using the inspector on the right side of the window or the manipulator in the 3D view. RealityKit follows a design pattern called Entity Component System (ECS). In an ECS app, you store additional data on an entity using components and can implement entity behavior by writing systems that use the data from those components. You can add and configure components to entities in Reality Composer Pro, including both shipped components like PhysicsBodyComponent, and custom components that you write and place in the Sources folder of your Reality Composer Pro Swift package. You can even create new components in Reality Composer Pro and then edit them in Xcode. For more information about ECS, see Understanding RealityKit’s modular architecture. Diorama uses custom components to identify which transforms are points of interest, to mark the birds so the app can make sure they flock together, and to control the opacity of entities that are specific to just one of the two maps. To add a component to an entity, select that entity in the hierarchy view or 3D view. At the bottom right of the inspector window, click on the Add Component button. A list of available components appears and the first item in that list is New Component. This item creates a new component class, and optionally a new system class, and adds the component to the selected entity. If you look at the list of components, you see the PointOfInterestComponent that Diorama uses to indicate which transforms are points of interest. If the selected entity doesn’t already contain a PointOfInterestComponent, selecting that adds it to the selected entity. Each entity can only have one component of a specific type. You can edit the values of the existing component in the inspector, which changes what shows up when you tap that point of interest in the app.  In Reality Composer Pro, a transform is an empty entity that marks a point in space. A transform contains a location, rotation, and scale, and its child entities inherit those. But, transforms have no visual representation and do nothing by themselves. Use transforms to mark locations in your scene or organize your entity hierarchy. For example, you might make several entities that need to move together into child entities of the same transform, so you can move them together by moving the parent transform. Diorama uses transforms with a PointOfInterestComponent to indicate points of interest on the map. When the app runs, those transforms mark the location of the floating placards with the name of the location. Tapping on a placard expands it to show more detailed information. To turn transforms into an interactive view, the app looks for a specific component on transforms called a PointOfInterestComponent. Because a transform contains no data other than location, orientation, and scale, it uses this component to hold the data the app needs to display on the placards. If you open the DioramaAssembled scene in Reality Composer Pro and click on the transform called Cathedral_Rocks, you see the PointOfInterestComponent in the inspector.  To load a Reality Composer Pro scene, use load(named:in:), passing the name of the scene you want to load and the project’s bundle. Reality Composer Pro Swift packages define a constant that provides ready access to its bundle. The constant is the name of the Reality Composer Pro project with “Bundle” appended to the end. In this case, the project is called RealityKitContent, so the constant is called RealityKitContentBundle. Here’s how Diorama loads the map table in the RealityView initializer: The load(named:in:) function is asynchronous when called from an asynchronous context. Because the content closure of the RealityView initializer is asynchronous, it automatically uses the async version to load the scene. Note that when using it asynchronously, you must call it using the await keyword. Diorama adds a PointOfInterestComponent to a transform to display details about interesting places. Every point of interest’s name appears in a view that floats above its location on the map. When you tap the floating view, it expands to show detailed information, which the app pulls from the PointOfInterestComponent. The app shows these details by creating a SwiftUI view for each point of interest and querying for all entities that have a PointOfInterestComponent using this query declared in ImmersiveView.swift: In the RealityView initializer, Diorama queries to retrieve the points of interest entities and passes them to a function called createLearnMoreView(for:), which creates the view and saves it for display when it’s tapped. Diorama displays the information added to a PointOfInterestComponent in a LearnMoreView, which it stores as an attachment. Attachments are SwiftUI views that are also RealityKit entities and that you can place into a RealityKit scene at a specific location. Diorama uses attachments to position the view that floats above each point of interest. The app first checks to see if the entity has a component called PointOfInterestRuntimeComponent. If it doesn’t, it creates a new one and adds it to the entity. This new component contains a value you only use at runtime that you don’t need to edit in Reality Composer Pro. By putting this value into a separate component and adding it to entities at runtime, Reality Composer Pro never displays it in the inspector. The PointOfInterestRuntimeComponent stores an identifier called an attachment tag, which uniquely identifies an attachment so the app can retrieve and display it at the appropriate time. Next, Diorama creates a SwiftUI view called a LearnMoreView with the information from the PointOfInterestComponent, tags that view, and stores the tag in the PointOfInterestRuntimeComponent. Finally, it stores the view in an AttachmentProvider, which is a custom class that maintains references to the attachment views so they don’t get deallocated when they’re not in a scene. Assigning a view to an attachment provider doesn’t actually display that view in the scene. The initializer for RealityView has an optional view builder called attachments that’s used to specify the attachments. In the update closure of the initializer, which RealityKit calls when the contents of the view change, the app queries for entities with a PointOfInterestRuntimeComponent, uses the tag from that component to retrieve the correct attachment for it, and then adds that attachment and places it above its location on the map. To switch between the two different topographical maps, Diorama shows a slider that morphs the map between the two locations. To accomplish this, and to draw elevation lines on the map, the FlatTerrain entity in the DioramaAssembled scene uses a Shader Graph material. Shader Graph is a node-based material editor that’s built into Reality Composer Pro. Shader Graph gives you the ability to create dynamic materials that you can change at runtime. Prior to Reality Composer Pro, the only way to implement a dynamic material like this was to create a CustomMaterial and write Metal shaders to implement the necessary logic. Diorama’s DynamicTerrainMaterialEnhanced does two things. It draws contour lines on the map based on height data stored in displacement map images, and it also offsets the vertices of the flat disk based on the same data. By interpolating between two different height maps, the app achieves a smooth transition between the two different sets of height data. When you build Shader Graph materials, you can give them input parameters called promoted inputs that you set from Swift code. This allows you to implement logic that previously required writing a Metal shader. The materials you build in the editor can affect both the look of an entity using the custom surface output node, which equates to writing Metal code in a fragment shader, or the position of vertices using the geometry modifier output, which equates to Metal code running in a vertex shader.  Node graphs can contain subgraphs, which are similar to functions. They contain reusable sets of nodes with inputs and outputs. Subgraphs contain the logic to draw the contour lines and the logic to offset the vertices. Double-click a subgraph to edit it. For more information about building materials using Shader Graph, see Explore Materials in Reality Composer Pro. To change the map, DynamicTerrainMaterialEnhanced has a promoted input called Progress. If that parameter is set to 1.0, it displays Catalina Island. If it’s set to 0, it displays Yosemite. Any other number shows a state in transition between the two. When someone manipulates the slider, the app updates that input parameter based on the slider’s value. Important Shader Graph material parameters are case-sensitive. If the capitalization is wrong, your code won’t actually update the material. The app sets the value of the input parameter in a function called handleMaterial() that the slider’s .onChanged closure calls. That function retrieves the ShaderGraphMaterial from the terrain entity and calls setParameter(name:value:) on it.       .rkassets .usda RealityView .usda PhysicsBodyComponent PointOfInterestComponent PointOfInterestComponent PointOfInterestComponent PointOfInterestComponent DioramaAssembled Cathedral_Rocks PointOfInterestComponent load(named:in:) RealityKitContent RealityKitContentBundle RealityView let entity = try await Entity.load(named: ""DioramaAssembled"", 
                                   in: RealityKitContent.RealityKitContentBundle) load(named:in:) RealityView async await PointOfInterestComponent PointOfInterestComponent PointOfInterestComponent ImmersiveView.swift static let markersQuery = EntityQuery(where: .has(PointOfInterestComponent.self)) RealityView createLearnMoreView(for:) subscriptions.append(content.subscribe(to: ComponentEvents.DidAdd.self, componentType: PointOfInterestComponent.self, { event in
    createLearnMoreView(for: event.entity)
})) PointOfInterestComponent LearnMoreView PointOfInterestRuntimeComponent PointOfInterestRuntimeComponent struct PointOfInterestRuntimeComponent: Component {
    let attachmentTag: ObjectIdentifier
} LearnMoreView PointOfInterestComponent PointOfInterestRuntimeComponent AttachmentProvider let tag: ObjectIdentifier = entity.id


let view = LearnMoreView(name: pointOfInterest.name,
                         description: pointOfInterest.description ?? """",
                         imageNames: pointOfInterest.imageNames,
                         trail: trailEntity,
                         viewModel: viewModel)
    .tag(tag)
entity.components[PointOfInterestRuntimeComponent.self] = PointOfInterestRuntimeComponent(attachmentTag: tag)


attachmentsProvider.attachments[tag] = AnyView(view) RealityView attachments ForEach(attachmentsProvider.sortedTagViewPairs, id: \.tag) { pair in
    pair.view
} update PointOfInterestRuntimeComponent viewModel.rootEntity?.scene?.performQuery(Self.runtimeQuery).forEach { entity in


    guard let attachmentEntity = attachments.entity(for: component.attachmentTag) else { return }
    
    if let pointOfInterestComponent = entity.components[PointOfInterestComponent.self] {
        attachmentEntity.components.set(RegionSpecificComponent(region: pointOfInterestComponent.region))
        attachmentEntity.components.set(OpacityComponent(opacity: 0))
    }
    
    viewModel.rootEntity?.addChild(attachmentEntity)
    attachmentEntity.setPosition([0, 0.2, 0], relativeTo: entity)
} FlatTerrain DioramaAssembled CustomMaterial DynamicTerrainMaterialEnhanced DynamicTerrainMaterialEnhanced Progress 1.0 0 handleMaterial() .onChanged ShaderGraphMaterial setParameter(name:value:) private func handleMaterial() {
    guard let terrain = viewModel.rootEntity?.terrain,
            let terrainMaterial = terrainMaterial else { return }
    do {
        var material = terrainMaterial
        try material.setParameter(name: materialParameterName, value: .float(viewModel.sliderValue))
        
        if var component = terrain.modelComponent {
            component.materials = [material]
            terrain.components.set(component)
        }
        
        try terrain.update(shaderGraphMaterial: terrainMaterial, { m in
            try m.setParameter(name: materialParameterName, value: .float(viewModel.sliderValue))
        })
    } catch {
        print(""problem: \(error)"")
    }
} "
11,"Understanding RealityKit’s modular architecture  Overview Start with Entities Add components to entities Create systems to implement entity behavior See Also       RealityKit and Reality Composer Pro                                RealityKit is a 3D framework designed for building apps, games, and other immersive experiences. Although it’s built in an object-oriented language and uses object-oriented design principles, RealityKit’s architecture avoids heavy use of composition — where objects are built by adding instance variables that hold references to other objects — in favor of a modular design based on a paradigm called Entity Component System (ECS) that divides application objects into one of three types. Following the ECS paradigm allows you to re-use the functionality contained in a component in many different entities, even if they have very different inheritance chains. Even if two objects have no common ancestors other then Entity, you can add the same components to both of them and give them the same behavior or functionality. Entities are the core actors of RealityKit. Any object that you can put into a scene, whether visible or not, is an entity and must be a descendent of Entity. Entities can be 3D models, shape primitives, lights, or even invisible items like sound emitters or trigger volumes. Add components to entities to let them store additional state relevant to a specific type of functionality. Entities themselves contain relatively few properties: Nearly all entity state is stored on an entity’s components. RealityKit provides a number of entity types you use to represent different kinds of objects. For example, a ModelEntity represents a 3D model, such as one imported from a .usdz or .reality file. These provided entities are essentially just an Entity with certain components already added to them. Adding a ModelComponent to an instance of Entity, for example, results in an entity with identical functionality to a ModelEntity. Components are modular building blocks that you add to an entity; they identify which entities a system will act on, and maintain the per-entity state that systems rely on. Components can contain logic, but limit component logic to code that validates its property values or sets its initial state. Use systems for any logic that affects the behavior of entities or that potentially changes their state on every frame. To add accessibility information to an entity, for example, add a AccessibilityComponent to it and populate its fields with the information the accessibility system needs, such as putting the description that VoiceOver reads into its label property. Keep in mind that an entity can only hold one copy of any particular type of component at a time. So, for example, you can’t add two accessibility components to one entity. If you add an accessibility component to an entity that already has one, the new component replaces the previous one. A System contains code that RealityKit calls on every frame to implement a specific type of entity behavior or to update a particular type of entity state. Systems use components to store their entity-specific state and query for entities to act on by looking for ones with a specific component or combination of components. For example, a game might have a damage system that monitors and updates the health of every entity that can be damaged or destroyed. Systems typically work together with one or more components, so that damage system might use a health component to keep track of how much damage each entity has taken and how much each one is able to take before it’s destroyed. It might also interact with other components. For example, an entity might have an armor component that provides protection to the entity, and the damage system would also need to use the state stored in that component. Every frame, the damage system queries for entities that have the health component and updates values on those entities’ components based on the current state of the app. If an entity has taken too much damage, the system might trigger a specific animation or remove the entity from the scene. Writing entity logic in a system avoids duplication of work. Using traditional OOP design patterns, where this type of logic would reside on the entity class, can often result in the same calculations being performed multiple times, once for every entity potentially affected. No matter how many entities the calculation potentially impacts the system only has to do the calculation once. For more information on creating systems, see Implementing systems for entities in a scene       Entity Entity ModelEntity .usdz .reality Entity ModelComponent Entity ModelEntity AccessibilityComponent label System "
12,"Designing RealityKit content with Reality Composer Pro  Overview See Also        Launch Reality Composer Pro Orient yourself in Reality Composer Pro Add assets to your project Compose scenes from assets Activate and deactivate scene entities Add components to entities Create or modify entity hierarchies Modify or create new materials Build materials in Shader Graph Use references to reuse assets Preview scenes on device Load Reality Composer Pro scenes in RealityKit RealityKit and Reality Composer Pro                                Use Reality Composer Pro to visually design, edit, and preview RealityKit content. In Reality Composer Pro, you can create one or more scenes, which act as a container for RealityKit content. Scenes contain hierarchies of entities, which are virtual objects such as 3D models.  In addition to helping you compose scenes, Reality Composer Pro also gives you the ability to add and configure components — even custom components that you’ve written — to the entities in your scenes and also lets you create complex materials and effects using a node-based material editor called Shader Graph. When you create a visionOS project in Xcode, it also contains a default Reality Composer Pro project named RealityKitContent within the Packages folder, which is a Swift package. The RealityKitContent package can include images, 3D models, and other assets like audio and video files. The assets you add to your project go in the RealityKitContent.rkassets bundle, while your source code goes into its Sources directory. The package also contains a file called Package.realitycomposerpro, which is the actual Reality Composer Pro project. To launch Reality Composer Pro, double-click the Package.realitycomposerpro file in the Project navigator, or click the Open in Reality Composer Pro button. If your project doesn’t already have a Reality Composer Pro project, you can launch Reality Composer Pro directly by choosing Xcode > Open Developer Tool > Reality Composer Pro. For efficiency, store all of your RealityKit assets in Reality Composer Pro projects. Xcode compiles Reality Composer Pro projects into a more efficient format when you build your app. Note Loading assets from a .reality file is considerably faster and more resource efficient than loading individual asset files. The Reality Composer Pro window has several sections. The top-half displays the active scene. If you have multiple scenes, the window shows a tab bar at the top with one tab for each open scene. A scene in Reality Composer Pro is an entity hierarchy stored in a .usda file. The left side of the top pane contains the hierarchy browser, which shows a tree representation of the entities in the active scene. You can toggle it using the top-left toolbar button to reveal errors and warnings. The middle pane is the 3D View, which shows a 3D representation of the active scene. The top-right is the inspector, which shows configurable values for the item selected in the 3D view, hierarchy view, or Shader Graph, depending on which has focus. Tip A Reality Composer Pro scene can represent an entire RealityKit scene, and you can have multiple scenes in your Reality Composer Pro project, each driving a different RealityView in the same app. A scene can also contain a collection of entities to use as a building block. For example, if you had an airplane model, you might build a scene for it that contains its 3D model, a particle effect to make smoke come out its engine, and audio entities or components that represent the various sounds a plane makes. Your app could then load those combined assets and use them together anywhere it needs. The bottom half of Reality Composer Pro contains the following four tabs: Displays all of the assets in your project. An advanced, node-based material editor. A tool for combining sound assets. Information about the currently open scene, such as the number of entities, vertices, and animations it contains.  Reality Composer Pro projects start with a single empty scene called Scene which is stored in a file called Scene.usda. You can create as many additional scenes as you need by choosing File > New > Scene. New scenes open as tabs along the top of the window, and they also appear in the Project Browser as .usda files. If you close a scene’s tab and need to re-open it, double-click on the scene’s .usda file in the Project Browser. If you no longer need a scene, delete its .usda file from the Project Browser or remove it from your project’s .rkassets bundle in Xcode. To delete a scene: Close the scene tab by selecting File > Close Tab Select the scene’s .usda file in the Project Browser Control-click the scene’s .usda file the Project Browser. Choose Delete from the contextual menu. Click Move to Trash. This removes the scene’s .usda and the scene tab at the top of the window. In Reality Composer Pro, you design scenes by first importing assets into your project. Then add assets to scenes and move, rotate, and scale them. The Project Browser tab displays all of the asset files in your project. You can add new assets by dragging them to the Project Browser or by choosing File > Import and select the assets to add to your project. To add an asset from the Project Browser to the current scene, drag it to the 3D view in the center of the window, or to the hierarchy view in the top-left of the window. Note Reality Composer Pro projects can contain assets not used in any scene. Such assets are still compiled into your app and can be loaded at runtime and take full advantage of the efficient loading process for .reality files. Reality Composer Pro can represent many assets as entities, but it can’t represent all assets assets that way; for example: USDZ models do become an entity or entity hierarchy when you add them to a scene. Image files do not become an entity. Reality Composer Pro only uses image assets indirectly, such as being the source texture for materials you build in Shader Graph. If you drag assets that Reality Composer Pro can’t turn into an entity, nothing happens. Add any 3D models, animations, sounds, and image files you need to your project. You can organize your assets into subfolders to make the Project Browser more manageable as your project grows in size. Reality Composer Pro has a library of assets that you can use in your own apps. You can access the library by clicking the Add button (+) in the toolbar. Click the icon of the down-arrow inside a circle next to an asset to download the asset to Reality Composer Pro. When the download finishes, you can double-click or drag the asset into your project.  Important Reality Composer Pro treats your imported assets as read-only. Changes you make to assets in a scene only affect that scene’s copy of the asset. The changes you make are stored in the scene’s .usda file, not in the original asset. That means you can work without fear of inadvertently changing other scenes. If you plan to make significant changes to an imported 3D model, such as by replacing its materials with dynamic Shader Graph materials, import the model as a.usdc file instead of as a .usdz file, and then separately import just the supporting assets you need to avoid Xcode compiling assets that you don’t need into your app. All RealityKit entities in a scene exist at a specific position, orientation, and scale, even if that entity has no visual representation. When you click to select an entity in the 3D view or hierarchy view, Reality Composer Pro displays: A manipulator over the entity in the 3D view. Any configurable values from the entity’s components in the inspector on the right. You can use the manipulator to move, rotate, and scale the selected entity. To move the selected entity around the 3D scene, drag the small colored cone that corresponds to the axis you want to move it along. Alternatively, you can drag the entity itself to move it freely relative to your viewing angle. To rotate the selected entity, click on the manipulator’s rotation control, which looks like a circle, and drag in a circular motion. Reality Composer Pro’s manipulator only shows one rotation control at a time. To rotate an entity on one of the other axes, click the cone corresponding to the axis you want to rotate. For example, if you want to rotate the entity on the X axis, tap the red cone to bring up the red rotation handle for that axis. To scale the selected entity uniformly, click the rotation circle and drag away from the entity origin to scale it up, or toward the entity origin to scale it down. Because it scales uniformly, it doesn’t matter which rotation handle Reality Composer Pro is showing. Note In the manipulator, Red indicates the X axis, Green indicates the Y axis, and Green indicates the Z axis. Alternatively, you can make the same changes to the selected entity by typing new values into the transform component of the inspector. The transform component stores the position, rotation, and scale for an entity. The manipulator is just a visual way to change the values on this component.  Reality Composer Pro scenes can get quite complex and sometimes contain overlapping entities, which can be difficult to work with. To simplify a scene, you can deactivate entities to remove them from the 3D view. Deactivate entities by Control-clicking them and selecting Deactivate from the contextual menu. The entity still exists in your project and is shown in the hierarchy view, albeit grayed out and without any child entities. It won’t, however, appear in the 3D view. Xcode doesn’t compile deactivated entities into your app’s bundle, so it’s important to re-activate any entities your app needs before saving your project. To reactivate an entity, Control-click the entity in the hierarchy view and select Activate from the contextual menu. RealityKit follows a design pattern called Entity-Component-System (ECS). In ECS, you store data on an entity using components and then implement entity behavior by writing systems that use the data from those components. You can add and configure components to entities in Reality Composer Pro, including both built-in components like ParticleEmitterComponent, and custom components that you write and place in the Sources folder of your Reality Composer Pro Swift package. You can also create new components in Reality Composer Pro and edit them in Xcode. For more information about ECS, see Understanding RealityKit’s modular architecture. To add a component to an entity, select that entity in the hierarchy view or 3D view. At the bottom-right of the inspector window, click Add Component. A list of available components appears with New Component at the top. If you select the first item, Reality Composer Pro creates a new component class in the Sources folder, and optionally a new system class. It also adds the component to the selected entity. If you select any other item in the list, it adds that component to the selected entity if it doesn’t already have that component.  Reality Composer Pro scenes are hierarchies of RealityKit entities. You can change the relationship between entities in the hierarchy browser except for parts of the hierarchy imported from a .usdz file, which Reality Composer Pro treats as a read-only file. To change the relationship between entities, or to create a relationship between two currently unrelated entities, use the hierarchy view and drag an entity onto the entity that you want it to be part of. If you want an entity to become a root entity, drag it to the Root transform at the top of the hierarchy view. When you import a USDZ model into Reality Composer Pro, it creates a RealityKit material for every physically-based rendering (PBR) material the asset contains. Reality Composer Pro displays materials in the hierarchy view just like it displays entities, except it uses a paintbrush icon. Reality Composer Pro doesn’t display materials in the 3D view. Note The library in Reality Composer Pro contains materials for several common real-world surfaces like metal, wood, and denim that you can import into your project. If you select a PBR material in the hierarchy view, you can edit it using the inspector. You can replace images, colors, or values for any of the PBR attributes with another image, color, or value of your choosing. Any changes you make to a material affects any entity that’s bound to that material. You can also create new materials from scratch by clicking the Add button (+) at the bottom of the scene hierarchy and choosing Material.  PBR materials are great at reproducing real-world surfaces. However, they can’t represent nonrealistic materials like cartoon shaders, and they can’t contain logic. This means that you can’t animate a PBR material or have it react to input from your app. Reality Composer Pro offers a second type of material called a custom material. You can build and edit custom materials using the Shader Graph tab. Shader Graph provides a tremendous amount of control over materials and allows you to do things that would otherwise require writing Metal shaders. For more information on writing Metal shaders, see Metal. Note RealityKit doesn’t represent Reality Composer Pro custom materials as an instance of CustomMaterial, as you might expect. Instead, RealityKit represents these materials as ShaderGraphMaterial instances.  The materials you build in the editor can affect both the look of an entity and its shape. If you build a node graph and connect it to the Custom Surface pin on the output node, that node graph controls the surface appearance of the model and roughly equates to writing Metal code in a fragment shader. If you build a node graph and connect it to the Custom Geometry Modifier output pin, those nodes control the shape of the entity, which equates to Metal code running in a vertex shader. Nodes represent values and operations and serve the same purpose as either a variable or constant, or a function in Metal. If you need the sine of a value, for example, connect the value’s output node to the input pin of a Sin node. Add new nodes to the graph by double-clicking the background of the Shader Graph view or click the New Node button on the right side of the screen. Important Some nodes, like Sin, are universal and can be used with either output pin. Other nodes are specific to either the Custom Surface or Geometry Modifier outputs. If a node name starts with Geometry Modifier, you can only connect it to the Geometry Modifier output pin. If the node’s name starts with “Surface”, you can only connect it to the Custom Surface output pin. To unlock the real power of Shader Graph, you need to be able to change values on the material from Swift code. Shader Graph allows you to do this by creating promoted inputs, which are parameters you can set and read from Swift to change your material at runtime. If you have a feature that you want to turn on and off, you might create a Boolean input parameter and have conditional logic based on its value. If you want to smoothly interpolate between two colors, you might create a Float input parameter and use it to control how to interpolate between the two colors. You can Control-click on a constant node and select Promote to turn it into a promoted input. You can also turn a promoted input back into a constant by Control-clicking it and selecting Demote. If you don’t have an existing constant to promote, you can create new promoted inputs using the inspector. The New Input button only shows up in the inspector when you select a material in the hierarchy view but have no nodes selected in the Shader Graph tab.  To change the value of an input parameter from Swift code, use setParameter(name:value:), passing the name of the parameter and the new value. Note that parameter names are case sensitive, so your name string must exactly match what you called the parameter in Shader Graph. For examples of Shader Graph use, see Diorama and Happy Beam. If your project has multiple scenes that share assets, you can use references to avoid creating duplicate assets. A reference acts like an alias in Finder — it points to the original asset and functions just as if it were another copy of that asset. Create references using the inspector. By default, the references section is hidden for entities and materials that don’t have any references. To add a new reference to an asset or material that doesn’t have one, choose Reality Composer Pro > Settings and uncheck Hide Empty References.  To add a reference, click the Add button (+) at the bottom of the references section in the inspector, choose the .usda file for the scene that contains the asset, then choose the asset you want to link to. After you do that, the selected entity or material becomes a copy of the one you linked to. Important If you make changes to a linked asset, those changes will affect every linked reference. If you have an Apple Vision Pro connected to your Mac, choose Preview > Play or click the preview button in the Reality Composer Pro toolbar to view your scene on device. The Preview button is the left-most button on the right side of the toolbar — the one with an Vision Pro icon. If you have multiple Vision Pro devices connected, choose which device to use by clicking the pull-down menu next to the Preview button. Loading a Reality Composer Pro scene is nearly identical to loading a USDZ asset from your app bundle, except you have to specify the Reality Composer Pro package bundle instead. You typically do this in the make closure of a RealityView initializer. Reality Composer Pro packages define a global constant that points to its bundle, which is named after the project with “Bundle” appended to it. In the default Xcode visionOS template, the Reality Composer Pro project is called RealityKitContent, so the global bundle variable is called realityKitContentBundle: Note The code above saves a reference to the root node. This isn’t required, but with RealityView, unlike ARView on iOS and macOS, you don’t have ready access to the scene content, so it’s often handy to maintain your own reference to the root entity of your scene in your app’s data model. When RealityKit finishes loading the scene, the scene variable contains the root entity of the scene you specified. Add it to content and RealityKit displays it to the user.       USDZ models do become an entity or entity hierarchy when you add them to a scene.
Image files do not become an entity. Reality Composer Pro only uses image assets indirectly, such as being the source texture for materials you build in Shader Graph. If you drag assets that Reality Composer Pro can’t turn into an entity, nothing happens. A manipulator over the entity in the 3D view.
Any configurable values from the entity’s components in the inspector on the right.
You can use the manipulator to move, rotate, and scale the selected entity.
To move the selected entity around the 3D scene, drag the small colored cone that corresponds to the axis you want to move it along. Alternatively, you can drag the entity itself to move it freely relative to your viewing angle.
To rotate the selected entity, click on the manipulator’s rotation control, which looks like a circle, and drag in a circular motion.
Reality Composer Pro’s manipulator only shows one rotation control at a time.
To rotate an entity on one of the other axes, click the cone corresponding to the axis you want to rotate. For example, if you want to rotate the entity on the X axis, tap the red cone to bring up the red rotation handle for that axis.
To scale the selected entity uniformly, click the rotation circle and drag away from the entity origin to scale it up, or toward the entity origin to scale it down. Because it scales uniformly, it doesn’t matter which rotation handle Reality Composer Pro is showing. Reality Composer Pro’s manipulator only shows one rotation control at a time.
To rotate an entity on one of the other axes, click the cone corresponding to the axis you want to rotate. For example, if you want to rotate the entity on the X axis, tap the red cone to bring up the red rotation handle for that axis. RealityKitContent RealityKitContent RealityKitContent.rkassets Package.realitycomposerpro Package.realitycomposerpro .reality .usda RealityView Scene Scene.usda .usda .usda .usda .rkassets .usda .usda .usda .reality .usda .usdc .usdz X ParticleEmitterComponent .usdz CustomMaterial ShaderGraphMaterial Sin Sin Float setParameter(name:value:) name .usda make RealityView RealityKitContent realityKitContentBundle RealityView { content in
    if let scene = try? await Entity.load(named: ""Biplane"", 
                                          in: realityKitContentBundle) {
        myDataModel.add(scene) 
        content.add(scene)
    }
} update: { content in
    // ...
} RealityView ARView scene content "
13,"Happy Beam  Overview Design the game interface in SwiftUI Detect a heart gesture with ARKit Support several kinds of input Display 3D content with RealityKit Add SharePlay support for multiplayer gaming experiences See Also       Related samples Related articles Related videos                                In visionOS, you can create fun, dynamic games and apps using several different frameworks to create new kinds of spatial experiences: RealityKit, ARKit, SwiftUI, and Group Activities. This sample introduces Happy Beam, a game where you and your friends can hop on a FaceTime call and play together. You’ll learn the mechanics of the game where grumpy clouds float around in the space, and people play by making a heart shape with their hands to project a beam. People aim the beam at the clouds to cheer them up, and a score counter keeps track of how well each player does cheering up the clouds. Most apps in visionOS launch as a window that opens different scene types depending on the needs of the app. Here you see how Happy Beam presents a fun interface to people by using several SwiftUI views that display a welcome screen, a coaching screen that gives instructions, a scoreboard, and a game-ending screen.     The following shows you the primary view in the app that displays each phase of gameplay: When 3D content starts to appear, the game opens an immersive space to present content outside of the main window and in a person’s surroundings. The HappyBeam container view declares a dependency on openImmersiveSpace: It later uses that dependency to open the space from the app’s declaration when it’s time to start showing 3D content: The Happy Beam app recognizes the central heart-shaped hands gesture using ARKit’s support for 3D hand tracking in visionOS. Using hand tracking requires a running session and authorization from the wearer. It uses the NSHandsTrackingUsageDescription user info key to explain to players why the app requests permission for hand tracking.  Hand-tracking data isn’t available when your app is only displaying a window or volume. Instead, it’s available when you present an immersive space, as in the previous example. You can detect gestures using ARKit data with a level of accuracy that depends on your use case and intended experience. For example, Happy Beam could require strict positioning of finger joints to closely resemble a heart shape. Instead, however, it prompts people to make a heart shape and uses a heuristic to indicate when the gesture is close enough. The following checks whether a person’s thumbs and index fingers are almost touching: To support accessibility features and general user preferences, include multiple kinds of input in an app that uses hand tracking as one form of input. Happy Beam supports several kinds of input: Interactive hands input from ARKit with the custom heart gesture. Drag gesture input to rotate the stationary beam on its platform. Accessibility components from RealityKit to support custom actions for cheering up the clouds. Game Controller support to make control over the beam more interactive from Switch Control. The 3D content in the app comes in the form of assets that you can export from Reality Composer Pro. You place each asset in the RealityView that represents your immersive space. The following shows how Happy Beam generates clouds when the game starts, as well as materials for the floor-based beam projector. Because the game uses collision detection to keep score — the beam cheers up grumpy clouds when they collide — you make collision shapes for each model that might be involved. You use the Group Activities framework in visionOS to support interactive experiences in a shared activity during a FaceTime call. Happy Beam uses Group Activities to sync the score, active players list, and the position of each player’s projected beam. Use a reliable channel to send information that’s important to be correct, even if it can be slightly delayed as a result. The following shows how Happy Beam updates the game model’s score state in response to a score message: Use an unreliable messenger for sending data with low-latency requirements. Because the delivery mode is unreliable, some messages might not make it. Happy Beam uses the unreliable mode to send live updates to the position of the beam when each participant in the call chooses the Spatial option in FaceTime. The following shows how Happy Beam serializes beam data for each message:        struct HappyBeam: View {
    @Environment(\.openImmersiveSpace) private var openImmersiveSpace
    @EnvironmentObject var gameModel: GameModel
    
    @State var session: GroupSession<HeartProjection>? = nil
    @State var timer = Timer.publish(every: 1, on: .main, in: .common).autoconnect()
    @State var subscriptions = Set<AnyCancellable>()
    
    var body: some View {
        let gameState = GameScreen.from(state: gameModel)
        VStack {
            Spacer()
            Group {
                switch gameState {
                case .start:
                    Start()
                case .soloPlay:
                    SoloPlay()
                case .lobby:
                    Lobby()
                case .soloScore:
                    SoloScore()
                case .multiPlay:
                    MultiPlay()
                case .multiScore:
                    MultiScore()
                }
            }
            .glassBackgroundEffect(
                in: RoundedRectangle(
                    cornerRadius: 32,
                    style: .continuous
                )
            )
        }
    }
} @main
struct HappyBeamApp: App {
    @StateObject var gameModel = GameModel()
    @State var immersionState: ImmersionStyle = .mixed
    
    var body: some SwiftUI.Scene {
        WindowGroup(""HappyBeam"", id: ""happyBeamApp"") {
            HappyBeam()
                .environmentObject(gameModel)
        }
        .windowStyle(.plain)
        
        ImmersiveSpace(id: ""happyBeam"") {
            HappyBeamSpace(gestureModel: HeartGestureModelContainer.heartGestureModel)
                .environmentObject(gameModel)
        }
        .immersionStyle(selection: $immersionState, in: .mixed)
    }
} HappyBeam openImmersiveSpace @Environment(\.openImmersiveSpace) private var openImmersiveSpace if gameModel.countDown == 0 {
    Task {
        await openImmersiveSpace(id: ""happyBeam"")
    }
} NSHandsTrackingUsageDescription Task {
    do {
        try await session.run([handTrackingProvider])
    } catch {
        print(""ARKitSession error:"", error)
    }
} func computeTransformOfUserPerformedHeartGesture() -> simd_float4x4? {
    // Get the latest hand anchors and return false if either of them isn't tracked.
    guard let leftHandAnchor = latestHandTracking.left,
          let rightHandAnchor = latestHandTracking.right,
          leftHandAnchor.isTracked, rightHandAnchor.isTracked else {
        return nil
    }
    
    // Get all the required joints and check whether they're tracked.
    let leftHandThumbKnuckle = leftHandAnchor.skeleton.joint(named: .handThumbKnuckle)
    let leftHandThumbTipPosition = leftHandAnchor.skeleton.joint(named: .handThumbTip)
    let leftHandIndexFingerTip = leftHandAnchor.skeleton.joint(named: .handIndexFingerTip)
    let rightHandThumbKnuckle = rightHandAnchor.skeleton.joint(named: .handThumbKnuckle)
    let rightHandThumbTipPosition = rightHandAnchor.skeleton.joint(named: .handThumbTip)
    let rightHandIndexFingerTip = rightHandAnchor.skeleton.joint(named: .handIndexFingerTip)
    
    guard leftHandIndexFingerTip.isTracked && leftHandThumbTipPosition.isTracked &&
            rightHandIndexFingerTip.isTracked && rightHandThumbTipPosition.isTracked &&
            leftHandThumbKnuckle.isTracked &&  rightHandThumbKnuckle.isTracked else {
        return nil
    }
    
    // Get the position of all joints in world coordinates.
    let leftHandThumbKnuckleWorldPosition = matrix_multiply(leftHandAnchor.transform, leftHandThumbKnuckle.rootTransform).columns.3.xyz
    let leftHandThumbTipWorldPosition = matrix_multiply(leftHandAnchor.transform, leftHandThumbTipPosition.rootTransform).columns.3.xyz
    let leftHandIndexFingerTipWorldPosition = matrix_multiply(leftHandAnchor.transform, leftHandIndexFingerTip.rootTransform).columns.3.xyz
    let rightHandThumbKnuckleWorldPosition = matrix_multiply(rightHandAnchor.transform, rightHandThumbKnuckle.rootTransform).columns.3.xyz
    let rightHandThumbTipWorldPosition = matrix_multiply(rightHandAnchor.transform, rightHandThumbTipPosition.rootTransform).columns.3.xyz
    let rightHandIndexFingerTipWorldPosition = matrix_multiply(rightHandAnchor.transform, rightHandIndexFingerTip.rootTransform).columns.3.xyz
    
    let indexFingersDistance = distance(leftHandIndexFingerTipWorldPosition, rightHandIndexFingerTipWorldPosition)
    let thumbsDistance = distance(leftHandThumbTipWorldPosition, rightHandThumbTipWorldPosition)
    
    // Heart gesture detection is true when the distance between the index fingertips' centers
    // and the distance between the thumb tips' centers is each less than four centimeters.
    let isHeartShapeGesture = indexFingersDistance < 0.04 && thumbsDistance < 0.04
    if !isHeartShapeGesture {
        return nil
    }
    
    // Computes a position in the middle of the heart gesture.
    let halfway = (rightHandIndexFingerTipWorldPosition - leftHandThumbTipWorldPosition)/2
    let heartMidpoint = rightHandIndexFingerTipWorldPosition - halfway
    
    // Computes the vector from the left thumb knuckle to the right thumb knuckle and normalizes (x-axis).
    let xAxis = normalize(rightHandThumbKnuckleWorldPosition - leftHandThumbKnuckleWorldPosition)
    
    // Computes the vector from the right thumb tip to the right index fingertip and normalizes (y-axis).
    let yAxis = normalize(rightHandIndexFingerTipWorldPosition - rightHandThumbTipWorldPosition)
    
    let zAxis = normalize(cross(xAxis, yAxis))
    
    // Creates the final transform for the heart gesture from the three axes and midpoint vector.
    let heartMidpointWorldTransform = simd_matrix(SIMD4(xAxis.x, xAxis.y, xAxis.z, 0), SIMD4(yAxis.x, yAxis.y, yAxis.z, 0), SIMD4(zAxis.x, zAxis.y, zAxis.z, 0), SIMD4(heartMidpoint.x, heartMidpoint.y, heartMidpoint.z, 1))
    return heartMidpointWorldTransform
} RealityView @MainActor
func placeCloud(start: Point3D, end: Point3D, speed: Double) async throws -> Entity {
    let cloud = await loadFromRealityComposerPro(
        named: BundleAssets.cloudEntity,
        fromSceneNamed: BundleAssets.cloudScene
    )!
        .clone(recursive: true)
    
    cloud.generateCollisionShapes(recursive: true)
    cloud.components[PhysicsBodyComponent.self] = PhysicsBodyComponent()
    
    var accessibilityComponent = AccessibilityComponent()
    accessibilityComponent.label = ""Cloud""
    accessibilityComponent.value = ""Grumpy""
    accessibilityComponent.isAccessibilityElement = true
    accessibilityComponent.traits = [.button, .playsSound]
    accessibilityComponent.systemActions = [.activate]
    cloud.components[AccessibilityComponent.self] = accessibilityComponent
    
    let animation = cloudMovementAnimations[cloudPathsIndex]
    
    cloud.playAnimation(animation, transitionDuration: 1.0, startsPaused: false)
    cloudAnimate(cloud, kind: .sadBlink, shouldRepeat: false)
    spaceOrigin.addChild(cloud)
    
    return cloud
} sessionInfo.reliableMessenger = GroupSessionMessenger(session: newSession, deliveryMode: .reliable)


Task {
    for await (message, sender) in sessionInfo!.reliableMessenger!.messages(of: ScoreMessage.self) {
        gameModel.clouds[message.cloudID].isHappy = true
        gameModel
            .players
            .filter { $0.name == sender.source.id.asPlayerName }
            .first!
            .score += 1
    }
} sessionInfo.messenger = GroupSessionMessenger(session: newSession, deliveryMode: .unreliable) // Send each player's beam data during FaceTime calls where players have selected the Spatial option.
func sendBeamPositionUpdate(_ pose: Pose3D) {
    if let sessionInfo = sessionInfo, let session = sessionInfo.session, let messenger = sessionInfo.messenger {
        let everyoneElse = session.activeParticipants.subtracting([session.localParticipant])
        
        if isShowingBeam, gameModel.isSpatial {
            messenger.send(BeamMessage(pose: pose), to: .only(everyoneElse)) { error in
                if let error = error { print(""Message failure:"", error) }
            }
        }
    }
} "
14,"Setting up access to ARKit data  Overview See Also        Add usage descriptions for ARKit data access Choose between up-front or as-needed authorization Open a space and run a session Provide alternatives for declined and revoked authorizations ARKit                                In visionOS, ARKit can enable new kinds of experiences that leverage data such as hand tracking and world sensing. The system gates access to this kind of sensitive information. Because people can decline your app’s request to use ARKit data or revoke access later, you need to provide alternative ways to use your app and to handle cases where your app loses access to data.  People need to know why your app wants to access data from ARKit. Add the following keys to your app’s information property list to provide a user-facing usage description that explains how your app uses the data: Use this key if your app uses hand tracking. Use this key if your app uses image tracking, plane detection, or scene reconstruction. Note World tracking — unlike world sensing — doesn’t require authorization. For more information, see Tracking specific points in world space. You can choose when someone sees an authorization request to use ARKit data. If you need precise control over when the request appears, call the requestAuthorization(for:) method on ARKitSession to explicitly authorize access at the time you call it. Otherwise, people see an authorization request when you call the run(_:) method. This is an implicit authorization because the timing of the request depends entirely on when you start the session. To help protect people’s privacy, ARKit data is available only when your app presents a Full Space and other apps are hidden. Present one of these space styles before calling the run(_:) method. The following shows an app structure that’s set up to use a space with ARKit: Call doc://com.apple.documentation/documentation/swiftui/environmentvalues/openimmersivespace from your app’s user interface to create a space, start running an ARKit session, and kick off an immersive experience. The following shows a simple view with a button that opens the space: Someone might not want to give your app access to data from ARKit, or they might choose to revoke that access later in Settings. Handle these situations gracefully, and remove or transition content that depends on ARKit data. For example, you might fade out content that you need to remove, or recenter content to an appropriate starting position. If your app uses ARKit data to place content in a person’s surroundings, consider letting people place content using the system-provided interface. Providing alternatives is especially important if you’re using ARKit for user input. People using accessibility features, trackpads, keyboards, or other forms of input might need a way to use your app without ARKit.       NSHandsTrackingUsageDescription NSWorldSensingUsageDescription requestAuthorization(for:) ARKitSession run(_:) run(_:) @main
struct MyApp: App {
    @State var session = ARKitSession()
    @State var immersionState: ImmersionStyle = .mixed
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
        ImmersiveSpace(id: ""appSpace"") {
            MixedImmersionView()
            .task {
                let planeData = PlaneDetectionProvider(alignments: [.horizontal])
                
                if PlaneDetectionProvider.isSupported {
                    do {
                        try await session.run([planeData])
                        for await update in planeData.anchorUpdates {
                            // Update app state.
                        }
                    } catch {
                        print(""ARKit session error \(error)"")
                    }
                }
            }
        }
        .immersionStyle(selection: $immersionState, in: .mixed)
    }
} struct ContentView: View {
    @Environment(\.openImmersiveSpace) private var openImmersiveSpace
    
    var body: some View {
        Button(""Start ARKit experience"") {
            Task {
                await openImmersiveSpace(id: ""appSpace"")
            }
        }
    }
} "
15,"Placing content on detected planes  Overview See Also        Use RealityKit anchor entities for basic plane anchoring Configure an ARKit session for plane detection Create and update entities associated with each plane ARKit                                Flat surfaces are an ideal place to position content in an app that uses a Full Space in visionOS. They provide a place for virtual 3D content to live alongside a person’s surroundings. Use plane detection in ARKit to detect these kinds of surfaces and filter the available planes based on criteria your app might need, such as the size of the plane, its proximity to someone, or a required plane orientation. If you don’t need a specific plane in your app and you’re rendering your app’s 3D content in RealityKit, you can use an AnchorEntity instead. This approach lets you attach 3D content to a plane without prompting the person for world-sensing permission and without any particular knowledge of where that plane is relative to the person. The following shows an anchor that you can use to attach entities to a table: Anchor entities don’t let you choose a specific plane in a person’s surroundings, but rather let you ask for a plane with certain characteristics. When you need more specific plane selection or real-time information about the plane’s position and orientation in the world, use ARKitSession and PlaneDetectionProvider. Plane-detection information comes from an ARKitSession that’s configured to use a PlaneDetectionProvider. You can choose to detect horizontal planes, vertical planes, or both. Each plane that ARKit detects comes with a classification, like PlaneAnchor.Classification.table or PlaneAnchor.Classification.floor. You can use these classifications to further refine which kinds of planes your app uses to present content. Plane detection requires ARKitSession.AuthorizationType.worldSensing authorization. The following starts a session that detects both horizontal and vertical planes, but filters out planes classified as windows: If you’re displaying content that needs to appear attached to a particular plane, update your content whenever you receive new information from ARKit. When a plane is no longer available in the person’s surroundings, ARKit sends a removal event. Respond to these events by removing content associated with the plane. The following shows plane updates that place a text entity on each plane in a person’s surroundings; the text entity displays the kind of plane ARKit detected:       AnchorEntity AnchorEntity(plane: .horizontal, classification: .table) ARKitSession PlaneDetectionProvider ARKitSession PlaneDetectionProvider PlaneAnchor.Classification.table PlaneAnchor.Classification.floor ARKitSession.AuthorizationType.worldSensing let session = ARKitSession()
let planeData = PlaneDetectionProvider(alignments: [.horizontal, .vertical])


Task {
    try await session.run([planeData])
    
    for await update in planeData.anchorUpdates {
        // Skip planes that are windows.
        if update.anchor.classification == .window { continue }
        
        switch update.event {
        case .added, .updated:
            updatePlane(update.anchor)
        case .removed:
            removePlane(update.anchor)
        }
    }
} var entityMap: [UUID: Entity] = [:]


func updatePlane(_ anchor: PlaneAnchor) {
    if entityMap[anchor.id] == nil {
        // Add a new entity to represent this plane.
        let entity = ModelEntity(
            mesh: .generateText(anchor.classification.description)
        )
        
        entityMap[anchor.id] = entity
        rootEntity.addChild(entity)
    }
    
    entityMap[anchor.id]?.transform = Transform(matrix: anchor.transform)
}


func removePlane(_ anchor: PlaneAnchor) {
    entityMap[anchor.id]?.removeFromParent()
    entityMap.removeValue(forKey: anchor.id)
} "
16,"Incorporating real-world surroundings in an immersive experience  Overview See Also        Configure a scene reconstruction session Add real-world interactivity using collision components ARKit                                Scene reconstruction helps bridge a gap between the rendered 3D content in your app and the person’s surroundings. Use scene reconstruction in ARKit to give your app an idea of the shape of the person’s surroundings and to bring your app experience into their world. Immersive experiences — those that use the doc://com.apple.documentation/documentation/swiftui/immersionstyle/mixed space style — are best positioned to incorporate this kind of contextual information: scene reconstruction is only available in spaces and isn’t as relevant for the doc://com.apple.documentation/documentation/swiftui/immersionstyle/full space style. In addition to providing a 3D mesh of the shape of different nearby objects, ARKit gives a classification to each mesh face it detects. For example, it might classify a face of a mesh as being part of an appliance, a piece of furniture, or structural information about the room like the position of walls and floors. Scene reconstruction requires the ARKitSession.AuthorizationType.worldSensing authorization type and corresponding usage description that you supply in your app’s Info.plist file. The following creates a session and lists anchors as ARKit updates its reconstruction of the person’s surroundings: You can make rendered 3D content more lifelike by having it appear to interact physically with objects in the person’s surroundings, like furniture and floors. RealityKit’s collision components and physics support make it possible to provide these interactions in your app. The generateStaticMesh(from:) method bridges between scene reconstruction and RealityKit’s physics simulation. Warning Be mindful of how much content you include in immersive scenes that use the doc://com.apple.documentation/documentation/swiftui/immersionstyle/mixed style. Content that fills a significant portion of the screen, even if that content is partially transparent, can prevent the person from seeing potential hazards in their surroundings. If you want to immerse the person in your content, configure your space with the doc://com.apple.documentation/documentation/swiftui/immersionstyle/full style. For more information, see Creating fully immersive experiences in your app. Use low-priority tasks to generate meshes, because generating them is a computationally expensive operation. The following creates a mesh entity with collision shapes using scene reconstruction: Note Scene reconstruction meshes only support the PhysicsBodyMode.static physics body component mode. Each object in the scene reconstruction mesh updates its transform information independently and requires a separate static mesh because ARKit subdivides its representation of the world into multiple, distinct sections.       ARKitSession.AuthorizationType.worldSensing Info.plist let session = ARKitSession()
let sceneInfo = SceneReconstructionProvider(modes: [.classification])


Task {
    try await session.run([sceneInfo])
    
    for await update in sceneInfo.anchorUpdates {
        print(""Updated a portion of the scene: "", update.anchor)
        // Update app with new surroundings.
    }
} generateStaticMesh(from:) let meshConstructionTask = Task(priority: .low) {
    let shape = await ShapeResource.generateStaticMesh(from: update.anchor)


    // Call synchronous RealityKit methods from the main actor.
    await MainActor.run {
        let entity = Entity()
        entity.components[CollisionComponent.self] = .init(shapes: [shape])
        entity.components[PhysicsBodyComponent.self] = .init(
            massProperties: .default,
            material: nil,
            mode: .static
        )
        rootEntity.addChild(entity)
    }
} PhysicsBodyMode.static transform "
17,"Tracking specific points in world space  Overview See Also       Start an ARKit session with world tracking Create and add world anchors Persist world anchors across sessions Track the device position in the world ARKit                                Use world anchors along with an ARKit session’s WorldTrackingProvider to track points of interest in the world over time, as a person moves while wearing the device, and across device usage sessions. For example, someone might place a 3D object in a specific position on their desk and expect it to come back the next time they use the device. ARKit keeps track of a unique identifier for each world anchor your app creates and automatically places those anchors back in the space when the person returns to your app in the same location. A world tracking provider also provides the position of the device the person is wearing. Use an ARKitSession configured for world tracking to start receiving updates on the world anchors your app places. The following shows updates to world anchors your app previously registered using the addAnchor(_:) method: Important If a person repositions the current space — for example, by holding down the Digital Crown — world anchor updates begin updating their position relative to the new world origin. For example, a world anchor placed on a table still reports information about the table’s position, but those positions are relative to the updated world origin. You can create world anchors for any point of interest in your app’s world coordinate system once you’ve started a world tracking ARKit session. For example, you might track that a person placed an item at a particular offset from a desk in their space: Once you add a world anchor to your app’s tracking provider using the addAnchor(_:) method, the anchorUpdates sequence in the current session and future runs of your app provides updates to the current position of that new world anchor. The only information ARKit persists about the world anchors in your app is their UUID — a WorldAnchor instance’s id property — and pose in a particular space. It’s your app’s responsibility to persist additional information, such as the meaning of each anchor. For example, you might save local data about a custom 3D lamp model that a person placed on their desk. As a person moves from town-to-town or room-to-room, your app won’t receive all of the world anchor updates from each place someone used your app. Instead, the anchorUpdates sequence only provides world anchors for nearby objects. Use the Compositor Services framework and the WorldTrackingProvider class’s queryPose(atTimestamp:) method to get low-latency information about the current and future-predicted pose of the person’s device in world space. For more information, see Drawing fully immersive content using Metal.       WorldTrackingProvider ARKitSession addAnchor(_:) let session = ARKitSession()
let worldInfo = WorldTrackingProvider()


Task {
    try await session.run([worldInfo])
    
    for await update in worldInfo.anchorUpdates {
        switch update.event {
        case .added, .updated:
            // Update the app's understanding of this world anchor.
            print(""Anchor position updated."")
        case .removed:
            // Remove content related to this anchor.
            print(""Anchor position now unknown."")
    }
} let anchor = WorldAnchor(transform: deskPlane.transform + offset)
try await worldInfo.addAnchor(anchor) addAnchor(_:) anchorUpdates UUID WorldAnchor id anchorUpdates WorldTrackingProvider queryPose(atTimestamp:) "
18,"Tracking preregistered images in 3D space  Overview See Also       ARKit                                Use ARKit’s support for tracking 2D images to place 3D content in a space. ARKit provides updates to the image’s location as it moves relative to the person. If you supply one or more reference images in your app’s asset catalog, people can use a real-world copy of that image to place virtual 3D content in your app. For example, if you design a pack of custom playing cards and provide those assets to people in the form of a real-world deck of playing cards, they can place unique content per card in a fully immersive experience. The following example tracks a set of images loaded from an app’s asset catalog: If you know the real-world dimensions of the images you’re tracking, use the physicalSize property to improve tracking accuracy. The estimatedScaleFactor property provides information about how the scale of the tracked image differs from the expected physical size you provide.       let session = ARKitSession()
let imageInfo = ImageTrackingProvider(
    referenceImages: ReferenceImage.loadReferenceImages(inGroupNamed: ""playingcard-photos"")
)


if ImageTrackingProvider.isSupported {
    Task {
        try await session.run([imageInfo])
        for await update in imageInfo.anchorUpdates {
            updateImage(update.anchor)
        }
    }
}


func updateImage(_ anchor: ImageAnchor) {
    if imageAnchors[anchor.id] == nil {
        // Add a new entity to represent this image.
        let entity = ModelEntity(mesh: .generateSphere(radius: 0.05))
        entityMap[anchor.id] = entity
        rootEntity.addChild(entity)
    }
    
    if anchor.isTracked {
        entityMap[anchor.id]?.transform = Transform(matrix: anchor.transform)
    }
} physicalSize estimatedScaleFactor "
19,"Destination Video  Overview See Also        Play video in an inline player Play video in a fullscreen player Configure the spatial audio experience Present an immersive space Provide a shared viewing experience Related samples Related articles Related videos                                Destination Video is a multiplatform video-playback app for visionOS, iOS, and tvOS. People get a familiar media-browsing experience navigating the libraryʼs content and playing videos they find interesting. The app provides a similar experience on supported platforms, but leverages unique features of visionOS to create a novel, immersive playback experience. When you select a video in the library, Destination Video presents a view that displays additional details about the item. The view presents controls to play the video and specify whether to include it in your Up Next list. In visionOS, it also displays a video poster along its leading edge. Tapping the view’s Preview button displays an inline preview of the video. When you present an AVPlayerViewController object’s interface as a child of another view, inline controls display, for example, pause, skip, and seek. Showing standard playback controls in your app provides a familiar UI that automatically adapts its appearance to fit each platform, and is the recommended choice in most cases. Destination Video uses a simple UI for the inline player view: a single button that toggles state of playback, and replays the content when it reaches the item’s end. AVPlayerViewController doesn’t provide this controls style, but the app uses it to display the video content without controls by setting the value of its showsPlaybackControls property to false. It then overlays the custom playback controls it requires. See the Destination Video’s InlinePlayerView type for details on how you can implement this. Note AVPlayerViewController only supports displaying 2D content when embedded inline. Present the player fullscreen to play 3D video. One of the most exciting features of visionOS is its ability to play 3D video along with spatial audio, which adds a deeper level of immersion to the viewing experience. Playing 3D content in your app requires that you display AVPlayerViewController in its full screen presentation. When you present the player fullscreen, the system automatically docks it into the ideal viewing position, and presents streamlined playback controls that keep the person’s focus on the content. Note In iOS or tvOS, you typically present video in a full-screen presentation using the fullScreenCover(isPresented:onDismiss:content:) modifier. You can present the player this way in visionOS, but the recommended way to present it fullscreen is to set it as the root view of your app’s window group. Destination Video’s ContentView displays the library view by default. It observes changes to the player model’s presentation property, which indicates whether the app requests inline or fullscreen playback. When the presentation state changes to fullScreen, the view redraws the UI to display the player view in place of the library: When someone selects the Play Video button on the detail screen, this calls the player model’s loadVideo(_: presentation:) method requesting the fullScreen presentation option: After the player model successfully loads the video content for playback, it updates its presentation value to fullScreen, which causes the app to replace the library with PlayerView. To dismiss the fullscreen player in visionOS, people tap the back button in the player UI. To handle this action, the app’s FullScreenPlayerView type defines an AVPlayerViewControllerDelegate object that handles the dismissal: When the delegate receives this call, it clears the media from the player model and resets the presentation state back to its default value, which results in the Destination Video app redisplaying the library view. Media playback apps require common configuration of their capabilities and audio session. In addition to performing the steps outlined in Configuring your app for media playback, Destination Video also adopts new AVAudioSession API to customize the a person’s spatial audio experience. After the app successfully loads a video for playback, it configures the spatial audio experience for the current presentation. For the inline player view, it sets the experience to a small front-focused sound stage where the audio comes from a person’s perception of front. When displaying a video full screen, it specifies automatic settings that let the system optimize the experience to best fit the video presentation. Building video playback apps for visionOS provides new opportunities to enhance the viewing experience beyond the bounds of the player window. To add a greater level of immersion, the sample presents an immersive space that displays a scene around a person as they watch the video. It defines the immersive space in the DestinationVideo app structure: The immersive space presents an instance of DestinationView, which maps a texture to the inside of a sphere that it displays around a person. The app presents it using the .progressive immersion style, which lets someone change their amount of immersion by turning the Digital Crown on the device. The Destination Video app automatically presents the immersive space when a person navigates to a video’s detail view, and dismisses it when they return to the library. To monitor these events, the app observes its navigation path to determine when a navigation event occurs so it can show or dismiss the space: One of the best ways to enhance your app’s playback experience is to make that experience shareable with others. You can use the AVFoundation and the GroupActivities frameworks to build SharePlay experiences that bring people together even when they can’t be in the same location. The Destination Video app creates an experience where people can watch videos with others across devices and platforms. It defines a group activity called VideoWatchingActivity that adopts the GroupActivity protocol. When people have a FaceTime call active and they play a video in the fullscreen player, it becomes eligible for playback for everyone on the call. The app’s VideoWatchingCoordinator actor manages Destination Video’s SharePlay functionality. It observes the activation of new VideoWatchingActivity sessions and when one starts, it sets the GroupSession instance on the player object’s AVPlaybackCoordinator: With the player configured to use the group session, when the app loads new videos, they become eligible to share with people in the FaceTime call.       AVPlayerViewController AVPlayerViewController showsPlaybackControls false InlinePlayerView AVPlayerViewController AVPlayerViewController fullScreenCover(isPresented:onDismiss:content:) ContentView presentation fullScreen struct ContentView: View {
    
    /// The library's selection path.
    @State private var selectionPath = [Video]()
    @EnvironmentObject private var player: PlayerModel
    
    var body: some View {
        switch player.presentation {
        case .fullScreen:
            PlayerView()
                .onAppear {
                    player.play()
                }
        default:
            LibraryView(path: $selectionPath)
                // Set a specific frame size in case the user resizes the video-player window.
                .frame(width: 960, height: 540)
        }
    }
} loadVideo(_: presentation:) fullScreen Button {
    /// Load the media item for full-screen presentation.
    player.loadVideo(video, presentation: .fullScreen)
} label: {
    Label(""Play Video"", systemImage: ""play.fill"")
} presentation fullScreen PlayerView FullScreenPlayerView AVPlayerViewControllerDelegate func playerViewController(_ playerViewController: AVPlayerViewController,
                          willEndFullScreenPresentationWithAnimationCoordinator coordinator: UIViewControllerTransitionCoordinator) {
  // Resets the player state, which dismisses the player view.
  player.reset()
} AVAudioSession /// Configures people's intended spatial audio experience to best fit the presentation.
func configureSpatialExperience(for presentation: Presentation) {
    do {
        let experience: AVAudioSessionSpatialExperience
        switch presentation {
        case .inline:
            // Set a small, front-focused experience when watching trailers.
            experience = .headTracked(soundStageSize: .small, anchoringStrategy: .front)
        case .fullScreen:
            // Set a large sound stage size when viewing fullscreen.
            experience = .headTracked(soundStageSize: .automatic, anchoringStrategy: .automatic)
        }
        try AVAudioSession.sharedInstance().setIntendedSpatialExperience(experience)
    } catch {
        logger.error(""Unable to set the intended spatial experience. \(error.localizedDescription)"")
    }
} DestinationVideo struct DestinationVideo: App {
    
    var body: some Scene {
        // The app's primary window.
        WindowGroup {
            ContentView()
        }


        // Defines an immersive space to present a destination in which to watch the video.
        ImmersiveSpace(for: Destination.self) { $destination in
            if let destination {
                DestinationView(destination)
            }
        }
        // Set the immersion style to progressive, so the person can use the Digital Crown to dial in their experience.
        .immersionStyle(selection: .constant(.progressive), in: .progressive)
    }
} DestinationView .progressive .onChange(of: navigationPath) {
    Task {
        // The selection path becomes empty when the person returns to the main library window.
        if navigationPath.isEmpty {
            if isSpaceOpen {
                // Dismiss the space and return the person to their real-world space.
                await dismissSpace()
                isSpaceOpen = false
            }
        } else {
            // The navigationPath has one video, or is empty.
            guard let video = navigationPath.first else { fatalError() }
            // Await the request to open the destination and set the state accordingly.
            switch await openSpace(value: video.destination) {
            case .opened: isSpaceOpen = true
            default: isSpaceOpen = false
            }
        }
    }
} VideoWatchingActivity GroupActivity VideoWatchingCoordinator VideoWatchingActivity GroupSession AVPlaybackCoordinator private var groupSession: GroupSession<VideoWatchingActivity>? {
    didSet {
        guard let groupSession else { return }
        // Set the group session on the AVPlayer object's's playback coordinator.
        // so it can synchronize playback with other devices.
        playbackCoordinator.coordinateWithSession(groupSession)
    }
} "
20,"Configuring your app for media playback  Overview    Configure the audio session Configure the background modes         When you build media playback apps for iOS, tvOS, and visionOS, you need to do additional configuration to enable the expected playback behavior. Configuring the audio experience and background operations helps ensure that your app’s audio works as intended. It also enables advanced features like AirPlay streaming and Picture in Picture playback on supported platforms. Apple platforms, other than macOS which primarily leaves control to an app, provide an audio experience that the operating system manages. This enables the OS to provide a seamless audio experience to people as they switch between apps and receive high-priority audio requests such as phone or FaceTime calls. Your app uses an AVAudioSession to configure its audio behavior semantically, for example to have a primary purpose of playback or recording. You delegate the management of those details to the audio session, which ensures that the operating system can best manage a person’s audio experience. Your app automatically has an audio session that the system configures with this default behavior: Audio playback in your app silences other background audio Support for audio playback but not audio recording Ring/Silent switch set to silent mode in iOS silences app audio Locked device in iOS silences app audio The default audio session provides useful behavior, but typically doesn’t provide the experience and features you need when building a playback app. To add the required behavior, configure your app’s audio session category. An audio session category defines the general audio behavior your app requires. AVFoundation defines several audio session categories you can use, but the one most relevant for media playback apps is playback. This category indicates that media playback is a central feature of your app. When you specify this category, the system doesn’t silence your app’s audio when someone sets the Ring/Silent switch to silent mode in iOS only. Enabling this category means your app can play background audio if you’re using the Audio, AirPlay, and Picture in Picture background mode as explained in the section below. Use an AVAudioSession object to configure your app’s audio session. An audio session is a singleton object you use to set the audio session category, mode, and other settings. To configure the audio session for optimized playback of movies: To enable this category, activate the audio session using the setActive(_:options:) method. Note You can activate the audio session at any time after setting its category, but it’s recommended to defer this call until your app begins audio playback. Deferring the call ensures that you don’t prematurely interrupt any other background audio that may be in progress. Setting the category is the minimal interaction with an audio session, but other configuration options and features are available. For example, in visionOS, you customize a user’s spatial audio experience by configuring the audio session. For more information, see AVAudioSession. The system requires you to enable certain capabilities to perform some background operations. A common capability that playback apps require is playing background audio. With this capability enabled, your app’s audio continues when people switch to another app or lock their iOS device. Your app also needs this capability to enable advanced playback features like AirPlay streaming and Picture in Picture playback on supported platforms. Use Xcode to configure this capability: Select your app’s target in Xcode and select the Signing & Capabilities tab. Click the + Capability button and add the Background Modes capability to the project. In the Background Modes interface, select the Audio, AirPlay, and Picture in Picture option under its list of background modes.  With this mode enabled and your audio session configured, your app is ready to play background audio. In iOS, when you enable this option, your app can stream its content over AirPlay, and in iOS and tvOS it can use Picture in Picture playback.        Audio playback in your app silences other background audio
Support for audio playback but not audio recording
Ring/Silent switch set to silent mode in iOS silences app audio
Locked device in iOS silences app audio AVAudioSession playback AVAudioSession category mode class PlayerModel: ObservableObject {
    
    func configureAudioSession() {
        do {
            let session = AVAudioSession.sharedInstance()
            // Configure the app for playback of long-form movies.
            try session.setCategory(.playback, mode: .moviePlayback)
        } catch {
            // Handle error.
        }
    }
} setActive(_:options:) AVAudioSession "
21,"Adopting the system player interface in visionOS  Overview See Also        Explore presentation options Display supporting metadata Display custom informational views Present actions in the Info tab Display actions contextually visionOS Playback                                The recommended way to provide a video playback interface for your visionOS app is to adopt AVPlayerViewController. Using this class makes it simple to provide the same playback user interface and features found in system apps like TV and Music. It also provides essential system integration to deliver an optimal viewing experience whether you’re playing standard 2D content or immersive 3D video with spatial audio. This article describes best practices for presenting the player in visionOS and covers the options the player provides to customize its user interface to best fit your app. Note In addition to providing the system playback interface, you can also use AVPlayerViewController to present a media-trimming experience similar to QuickTime Player in macOS. See Trimming and exporting media in visionOS for more information. Use AVPlayerViewController to play video in windowed environments in visionOS. It automatically adapts its user interface to best fit its presentation. For example, when you present it nested inside another view, it displays an inline user interface:  Note When you present the player inline, it only displays standard 2D video. To play 3D content, present it fullscreen. Present the player in full-screen mode by setting it as the exclusive root view of your app, or by presenting it using the fullScreenCover(item:onDismiss:content:) modifier. In full-screen mode, the player presents a more content-forward design that dims the environment by default to provide more suitable viewing. This provides a streamlined viewing experience for both 2D and 3D content.   The user interface displays a title view above the transport bar when the current player item contains title and subtitle metadata. When playing live-streaming content, the title view may also display a badge to indicate the content state to the viewer.  The title view displays the values of an asset’s commonIdentifierTitle and iTunesMetadataTrackSubTitle metadata items, when available. If your media doesn’t provide embedded metadata, you can add supplemental metadata to display by creating instances of AVMetadataItem. The table below lists the metadata values the player user interface supports. Metadata Identifier Type Title commonIdentifierTitle String Subtitle iTunesMetadataTrackSubTitle String Artwork commonIdentifierArtwork Data Description commonIdentifierDescription String Genre quickTimeMetadataGenre String Content rating iTunesMetadataContentRating String In an app that defines a simple structure to hold string and data items, you can map its values to their appropriate metadata identifiers and build an array of metadata items: To apply the metadata to the current player item, set the array of metadata items as the value of the player item’s externalMetadata property: Only the title and subtitle values display in the title view. The player presents the other supported metadata values in its Info tab, which the section below describes. The visionOS player UI can display one or more content tabs in the user interface to show supporting information or related content. By default, the player presents an Info tab when an asset contains embedded metadata or when you set external metadata on the player item, as the Display supporting metadata section above describes. Your app can also present custom tabs to show supporting content. You define your tab content as standard SwiftUI views, wrap them in a UIHostingController, and set them as the customInfoViewControllers property. The player UI uses the title property of the hosting controller to display as the tab title in the interface, so set this value before setting it on the player view controller.   The player UI presents an Info tab when the asset it displays provides embedded or external metadata. The tab’s view displays the metadata details, and it may show up to two UIAction controls along its trailing edge:  Customize the actions the view presents by setting a value for the player view controller’s infoViewActions property. When playing nonlive content, this property contains a single-element array that presents an action to play the content from the beginning. You can replace the default value (if present), add an additional action, or set this property value to an empty array to display no actions. The example below shows how to add a Add to Favorites action to the view:  You can use the visionOS player UI to present controls contextually, which your app displays for a specific range of time in the content and then dismiss. A common use for this type of control is a skip button that displays during the title sequence of a movie or TV show. People can tap the button to bypass the introduction and quickly skip to the main content.  AVPlayerViewController provides a contextualActions property you can use to specify one or more actions to present. The player displays them along the bottom-trailing side of the screen. The following code example shows a simple implementation of an action that seeks the player forward to the time of the main content: When you set a value for the contextualActions property, the player presents the controls immediately. To present them only during a relevant section of the content, observe the player timing by adding a periodic or boundary time observer. The following example defines a periodic time observer that fires every second during normal playback. In each invocation, it evaluates the new time to determine whether it falls within the presentation range. If it does, the example sets the skip action as the contextual actions value; otherwise, it clears the value by setting it to an empty array.          AVPlayerViewController AVPlayerViewController AVPlayerViewController fullScreenCover(item:onDismiss:content:) commonIdentifierTitle iTunesMetadataTrackSubTitle AVMetadataItem commonIdentifierTitle iTunesMetadataTrackSubTitle commonIdentifierArtwork commonIdentifierDescription quickTimeMetadataGenre iTunesMetadataContentRating private func createMetadataItems(for metadata: Metadata) -> [AVMetadataItem] {
    [
        metadataItem(key: .commonIdentifierTitle, value: metadata.title),
        metadataItem(key: .iTunesMetadataTrackSubTitle, value: metadata.subtitle),
        metadataItem(key: .commonIdentifierArtwork, value: metadata.imageData),
        metadataItem(key: .commonIdentifierDescription, value: metadata.description),
        metadataItem(key: .iTunesMetadataContentRating, value: metadata.rating),
        metadataItem(key: .quickTimeMetadataGenre, value: metadata.genre)
    ]
}


private func metadataItem(key identifier: AVMetadataIdentifier,
                          value: Any) -> AVMetadataItem {
    let item = AVMutableMetadataItem()
    item.identifier = identifier
    item.value = value as? NSCopying & NSObjectProtocol
    item.extendedLanguageTag = ""und""
    // Return an immutable copy of the item.
    return item.copy() as! AVMetadataItem
} externalMetadata let metadata: Metadata = // A structure that contains simple string values.
playerItem.externalMetadata = createMetadataItems(for: metadata) UIHostingController customInfoViewControllers title // Set custom content tabs on the player UI.
let title = String(localized: ""Up Next"")


let view = UpNextView(videos: upNextVideos)
let hostingController = UIHostingController(title: title, rootView: view)


hostingController.preferredContentSize = CGSize(width: 300, height: 100)
controller.customInfoViewControllers = [hostingController] UIAction infoViewActions let infoCircle = UIImage(systemName: ""info.circle"")
let showMoreInfo = UIAction(title: ""More Information"", image: infoCircle) { action in
    // Navigate to a screen to display more information.
}
// Append the action to the array.
playerViewController.infoViewActions.append(showMoreInfo) AVPlayerViewController contextualActions // Define an action to skip the intro of a media item.
private lazy var skipActions: [UIAction] = {
    [UIAction(title: ""Skip Intro"") { [weak self] _ in
        guard let self else { return }
        avPlayer.seek(to: skipToTime)
    }]
}() contextualActions private func addTimeObserver() {
    // Observe the player's timing every second.
    let interval = CMTime(value: 1, timescale: 1)
    let fifteenSeconds = CMTime (value: 15, timescale: 1)
    timeObserver = avPlayer.addPeriodicTimeObserver(forInterval: interval,
                                                    queue: .main) { [weak self] time in
        guard let self else { return }
        let duration = avPlayer.currentItem?.duration ?? .zero
        // Show the Skip Intro button during the first 15 seconds of the content.
        showSkipIntroAction = time <= fifteenSeconds
    }
} class AVPlayerViewController protocol AVPlayerViewControllerDelegate "
22,"Controlling the transport behavior of a player  Overview See Also        Observe playback readiness Control the playback rate Seek through the media timeline Playback control                                AVFoundation provides comprehensive support for playing media assets, including local and remote file-based media and also media streamed with HTTP Live Streaming. The framework models its media assets using the AVAsset class, which provides a consistent interface to load and inspect your media, regardless of its type or location. Use an AVPlayer object to play media assets in the form of AVPlayerItem objects, which model the dynamic state of an asset such as its currentTime(). Understanding how to effectively use AVPlayer is essential for anyone building a custom player UI or otherwise requiring programmatic control of playback. When you create a player item, it starts with a status of AVPlayerItem.Status.unknown, which means the system hasn’t attempted to load its media for playback. Only when you associate the item with an AVPlayer object does the system begin loading an asset’s media. To know when the player item is ready for playback, observe the value of its status property. Add this observation before you call the player’s replaceCurrentItem(with:) method, because associating the player item with a player is the system’s cue to load the item’s media: When the player item reaches a AVPlayerItem.Status.readyToPlay state, present or enable your playback UI. Alternatively, if a failure occurs, show the appropriate status in the player. A player provides the play() and pause() methods as its primary means of controlling its playback rate. When a player item is ready for playback, call the player’s play() method to request that playback begins at the defaultRate, which has an initial value of 1.0 (the natural rate). By default, a player automatically waits to start playback until it has sufficient media data available to minimize stalling. You can determine whether a player is in a paused, waiting to play, or playing state by observing its timeControlStatus value: Observe changes to the rate property by observing notifications of type rateDidChangeNotification. Observing this notification is similar to key-value observing the rate property, but provides additional information about the reason for the rate change. Retrieve the reason from the notification’s userInfo dictionary using the rateDidChangeReasonKey constant: You can seek through a media timeline in several ways using the methods of AVPlayer and AVPlayerItem. The most common way is to use the player’s seek(to:) method, passing it a destination CMTime value. Call this method in an asynchronous context: You can call this method a single time to seek to the location, but you can also call it continuously such as when you use a Slider view. The seek(to:) method is a convenient way to quickly seek through your presentation, but it’s tuned for speed rather than precision. This means the actual time to which the player seeks may differ slightly from the time you request. If you need to implement precise seeking behavior, use the seek(to:toleranceBefore:toleranceAfter:) method, which lets you indicate the tolerated amount of deviation from your target time (before and after). For example, if you need to provide sample-accurate seeking behavior, specify tolerance values of zero:         AVAsset AVPlayer AVPlayerItem currentTime() AVPlayer AVPlayerItem.Status.unknown AVPlayer status replaceCurrentItem(with:) func playMedia(at url: URL) {
    let asset = AVAsset(url: url)
    let playerItem = AVPlayerItem(
        asset: asset,
        automaticallyLoadedAssetKeys: [.tracks, .duration, .commonMetadata]
    )
    // Register to observe the status property before associating with player.
    playerItem.publisher(for: \.status)
        .removeDuplicates()
        .receive(on: DispatchQueue.main)
        .sink { [weak self] status in
            guard let self else { return }
            switch status {
            case .readyToPlay:
                // Ready to play. Present playback UI.
            case .failed:
                // A failure while loading media occurred.
            default:
                break
            }
        }
        .store(in: &subscriptions)
    
    // Set the item as the player's current item.
    player.replaceCurrentItem(with: playerItem)
} AVPlayerItem.Status.readyToPlay play() pause() play() defaultRate 1.0 timeControlStatus @Published var isPlaying = false


private func observePlayingState() {
    player.publisher(for: \.timeControlStatus)
        .receive(on: DispatchQueue.main)
        .map { $0 == .playing }
        .assign(to: &$isPlaying)
} rate rateDidChangeNotification rate userInfo rateDidChangeReasonKey // Observe changes to the playback rate asynchronously.
private func observeRateChanges() async {
    let name = AVPlayer.rateDidChangeNotification
    for await notification in NotificationCenter.default.notifications(named: name) {
        guard let reason = notification.userInfo?[AVPlayer.rateDidChangeReasonKey] as? AVPlayer.RateDidChangeReason else {
            continue
        }
        switch reason {
        case .appBackgrounded:
            // The app transitioned to the background.
        case .audioSessionInterrupted:
            // The system interrupts the app’s audio session.
        case .setRateCalled:
            // The app set the player’s rate.
        case .setRateFailed:
            // An attempt to change the player’s rate failed.
        default:
            break
        }
    }
} AVPlayer AVPlayerItem seek(to:) CMTime // Handle time update request from user interface.
func seek(to timeInterval: TimeInterval) async {
    // Create a CMTime value for the passed in time interval.
    let time = CMTime(seconds: timeInterval, preferredTimescale: 600)
    await avPlayer.seek(to: time)
} Slider seek(to:) seek(to:toleranceBefore:toleranceAfter:) // Seek precisely to the specified time.
await avPlayer.seek(to: time, toleranceBefore: .zero, toleranceAfter: .zero) class AVPlayer class AVPlayerItem class AVPlayerItemTrack class AVQueuePlayer class AVPlayerLooper "
23,"Monitoring playback progress in your app  Overview See Also       Observe the current playback time at regular intervals Observe the playback of specific times within a media presentation Presentation                                Media playback apps commonly need to monitor playback progress to drive the state of player UI or perform other actions. Monitoring this state requires a higher level of time precision than key-value observing can deliver, so AVPlayer provides specific API to observe playback time. This article describes how you can observe this state at regular intervals or as playback crosses specific time boundaries. The most common way to observe a player’s current time is at regular intervals. Observing it this way is useful when driving the state of a time display in a player’s user interface. To observe the player’s current time at regular intervals, call its addPeriodicTimeObserver(forInterval:queue:using:) method. This method takes a CMTime value that represents the interval at which to observe the time, a serial dispatch queue, and a callback that the player invokes at the specified time interval. The following example adds an observer that the player calls every half-second during normal playback: Always pair a call to the player’s addPeriodicTimeObserver(forInterval:queue:using:) method with a call to removeTimeObserver(_:) when you’re finished monitoring the state. Failing to observe this rule results in undefined behavior. Another way to observe the player is when it crosses specific times boundaries during playback. You can respond to the passage of these times by updating your player UI or performing other actions. To have the player notify your app as it cross specific points in the media timeline, call the player’s addBoundaryTimeObserver(forTimes:queue:using:) method. This method takes an array of NSValue objects that wrap CMTime values that define your boundary times, a serial dispatch queue, and a callback closure. The following example shows how to define boundary times for each quarter of playback: If you add either a periodic or boundary time observer, you need to remove observation by calling removeTimeObserver(_:) when complete.        AVPlayer addPeriodicTimeObserver(forInterval:queue:using:) CMTime @Published private(set) var duration: TimeInterval = 0.0
@Published private(set) var currentTime: TimeInterval = 0.0


private let player = AVPlayer()
private var timeObserver: Any?


/// Adds an observer of the player timing.
private func addPeriodicTimeObserver() {
    // Create a 0.5 second interval time.
    let interval = CMTime(value: 1, timescale: 2)
    timeObserver = player.addPeriodicTimeObserver(forInterval: interval,
                                                  queue: .main) { [weak self] time in
        guard let self else { return }
        // Update the published currentTime and duration values.
        currentTime = time.seconds
        duration = player.currentItem?.duration.seconds ?? 0.0
    }
}


/// Removes the time observer from the player.
private func removePeriodicTimeObserver() {
    guard let timeObserver else { return }
    player.removeTimeObserver(timeObserver)
    self.timeObserver = nil
} addPeriodicTimeObserver(forInterval:queue:using:) removeTimeObserver(_:) addBoundaryTimeObserver(forTimes:queue:using:) NSValue CMTime /// Adds an observer of the player traversing specific times during standard playback.
private func addBoundaryTimeObserver() async throws {
    
    // Asynchronously load the duration of the asset.
    let duration = try await asset.load(.duration)
    
    // Divide the asset's duration into quarters.
    let quarterDuration = CMTimeMultiplyByRatio(duration,
                                                multiplier: 1,
                                                divisor: 4)
    
    var currentTime = CMTime.zero
    var times = [NSValue]()
    
    // Calculate boundary times.
    while currentTime < duration {
        currentTime = currentTime + quarterDuration
        times.append(NSValue(time:currentTime))
    }
    
    timeObserver = player.addBoundaryTimeObserver(forTimes: times,
                                                  queue: .main) { [weak self] in
        // Update the percentage complete in the user interface.
    }
} removeTimeObserver(_:) class AVPlayerLayer class AVSynchronizedLayer "
24,"Trimming and exporting media in visionOS  Overview See Also         Determine whether the media supports trimming Enable the trimming user interface Export the trimmed media selection visionOS Playback                                You use AVPlayerViewController to present the system video-player interface in your visionOS app. In addition to its primary role, AVPlayerViewController can also provide a media-trimming experience similar to the interface of QuickTime Player in macOS, like that below.  When you enable this feature, people can specify a segment of the media timeline for display. This article describes how to adopt this feature in your app, and shows how to use AVFoundation to export the trimmed result. Apps typically provide a user-interface element to put the player view controller into trimming mode. Because the player doesn’t support trimming certain media, such as HTTP Live Streaming or protected content, apps observe the state of the canBeginTrimming property to update the enabled state of their user interface accordingly. For example, the following code observes the state of the canBeginTrimming property and updates the state of a published property, which sets the appropriate enabled state in the UI:  After you determine that the player view controller supports editing the current media’s timeline, call the player’s beginTrimming(completionHandler:) method to enable its trimming interface. Call this method from an asynchronous context: This method returns a Boolean value that indicates whether the user pinched the Done button or the Cancel button. Pinching the Done button causes the view controller to update the values of the player item’s reversePlaybackEndTime and forwardPlaybackEndTime properties to match the trimmed selection. A convenient way to export your trimmed selection is to use AVAssetExportSession. This object provides a simple preset-based approach to transcode media in various formats. Create an instance of an export session by passing it the player item’s asset and an export preset. Additionally, configure its output URL and file type: To export only the portion of the asset that matches your trimmed selection, create a CMTimeRange based on the reverse and forward playback end times of the current player item: Finally, begin the export operation to begin asynchronously transcoding the media to the output URL:         AVPlayerViewController AVPlayerViewController canBeginTrimming canBeginTrimming @Published private(set) var isTrimming = true
@Published private(set) var supportsTrimming = true


var controller: AVPlayerViewController? {
    didSet {
        // Reset the internal state variables to false and exit.
        guard let controller else {
            isTrimming = false
            supportsTrimming = false
            return
        }
        // Connect the AVPlayer object to the the view controller.
        controller.player = player
        /// Update the state of `supportsTrimming` based on the value of `canBeginTrimming`.
        controller.publisher(for: \.canBeginTrimming)
            .removeDuplicates()
            .assign(to: &$supportsTrimming)
    }
} beginTrimming(completionHandler:) /// Enables the player view controller's media trimming interface.
func startTrimming() async {
    // Exit early if the controller doesn't support trimming.
    guard let controller, controller.canBeginTrimming else { return }
    
    isTrimming = true
    if await controller.beginTrimming() {
        // A user pinched the button to complete the trim operation.
    } else {
        // A user pinched the button to cancel their changes.
    }
    isTrimming = false
} reversePlaybackEndTime forwardPlaybackEndTime AVAssetExportSession // Export the asset in the highest quality.
let preset = AVAssetExportPresetHighestQuality
// Check the compatibility of the preset to export the video to the output file type.
guard await AVAssetExportSession.compatibility(ofExportPreset: preset,
                                               with: playerItem.asset,
                                               outputFileType: .mp4) else {
    print(""The selected preset can't export the video to the output file type."")
    return
}


guard let exportSession = AVAssetExportSession(asset: playerItem.asset,
                                               presetName: preset) else {
    print(""Unable to create an export session that supports the asset and preset."")
    return
} CMTimeRange // Create a time range that matches the trimmed selection.
let startTime = playerItem.reversePlaybackEndTime
let endTime = playerItem.forwardPlaybackEndTime
exportSession.timeRange = CMTimeRange(start: startTime, end: endTime) // Export the content.
await exportSession.export() class AVPlayerViewController protocol AVPlayerViewControllerDelegate "
25,"Diagnosing and resolving bugs in your running app  Overview           Pause the app to inspect variables and isolate bugs Locate crashes, exceptions, and runtime issues Inspect variables and execution sequence without pausing Identify potential overuse of CPU and memory Detect high disk access and network use Visualize and diagnose increasing memory usage Inspect and resolve appearance and layout issues                                Unit testing determines whether your code delivers results that meet your expectations, but it doesn’t explain the cause when it doesn’t. To diagnose an error, attach the debugger, reproduce the error, and then narrow down its root cause by inspecting your variables at key points in your code while the app is running with breakpoints. If you configure a scheme’s run action for debugging by using the Debug executable checkbox in Info settings, the app will attach to the debugger automatically when the app uses the scheme. To attach the debugger to a process that is already running, choose Debug → Attach to Process, and select your app’s process from the list. Follow this same process to diagnose and resolve errors in your code, crashes, memory leaks, and layout problems. To fix a bug, you first need to understand what is causing it. To narrow down the cause of a bug, develop a set of steps to reliably reproduce it: Determine where the bug happens in your source code. Pause your app with a breakpoint in your source code before the point at which you believe the bug occurs. Look at your variables and confirm they have the values you expect. If they don’t, begin again with step 1. Step through your code and watch your variables change. Note where your variables have unexpected values. Analyze your code to determine a fix. After determining a potential fix for the bug, confirm the diagnosis by changing your code and retesting to reproduce it. If the change addresses the problem, you’ve resolved the bug. If your change doesn’t resolve it, reconsider where the bug might be occurring, and repeat the steps to isolate and fix it. For more information on setting breakpoints and inspecting variables, see Setting breakpoints to pause your running app and Stepping through code and inspecting variables to isolate bugs. When your app experiences a crash, exception, or runtime issue, it can be challenging to pinpoint the code causing the problem because the stack trace for the crash doesn’t always point to the line of code that causes the crash. Use the rubric below to identify the problem characteristics and then set the correct type of breakpoint: A crash that stops at main or highlights AppDelegate is frequently an Objective-C exception. A crash that is the result of a runtime issue also stops at main or highlights AppDelegate, and may have a message similar to: “Thread 8: EXC_BAD_INSTRUCTION (code=…)”. A crash that stops at an uncaught or unhandled Swift error displays a fatal error message and indicates a Swift error. Add a breakpoint to your code in a location based on problem characteristics, then when your app stops at the breakpoint, check the state of the code execution. For more information on setting breakpoints and identifying crashes, see Setting breakpoints to pause your running app and Identifying the cause of common crashes. Note Your Swift code can receive an Objective-C exception when it uses code from a module that uses Objective-C. When you develop code, it’s helpful to log actions and variable values so you understand how your code runs and what values your variables have at different points in your app. This is especially true when you develop concurrent code, or code that executes simultaneously across multiple queues or threads, because bugs can be intermittent and difficult to reproduce. Often, you reproduce a bug in normal execution, but not when stepping through the debugger, because the timing is different between normal execution and debugging. The debugger provides tools to inspect variables without pausing and disturbing the timing of your concurrent code. Developers commonly add print or NSLog statements to see variable values. While this technique works, it adds extra code that isn’t useful after you finish development, and leaves your app with a noisy console that makes diagnosing subsequent bugs more difficult. Instead, use breakpoint actions to know when events in your app take place, and inspect variable values without pausing. To determine whether your code executes with minimal effect to timing, use a breakpoint action to play a sound and continue executing. If the debugger reaches the breakpoint when you run the app, it plays the sound and confirms execution. To log a variable value to the console without pausing, add a breakpoint with a Debugger Command action using po to print out the evaluation of an object, or v to print the value of a variable to the console. Select the Automatically continue after evaluating actions option for the breakpoint to prevent pausing.  To log custom text to the console and add context to variable values, add a breakpoint with a Log Message action. Specify your custom text, and include expressions, the breakpoint name, or the breakpoint hit count to provide more information. Note Because po compiles code dynamically to evaluate expressions, it takes more time to evaluate your variable and log it to the console. To reduce timing issues, use v to log variable values instead. Use other breakpoint actions to execute an AppleScript or a shell script, or to capture a GPU frame. For more information on inspecting variables, see Setting breakpoints to pause your running app and Stepping through code and inspecting variables to isolate bugs. An easily overlooked and common problem in development and testing is the overuse of CPU and memory. Xcode’s debugger provides gauges in the Debug navigator to help investigate potential problems. Monitor the gauges while you’re testing your app to uncover unusual usage. Click a gauge for a more detailed view.  The CPU gauge shows the amount of CPU the app requires to process instructions over time. When your app is drawing the user interface, processing data it retrieves from the network, or performing calculations, it‘s normal to see CPU usage increase to fairly high numbers for a short period of time. When those tasks are complete and your app is idle and waiting for the user to perform an action, CPU usage should be zero or very low. Do additional analysis if CPU usage is: Persistent at a level above zero when the app appears to be idle. Over 100% for more than very short periods of time. Very high and you see hitches in your user interface. For more information on improving performance, see Improving your app’s performance. The Memory gauge shows how much memory your app uses over time. It starts at a fairly small number, less than 10 MB, when you first launch your app, and then increases as people navigate through your user interface. It may also increase if you fetch, process, and store data from the network, or perform complex calculations. It then decreases when processing is complete. Watch the gauge as you navigate through your app, and note when memory usage goes up and down. Memory usage increases when you present modal views or add a view to a navigation controller, and decreases when you dismiss or navigate away from those views. If your usage continues to increase and doesn’t ever decrease, investigate whether you have a memory leak or abandoned memory. For more information on reducing memory use and resolving memory leaks, see the section below Visualize and diagnose increasing memory usage and Reducing your app’s memory use. Be aware of issues resulting from frequent access to resources on disk and over the network. You can monitor these resources using the gauges in Xcode’s debugger as well. The Disk I/O gauge shows how much data your app reads and writes from disk over time. The gauge shows if you: Store data that the user generates in your app. Store data in user preferences. Fetch data from the network and store it. Read data from your app bundle or the app’s directories. Storing and reading data frequently from disk uses more power than doing so from memory, and it adds wear and tear to the user’s device. To know whether disk usage is unusual, you need to understand the size of data you’re storing and reading, and compare that to what you observe in the gauge. For example, if you download and store a 5 MB graphic file for display in a view that you use frequently and it writes over 50 MB of data, investigate whether the remote image changes frequently, or whether you need to configure networking to prevent redownloading the same image. If you’re reading more data from disk than you expect, investigate whether a memory-caching solution might help, or whether you’re initiating a data read from the wrong point in your app or view life cycle, and reading it too often. For more information on reducing disk writes, see Reducing disk writes. The Network I/O gauge shows how much data your app reads from and writes to the network over time. If your app only uses local resources, your app may not read or write any data from the network. Communicating data over the network uses energy and reduces the device’s battery life, so minimize data transfer wherever possible. To understand your app’s network usage, watch the Network I/O gauge when your app is sending or receiving data from the network. For example, if you implement a cache system for downloaded images and your network usage increases when accessing them, confirm that your cache settings are correct in the app and on the server. If you’re uploading user-generated content and frequent upload failures during poor networking conditions lead to high network usage, implement a system to recover and restart failed uploads at the point of failure, rather than reuploading the entire file. Diagnose the cause of memory leaks and abandoned memory with the memory graph. The observable symptom of a memory leak is memory usage that continues to increase over time, even when conditions in the app indicate that memory usage is decreasing. A memory leak can occur in a retain cycle, which is when objects maintain strong references to each other, but the app no longer references them. These objects remain in memory and the app can’t remove them. Abandoned memory occurs when you create objects and your code still references them, but your app no longer needs them or uses them.  To see the memory graph in the debugger, pause your app at a breakpoint and click the Debug Memory Graph button in the debug bar. Alternatively, click the Debug Memory Graph button when the app is running to pause the app and show the memory graph. The memory graph view replaces the stack trace in the Debug navigator with a list of types, organized by library, each with a list of instances called nodes. Select a node to view its memory graph. A node’s memory graph shows all the memory references to that node, and highlights strong references. Control-click any node in the graph to perform more actions, such as accessing Quick Look or printing the description to the console. Choose Focus on Node to show the graph for the selected node. Click a reference to see its details, including the name of the variable, the type of reference, and the source and destination objects in memory.  To resolve a memory leak for a retain cycle: Observe the Memory gauge while you navigate the app. Note when memory usage increases when your app instantiates an object, but doesn’t decrease when you expect the system to deallocate the object. Examine the memory graph to see if there are an unexpected number of instances of the object or inappropriate strong references to it. If there is a strong reference to the object, Control-click the node with the strong reference and choose Focus on Node to view its graph. If the node also has a strong reference from the object, this is a retain cycle. Resolve the retain cycle by changing one side of the relationship to use a weak declaration for the reference to the other object, or remove the reference altogether by removing any dependencies on the other object. Retest to confirm that the change fixes the issue. To resolve abandoned memory issues, identify the time in your app’s life cycle that it no longer needs the abandoned object and remove any references to it. Some issues in the appearance or layout of your app only appear when you configure the system with a particular interface style, dynamic text size or when your app uses particular accessibility features. Use environmental overrides when targeting iOS, macOS, and tvOS apps to test your interface in these environments. To understand issues that involve the position or size of your view, you might need to inspect them within the context of views in other layers. Use the view debugger when targeting iOS, macOS, tvOS, and watchOS apps, which displays a 3D representation of the view hierarchy in layers, to help diagnose these issue. Entities within a visionOS app and their surroundings sometimes interact with each other in ways you don’t expect. Enable visualizations to represent coordinate axes, bounding boxes, and other information that is normally invisible, to help understand these interactions. For information on using these features to debug the appearance of your app, see Diagnosing issues in the appearance of a running app.       A crash that stops at main or highlights AppDelegate is frequently an Objective-C exception.
A crash that is the result of a runtime issue also stops at main or highlights AppDelegate, and may have a message similar to: “Thread 8: EXC_BAD_INSTRUCTION (code=…)”.
A crash that stops at an uncaught or unhandled Swift error displays a fatal error message and indicates a Swift error. Persistent at a level above zero when the app appears to be idle.
Over 100% for more than very short periods of time.
Very high and you see hitches in your user interface. Store data that the user generates in your app.
Store data in user preferences.
Fetch data from the network and store it.
Read data from your app bundle or the app’s directories. main AppDelegate main AppDelegate print NSLog po v po v "
26,"Diagnosing issues in the appearance of a running app  Overview See Also            Adjust system configurations to identify their impact on your views Resolve issues in the layout of your UIKit and SwiftUI views Understand the relationships between objects in a immersive space Debugging strategies                                At times your app’s content may appear to be missing, out of place, or have an appearance that is incorrect. To identify and diagnose the cause of these issues, attach the debugger, reproduce the error, and then narrow down its root cause by inspecting changes in your interface, the code that is executing, and the state of variables. If you configure a scheme’s run action for debugging by using the Debug executable checkbox in Info settings, the app will attach to the debugger automatically when the app uses the scheme. To attach the debugger to a process that is already running, choose Debug → Attach to Process, and select your app’s process from the list. Temporarily override system settings to control your app’s appearance and reveal problems that only occur when these settings are in effect, understand layout issues by visualize your views in stacked layers, and debug content in immersive space by adding visual overlays. Some visual issues only arise when you configure the system using specific environmental settings. Xcode provides environmental overrides when targeting iOS, macOS, and tvOS apps to help you debug these issue. Use these environmental overrides to change the interface style, dynamic text size, and to induce the effects of other accessibility options, so you can understand the effect of these changes on the layout and visual appearance of your views. To enable one or more of these overrides, click the Enable Overrides button on Xcode’s debugging toolbar, toggle the switch next to the override category, and configure the controls under the category heading.  View your app’s content with a light or dark appearance. Select between light and dark appearances using the radio button. View your app’s content using different dynamic type sizes. Select the size using the slider. For more information on dynamic types, see DynamicTypeSize. View the effects various accessibility features have on your app’s content. Click the checkboxes to enable or disable the accessibility features. Use the View debugger, available when targeting iOS, macOS, tvOS, and watchOS apps, to diagnose the reasons an item in your user interface is in the wrong position or is the wrong size. Set a breakpoint in your app after it presents the view, for example, in a viewDidAppear: method, then click the View Debugger button in the debug bar when the debugger pauses on your breakpoint. Alternatively, just click the View Debugger button after your app presents the view. The debugger displays a 3D rendering of the current view on the canvas in the center, with a representation of the view hierarchy in the Debug navigator. Drag the view in any direction to see a 3D representation of the current view stack, and use the controls at the bottom of the canvas to adjust the views and the spacing between them.  Click to select a view in either the visual rendering or the view hierarchy in the Debug navigator, then inspect details in the Object inspector or Size inspector. Resolve your layout issue according to the type of layout: The view debugger shows the frames you specify in the Size inspector. If the size isn’t what you expect, step through your code to diagnose issues with your frame calculations. Then fix and retest. The Size inspector shows constraints. Click a constraint to highlight it in the view debugger. Analyze the constraints that affect the misplaced or mis-sized view to diagnose the issue. Adjust your constraints, either in your code or in Interface Builder, and retest. The Size inspector shows how SwiftUI resolves the size and placement of your view. With that information, analyze and adjust your SwiftUI code and retest. For visionOS apps with content in an immersive space, it’s often helpful to see a visual representation of coordinate axes, bounding boxes, and other information that is normally invisible. Xcode’s debugging tools include options to display this information in Simulator or on a device. Use them to ensure that your entities are located where you expect, and interacting with each other and the surroundings the way you anticipate. For example, if an entity isn’t responding to events, enable Collision Shapes to confirm the presence of one, required for event handling, and indicate its boundary.  To annotate your content with an overlay, click the Enable Visualizations button on Xcode’s debugging toolbar and select one or more of the options: Red, green and blue arrows indicating the x, y, and z axes of each object in the space. Use this to understand the orientation of each object. Green lines that indicate the bounding edge of each entity. A dot marking the the origin of each object. Red, green and blue arrows indicating the x, y, and z axes of each anchor point and a yellow line betwen entities and their anchors. Use this to understand the placement of entities in relation to other real-world objects in the space. A white outline around the collision shape of an entity. Use this to understand issues in event detection. A multi-colored wireframe around real objects in the space. Use this to identify areas where virtual objects are correctly or incorrectly being hidden behind real objects. White border lines and diagonals marking each surface. Use this to understand the borders of surfaces that the system detects.       DynamicTypeSize viewDidAppear: "
27,"Creating a performance plan for your visionOS app  Overview           Set performance and power targets Identify the code flows and user scenarios to test Consider thermal and environmental factors Choose tools to collect performance data Profile on a physical device Build automated test cases and run them regularly                                                Performance tuning is an important part of the development process, regardless of platform. Performance tuning means making your app run as efficiently as possible, so it does more work in less time and with fewer system resources. Efficiency is especially important on devices that can support multiple apps in an immersive experience. Apps that consume too many resources, can push the device beyond thermal limits. When this occurs, the system takes steps to cool down to a more acceptable level. This can have a noticeable visual impact and be disorienting for the wearer. As you start development, set aggressive goals and evaluate progress throughout the development cycle. Automate the collection of performance metrics as much as possible and look at data over time to see if performance is improving or declining. When you detect a significant decrease in performance, take immediate steps to correct it. When you start fine-tuning early in development, you have more time to make needed changes to algorithms and approaches. For more information on performance tuning, see Improving your app’s performance. Performance isn’t a single metric that you measure and improve. Typically, you choose several metrics and set goals for each of them. For example, consider: Make sure your app launches quickly; this is your first chance to make a good impression. Your interface needs to respond quickly to interactions, even while doing other work. Minimize the time it takes to start tasks. For example, make sure audio and video start without noticeable delays. For an immersive experience with realtime rendering, it’s important to maintain consistently high frame rates. Help maintain these rates by avoiding unnecessary changes that result in more frequent updates to the shared render server. Measure things like update rates, stalls, and hangs in both the render server and your app. Only render the content you need, and optimize the textures and other resources you use during drawing. When the device begins to reach thermal limits, the system reduces CPU or GPU usage and performance degrades over time. Avoid this thermal ceiling by prioritizing and spreading out work, limiting the number of simultaneous threads your app maintains, and turning off hardware-related features like Core Location when you don’t need them. Make the app do as much as possible using the smallest amount of hardware resources. Minimize task-based overhead. Use as little free memory as possible. Don’t allocate or deallocate memory during critical operations, which might make your app appear slow. After you choose the metrics you want, set realistic goals and prioritize them, so you know which ones matter the most. Performance tuning often involves making tradeoffs between competing goals. For example, if you reduce CPU usage by caching computed data or pre-load assets to improve responsiveness, you increase your app’s memory usage. Make these kinds of tradeoffs carefully, and always measure the results of any changes to learn whether they were successful. In some cases, you might find the sacrifice isn’t worthwhile. Consider how people will use your app. If your app runs in the Shared Space, consider more conservative targets and goals for system resources. If you expect people to use your app for longer periods of time, factor this extended use into your targets and goals when choosing metrics. After you choose the metrics to collect, decide which portions of your app to test. Choose features that are repeatable, measurable, and reliable to test. Repeatable automated tests allow you to compare the results and know the comparisons represent the exact same task. Focus on places where your app executes code, but don’t ignore places where your app hands off data to the system and waits. If your app spends a significant amount of time waiting for information, consider eliminating the requests altogether or batching them to achieve better performance. Focus your tuning efforts on the parts of your app that people use the most, or that have the most impact on overall system performance, including: User-facing workflows Key algorithms Task that allocate or deallocate memory Background and network-based tasks Custom Metal shaders Choose actions that people perform frequently or that correspond to important features. For example, if your app lets someone add a new contact, test the workflow for creating the contact, editing the contact, and saving the results. Test your app with a particular feature enabled and disabled to determine whether the feature is solely responsible for any performance impacts. Choose lightweight workflows such as how your app performs at idle time, and also heavyweight workflows, for example, ones that involve user interactions and your app’s responses. For launch times, gather metrics for both hot and cold launches — that is, when the app is already resident in memory and when it is not. Consider how environmental factors impact your app. The characteristics of your physical environment can affect system load and thermals of the device. Consider the effect that ambient room temperature, the presence of other people, and the number and type of real-world objects can have on the your app‘s algorithms. Try to test in different settings to get an idea of whether you need to optimize for these scenarios or not. Use Xcode’s thermal inducers to mimic the device hitting its thermal limits and consider how your app responds to fair, serious, and critical thermal notifications. You might need to have different performance goals when under thermal pressure, and prioritize optimizing for power or find ways to dynamically lower your app‘s complexity in response to thermal pressure to give a smoother experience, even if latency is a bit higher. There are many tools and APIs you can use to collect performance-related data for your visionOS app. Use a variety of tools to make sure you have the data you need: Monitor the CPU, memory, disk and network gauges in the Debug navigator to track system resources utilization. Profile your app to gather performance data on most metrics. Instruments lets you profile your app’s code execution, find memory leaks, track memory allocations, analyze file-system or graphics performance, SwiftUI performance, and much more. Use the RealityKit Trace template to monitoring and investigate render server stalls and bottlenecks on visionOS. Use XCTest APIs to collect performance data. Use MetricKit to gather on-device app diagnostics and generate reports. Review diagnostic logs for hangs, disk and energy usage, and crashes in the Xcode Organizer. Review statistics on the contents of your RealityKit scenes. Use this information to optimize your 3D models and textures. Add signposts to your code to generate timing information you can view in Instruments. For more information, see Recording performance data. Include log messages to report significant events and relevant data for those events. For more information, see Generating log messages from your code. Get feedback from testers about their experiences with beta versions of your app. Fill out the Test Information page for your beta version, and request that testers provide feedback about the performance of your app. In general, profile and analyze performance on a physical device rather than in Simulator. Even if something works well in Simulator, it might not perform as well on devices for all use cases. Simulator doesn’t support some hardware features and APIs. There are differences in the rendering pipeline for Simulator running on macOS, so rendering performance characteristics will be different. Other pipelines such as input delivery and audio or video playback are also different. There are, however, some insights you can gain profiling in Simulator, such as CPU stalls, that help you spot areas to investigate and address. Xcode comes with tools to help you automate the collection of performance data: Use the XCTest framework to build test cases to collect performance metrics. XCTest lets you gather several different metrics, including the time it takes to perform operations, the amount of CPU activity that occurs during the test, details about memory or storage use, and more. Use Instruments to collect metrics for specific interactions with your app. Record those interactions and play them back later to collect a new set of metrics. Write custom scripts to gather performance-related data using system command-line tools. Integrate these scripts into your project’s build process to automate their execution. Configure Xcode to run test cases each time you build your app, or create a separate target to run test cases or custom scripts on demand. Integrate your performance tests into your Xcode Cloud workflows, or your own custom continuous integration solution. Note Collect performance data using a production version of your app to obtain more accurate results. Debug builds contain additional code to support debugging operations and logging. You can collect data from debug builds too, but keep those metrics separate from production-build metrics. For information about how to write test cases for your app, see Testing your apps in Xcode. For information about how to automate testing with Xcode Cloud, see Xcode Cloud.       User-facing workflows
Key algorithms
Task that allocate or deallocate memory
Background and network-based tasks
Custom Metal shaders Use the XCTest framework to build test cases to collect performance metrics. XCTest lets you gather several different metrics, including the time it takes to perform operations, the amount of CPU activity that occurs during the test, details about memory or storage use, and more.
Use Instruments to collect metrics for specific interactions with your app. Record those interactions and play them back later to collect a new set of metrics.
Write custom scripts to gather performance-related data using system command-line tools. Integrate these scripts into your project’s build process to automate their execution. "
28,"Managing files and folders in your Xcode project  Overview See Also        Add new files to a project Add existing files and folders to a project Organize project files in the navigator Delete files and folders Files and workspaces                        The Project navigator displays your project’s files and lets you open, add, delete, and rearrange those files. To open the Project navigator, at the top of your project window’s navigator area, click the icon that resembles a file folder.  When you select an file in the navigator, the inspector pane displays information about the file, and the editor area displays the contents of the file. The appearance of the editor area changes based on the type of file you select. For example, a source code file displays the source editor, and a property-list file displays the property-list editor. To locate files based on keywords or other criteria, use the filter bar at the bottom of the navigator area: To search for files, enter keywords in the filter bar’s text field. To show only recently modified files, click the Recent Files icon. To show only files with a changed source-control status, click the Source Control icon. Xcode provides templates for the common types of files you might want to add to your project, such as Swift files or playgrounds. In the Project navigator, select the folder or group where you want to add a file and perform one of the following actions: Click the Add button (+) in the filter bar and choose File from the pop-up menu. Choose New > File. Control-click and select New File. In the new file sheet, select a template for your new file. Xcode organizes templates by type to make them easier to find. You can also use the filter control to search for templates by name. After you select a template, click Next.  Some templates require you to specify additional information for the new file. For example, the Cocoa Touch template asks you to specify information about the class you’re creating, including the parent class name. Xcode uses this information to populate the file with some initial content. The final step is to save your file to the file system. When Xcode prompts you for the file’s location, it also asks you to specify group and target information. The group indicates where in your project to place the file, and Xcode selects a default group based on contextual information. Xcode also selects a default target. Make any relevant changes to the target and group values and click Create to create the file.  Xcode offers several ways to add existing files and folders to your project: Drag the files from the Finder into the Project navigator. Click the Add button (+) in the Project navigator’s filter bar, and choose Add Files to “projectName”. Choose File > Add Files to “projectName”. Xcode prompts you to select files and folders and configure how you want to add them to your project. (If you don’t see any configuration options, click the Options button at the bottom of the sheet.) Select at least one target and the options for how to incorporate the files and folders into your project. When you add files and folders to your project, select one of the following options: Copies all files and folders to the project folder before adding them to the Project navigator. Use this option to work on a copy of the files, instead of the original versions. Creates a group structure in your project that matches the structure of the selected field and folders on disk. Xcode creates a group in the Project navigator for each folder you add, and it adds the contents of the folder to that group. Displays the folders in the Project navigator, but doesn’t copy them to the project. In the Project navigator, each added folder points to a folder in the file system. When adding a local Swift Package’s folder to your project, perform the following additional steps: Place the package in a Packages group to make it easier to track. To create a new group in the project navgator, select File > New > Group. Drag the package from the Finder into the group. Add the package to the linked libraries build phase in your project settings. In the Project Editor, Select Build Phases and expand Link Binary With Libraries. Click the plus (+) button and select the package from your workspace. For more information about managing Swift packages, see Swift packages. Note To use RealityKit content that you create using Reality Composer Pro, you can add its folder to your Xcode project and link against the Swift Package it contains. For more information about Reality Composer Pro, see Designing RealityKit content with Reality Composer Pro. Most new projects contain some structure to organize the project’s content — for example, to separate source files from generated products. You can create additional groups and folders to organize your content and make it easier to navigate large projects. A folder is a file-system directory that you reference from your project. Xcode includes the contents of the folder in your project. A group is a collection of resources in your project. By default, Xcode maps each group to a folder in your project directory, but you can also create groups without an underlying file-system folder. You might use groups without folders if you want to manage files in your project without changing the underlying organization of the files on disk. In the Project navigator, create and modify groups using direct interaction or menu commands: Select an item and choose File > New > Group, or Control-click the item and select New Group from the contextual menu. Select an item and choose File > New > Group without Folder, or Control-click the item and select New Group without Folder from the contextual menu. Select the items and choose File > New > Group from Selection, or Control-click the selected items and select New Group from Selection. Double-click the file or group, and enter the new name. Select the group and choose View > Inspectors > Show File Inspector. Drag the new folder from the Finder to the old folder name under Location in the File inspector. Important If a group is associated with a folder, Xcode performs all rename, delete, move, and copy operations on the folder in the file system. For projects under source control, Xcode uses the source-control system to perform the operations and track the changes. If you move files between groups in the same Git repository, Xcode moves the files in the file system. If the files are in different repositories, Xcode copies the files into the folder in the new repository. To delete a file or folder from your project, select it and press the Delete key, or select Edit > Delete. Xcode prompts you to choose how to delete any selected items. This option removes files and folders from your project and the file system. Choose this option when you no longer need the information in the files. This option removes files and folders only from your project. Xcode doesn’t remove them from the file system.       To search for files, enter keywords in the filter bar’s text field.
To show only recently modified files, click the Recent Files icon.
To show only files with a changed source-control status, click the Source Control icon. Drag the files from the Finder into the Project navigator.
Click the Add button (+) in the Project navigator’s filter bar, and choose Add Files to “projectName”.
Choose File > Add Files to “projectName”. Place the package in a Packages group to make it easier to track. To create a new group in the project navgator, select File > New > Group. Drag the package from the Finder into the group.
Add the package to the linked libraries build phase in your project settings. In the Project Editor, Select Build Phases and expand Link Binary With Libraries. Click the plus (+) button and select the package from your workspace. A folder is a file-system directory that you reference from your project. Xcode includes the contents of the folder in your project.
A group is a collection of resources in your project. By default, Xcode maps each group to a folder in your project directory, but you can also create groups without an underlying file-system folder. You might use groups without folders if you want to manage files in your project without changing the underlying organization of the files on disk. "
29,"Configuring your app icon  Overview See Also            Create an app icon Specify app icon variations Configure the layers of an image stack Specify an App Store icon Change the default app icon set App icons and launch screen                                Every app has a distinct app icon that communicates the app’s purpose and makes it easy to recognize throughout the system. Apps require multiple variations of the app icon to look great in different contexts. Xcode can help generate these variations for you using a single high-resolution image, or you can configure your app icon variations by using an app icon’s image set in your project’s asset catalog. visionOS and tvOS app icons are made up of multiple stacked image layers you configure in your project’s asset catalog. When you create your project from a template, it automatically includes a default asset catalog (Assets.xcassets) that contains the AppIcon. If you don’t have a default asset catalog or existing AppIcon or you want to provide an alternate, you can add an app icon to an asset catalog manually: In the Project navigator, select an asset catalog. Click the Add button (+) at the bottom of the outline view. In the pop-up menu, choose OS variant > OS variant App Icon. Xcode creates a new app icon set or image stack with the name AppIcon. Variations of your app icon appear throughout the system in places like the Home View, Settings, and search results. In Xcode 14 and later, for iOS, macOS or watchOS apps, you can generate all variations of your icon using a single 1024×1024 pixel image or provide variations for all sizes. Although using a single size is the default behavior for new apps or new icons in the asset catalog, you might need all sizes in order to include detail in larger variants and simplify as app size decreases. If you have an existing project that provides multiple variants, consider providing a single size when that is all your icon requires. For each platform your app supports, choose between using a single size and providing all sizes in the Asset Catalog:  In the Project navigator, select an asset catalog. In the Asset Catalog, select the icon. To view and edit attributes, select Inspectors > Attributes from Xcode’s View menu. Select Single Size or All Sizes from the pop-up menu for the platform you want to change. For each platform your app supports, add a single image that Xcode can use to generate your icon variations, or add an image for each icon variation of an icon set in the Asset Catalog:  In the Project navigator, select an asset catalog. In the Asset Catalog, select the icon. From the Finder, drag image variations of the app icon to the image wells in the detail area of the Asset Catalog in Xcode that match their resolutions and use cases. visionOS and tvOS app icons combine a stack of multiple image layers to create a sense of depth. For tvOS apps, the asset catalog contains an App Icon & Top Shelf Image folder with the different app icon and launch image sets. For general design guidance, see App icons. For platform-specific design guidance: visionOS iOS macOS tvOS watchOS By default, visionOS and tvOS app icons are constructed with three layers. This is the maximum number of layers visionOS icons support but you can use up to five layers when constructing tvOS icons. To add a layer, Click the Add button (+), choose OS variant > OS variant App Icon Layer. To remove a layer, select the layer and click the Remove button (-).  Add images to each layer by dragging them from the Finder into the image wells in the detail area of the Asset Catalog in Xcode. For information on the use of layers, see App icons visionOS and tvOS. Note You can use Parallax Previewer app or Parallax Exporter plug-in to create and preview Layer Source Representation (.lsr and .xlsr) files that you can import into your Asset Catalog in Xcode. Save your file in the LSR file format to import a tvOS icon into Xcode, and save in the XLSR file format to import a visionOS icon. Download these from the Apple Design Resources site. If you distribute your app through the App Store, you must provide app icon imagery to use in the App Store. In the Project navigator, select an asset catalog and add icon images to the appropriate image wells in an app icon set or image stack. The App Store image well location varies by platform. Platform App Store icon location iOS Drag an icon image to the iOS 1,024pt image well. iMessage For the iOS target, drag an icon image to the iOS 1,024pt image well in the AppIcon set. For the iMessage Extension target, drag an icon to the Messages App Store image well in the iMessage App Icon set. Sticker Pack Drag an icon image to the iOS 1,024pt image well and the Messages App Store image well. macOS Drag an icon image to the App Store - 2x image well. tvOS Drag images to the image wells for the layers of your App Icon - App Store stack in the App Icon & Top Shelf Image folder. The App Store generates an icon from the layers of the image stack. visionOS Drag images to the image wells for the layers of your visionOS App Icon stack. The App Store generates an icon from the layers of the image stack. watchOS For the iOS target, drag an icon image to the iOS 1,024pt image well. For the WatchKit App target, drag an icon image to the watchOS image well. If you don’t create your project from a template, or you want to change your default app icon set, specify which one to use in your target’s build settings. In the Project navigator, select the project and in the project editor, select the target. In the App Icons and Launch Images section of the General pane, choose the app icon set from the App Icons Source pop-up menu.  If you don’t select the Include all app icon assets option, Xcode only includes the app icon set you specify in the App Icons Source pop-up menu when it builds your app. You might leave this option unselected if you want to use a different icon for the Debug and Release builds of your app without including the Debug icon in your Release app bundle. You can specialize the app icon for the Debug and Release configurations by modifying the Primary App Icon Set Name build setting in the Build Settings tab. Xcode also includes any additional app icon sets you specify under the Alternate App Icon Sets build setting. Include any icon sets your app can select using setAlternateIconName(_:completionHandler:) or use in App Store product pages. For information on configuring tests that use icons in App Store Connect, see Product Page Optimization.       visionOS
iOS
macOS
tvOS
watchOS Assets.xcassets AppIcon AppIcon AppIcon AppIcon iMessage App Icon setAlternateIconName(_:completionHandler:) "
30,"Running your app in Simulator or on a device  Overview See Also         Select a build scheme and run destination Configure the list of simulated devices Connect real devices to your Mac Run the app Interact with the simulated environment Essentials                                To test your app, build and run it on a simulated or real device. Use simulated devices to debug your app on a variety of hardware to which you don’t have immediate access. The tradeoff is that simulated devices run within the Simulator app on your Mac and don’t replicate the performance or features of an actual device. To verify your app runs exactly as intended, run it on one or more real devices. You can connect a real device to your Mac using a cable, or for iOS, tvOS, or visionOS apps, connect it over Wi-Fi after you pair it with Xcode. SwiftUI previews let you see your app’s interface without building and running your app. For more information on these dynamic previews, see Previews in Xcode. Before you build and run your app, select a build scheme that includes the target for your app. A scheme is a collection of project details and settings that tell Xcode how to build and run a product from your project. Xcode determines where the resulting product can run based on the scheme you select, and populates the run destination menu in the toolbar with the list of available devices. For example, if the scheme contains a tvOS app, Xcode includes only tvOS simulators and devices as potential run destinations. To learn more about schemes, see Customizing the build schemes for a project. Important When running apps in Simulator, some hardware-specific features might not be available. Frameworks that provide access to device-specific features also provide API to tell you when those features are available. Call those APIs and handle the case where a feature isn’t available. To test the feature itself, run your code on a real device. Manage real and simulated devices in the Devices and Simulators window in Xcode. To view this window, choose Window > Devices and Simulators. View and configure simulated devices from the Simulators tab.  To add a new simulated device, click the plus (+) button at the bottom of the list of simulators and specify the configuration you want. You can add new simulators to specify a different device type or operating system version than the default set. To remove a simulator from the list, select it and press Delete. Note Xcode requires the Simulator runtime for each platform and system version for which you build and run Simulator. If Xcode doesn’t display device types for a platform, you might need to install that platform’s Simulator runtime. For more information on this installation, see Installing and managing Simulator runtimes. To view and manage connections to your real devices, choose the Devices tab in the Devices and Simulators window in Xcode. The Devices tab shows the currently connected and disconnected devices and can help you diagnose problems that might occur. For example, Xcode might show a device as unavailable if it’s not running an operating system version your app supports. It also shows new devices available for pairing with your Xcode installation. Pair a device with Xcode to include them in the list of run destinations for your projects. To pair a device with a physical connection, connect the device to your Mac using an appropriate cable. Unlock the device and follow any instructions that appear in Xcode or on the device. To pair a device without a physical connection, such as visionOS and recent tvOS devices: Ensure that both your Mac and the device to connect are on the same Wi-Fi network. The Wi-Fi network must be compatible with Bonjour. Broadcast the device to the target Mac over the local network. To do this for a visionOS device, choose Settings > General > Remote Devices and for a tvOS device, choose Settings > Remotes and Devices > Remote App and Devices. Select the device from the list in the Devices and Simulators window in Xcode and click the pairing button which triggers a code to appear on the target device. Enter the code on the Mac to complete the pairing process. After pairing is complete, the device shows up under connected devices in Devices and Simulators window in Xcode. You don’t need to keep a paired device physically connected to your Mac to install and run apps. If your device is connected to Wi-Fi on the same network as your Mac, Xcode can use that connection to install and run your app. To pair an Apple Watch to a Mac, connect its companion iPhone to the Mac with a cable, and ensure that the iPhone is paired for development. After this step, follow any instructions on the Apple Watch to trust the Mac. When paired through an iPhone running iOS 17 or later, Xcode connects to the Apple Watch over Wi-Fi. Series 5 and older models of Apple Watch additionally require the Apple Watch and Mac to be associated with the same Bonjour-compatible Wi-Fi network. When paired through an iPhone running older versions of iOS, Xcode requires the iPhone to remain connected to the Mac in order to develop on any model of Apple Watch. Before installing your app, perform a few additional steps: Specify your Apple ID in the Account preferences in Xcode. Specify a valid team in your project’s Signing & Capabilities pane. Code sign your macOS app if it includes capabilities that require code signing; see Adding capabilities to your app. Register the device with your team if you belong to the Apple Developer Program. Enable Developer Mode on an iOS, watchOS, or visionOS device, as described in Enabling Developer Mode on a device. Note You don’t need to configure a Mac device to run your macOS apps. Similarly, to run the macOS version of an iPad app, choose My Mac (the Mac running Xcode) as the device. Click the Run button in the toolbar or choose Product > Run to build and run the app on the selected simulated or real device. View the status of the build in the activity area of the toolbar. If the build is successful, Xcode runs the app and opens a debugging session in the debug area. Use the controls in the debug area to step through your code, inspect variables, and interact with the debugger. If the build is unsuccessful, click the indicators in the activity area to read the error or warning messages in the Issue navigator. Alternatively, choose View > Navigators > Show Issue Navigator to view the messages. When you’re done testing the app, click the Stop button in the toolbar. If you choose a simulated device as the run destination, Simulator launches and displays a window that corresponds to the simulated environment. For some devices, Simulator surrounds the screen content with a shell that resembles the target device. In visionOS, it displays a synthetic space to mimic the experience someone would have when they wear the device. Each device shell and space has specific controls to support interactions. For device-specific details, see the reference on interactions. Interacting with your app in the iOS and iPadOS simulator Interacting with your app in the tvOS simulator Interacting with your app in the watchOS simulator Interacting with your app in the visionOS simulator       Specify your Apple ID in the Account preferences in Xcode.
Specify a valid team in your project’s Signing & Capabilities pane.
Code sign your macOS app if it includes capabilities that require code signing; see Adding capabilities to your app.
Register the device with your team if you belong to the Apple Developer Program.
Enable Developer Mode on an iOS, watchOS, or visionOS device, as described in Enabling Developer Mode on a device. Interacting with your app in the iOS and iPadOS simulator
Interacting with your app in the tvOS simulator
Interacting with your app in the watchOS simulator
Interacting with your app in the visionOS simulator "
31,"Interacting with your app in the visionOS simulator  Overview           Interact with your app Navigate the space Switch between simulated scenes                                           Use Simulator to run apps in visionOS without installing them on a physical device. When you run your app in Simulator, you can see a monoscopic view of your app’s windows and 3D content inside an immersive space. Use your Mac to alter the viewpoint of the app within the space and navigate the app’s interface.  To use your Mac’s pointer and keyboard to create gestures, choose “Select to interact with the scene” from the buttons at the bottom-right of Simulator’s window. The current gaze position tracks your pointer movements when you hover over content within the space. Use the following actions to trigger gestures: Gesture To simulate Tap Click and release the primary mouse button, or tap the trackpad with one finger. Double-tap Double-click and release the primary mouse button, or double-tap the trackpad with one finger. Touch and hold Click and hold the primary mouse button, or touch and hold the trackpad with one finger. Drag Click the primary mouse button and move the mouse, or touch and drag your finger on the trackpad. Drag toward you Hold the Shift key while you click and drag down. Two-handed gestures Press Option to display touch points. Move the pointer while pressing the Option key to change the distance between the touch points. Move the pointer and hold the Shift and Option keys to reposition the touch points. Activate device buttons using menu items or by clicking the controls in Simulator’s window toolbar. To use your Mac’s pointer to reposition your viewpoint left, right, up or down, drag inside Simulator’s window using two fingers on a trackpad or Magic Mouse. You can also choose “Click and drag to pan the camera” from the buttons at the bottom-right of Simulator’s window to use a click and drag gesture to reposition your viewpoint. To move the viewpoint forward or backward, choose “Click and drag to dolly the camera” from the buttons at the bottom-right of Simulator’s window and click and drag up or down inside Simulator’s window or perform a pinch gesture moving two fingers toward or away from each other on a trackpad. To change your viewing angle, hold the secondary mouse button or click and hold with two fingers on a trackpad and drag inside Simulator’s window. You can also choose “Click and drag to tilt the camera” from the buttons at the bottom-right of Simulator’s window to use a click and drag gesture to change the viewing angle. To reset the viewpoint and viewing angle, choose the Reset Camera button from the toolbar at the top-right of Simulator’s window. Note To capture the input from the pointer and keyboard, bypassing navigation control, and direct the input to the OS, use the Pointer Capture and Keyboard Capture buttons in the toolbar at the top-right of Simulator’s window. Click the Esc (Escape) key to disable capture and restore navigation controls. When moving with a trackpad or Magic Mouse, Simulator respects the natural scrolling setting on macOS. You can also use a game controller to control your movement. Use the left stick to move left, right, forward or back. Use R2 and L2 to move up and down. Use the right stick on a game controller to pan around the space. Simulator provides three types of built-in scenes you can use as simulated passthrough. Each scene has different lighting conditions, room layout, and furnitures for different testing scenarios. Use the simulated scene to test: Readability of your app in varying backgrounds and varying lighting conditions. Different scenarios, including limited and cluttered surroundings, to see how your app adapts to them. Snapping to different surface anchors. Content layout, positioning, and scale. Spatial audio and acoustics. To change the simulated scene, click and hold the Simulated Scenes button in the toolbar at the top-right of Simulator’s window and choose the scene you want.       Readability of your app in varying backgrounds and varying lighting conditions.
Different scenarios, including limited and cluttered surroundings, to see how your app adapts to them.
Snapping to different surface anchors.
Content layout, positioning, and scale.
Spatial audio and acoustics. "
32,"Bringing your existing apps to visionOS  Overview See Also          Add visionOS as a supported destination for your app Clean up code that uses deprecated APIs Isolate features that are unavailable in visionOS Update your interface to take advantage of visionOS features Update your app’s assets Decide whether to port your app at all iOS migration and compatibility                                          If you have an existing app that runs in iPadOS or iOS, you can build that app against the visionOS SDK to run it on the platform. Apps built specifically for visionOS adopt the standard system appearance, and they look more natural on the platform. Updating your app is also an opportunity to add elements that work well on the platform, such as 3D content and immersive experiences. In most cases, all you need to do to support visionOS is update your Xcode project’s settings and recompile your code. Depending on your app, you might need to make additional changes to account for features that are only found in the iOS SDK. While most of the same technologies are available on both platforms, some technologies don’t make sense or require hardware that isn’t present on visionOS devices. For example, people don’t typically use a headset to make contactless payments, so apps that that use the ProximityReader framework must disable those features when running in visionOS. Note If you use ARKit in your iOS app to create an augmented reality experience, you need to make additional changes to support ARKit in visionOS. For information on how to update this type of app, see Bringing your ARKit app to visionOS. The first step to updating your app is to add visionOS as a supported destination. In your project’s settings, select your app target and navigate to the General tab. In Supported Destinations, click the Add (+) button to add a new destination and select the Apple Vision option. Adding this option lets you build your app specifically for the visionOS SDK.  When you add Apple Vision as a destination, Xcode makes some one-time changes to your project’s build settings. After you add the destination, you can modify your project’s build settings and build phases to customize the build behavior specifically for visionOS. For example, you might remove dependencies for the visionOS version of your app, or change the set of source files you want to compile. For more information about how to update a target’s configuration, see Customizing the build phases of a target. Fix any deprecation warnings in the iOS version of your code before you build for visionOS. Apple marks APIs as deprecated when they are no longer relevant or a suitable replacement exists. When you compile code that calls deprecated APIs, the compiler generates warnings and often suggests replacements for you to use instead. visionOS removed many deprecated symbols entirely, turning these deprecation warnings into missing-symbol errors on the platform. Make changes in the iOS version of your app to see the original deprecation warning and replacement details. In addition to individual symbols, the following frameworks are deprecated in their entirety in both iOS and visionOS. If your app still uses these frameworks, stop using them immediately. The reference documentation for each framework includes information about how to update your code. Accounts Address Book Address Book UI Assets Library iAd Newsstand Kit NotificationCenter OpenGL ES The iOS SDK includes many frameworks that don’t apply to visionOS, either because they use hardware that isn’t available or their features don’t apply to the platform. Move code that uses these frameworks to separate source files whenever possible, and include those files only in the iOS version of your app. When you can’t isolate the code to separate source files, use conditional statements such as the ones below to offer a different code path for visionOS and iOS. When separating code for visionOS and iOS, understand that conditional checks for iOS also return true in visionOS, so place visionOS conditions first and execute the iOS code only if visionOS isn’t present. The following example shows how to configure conditional statements to execute separate code paths in visionOS and iOS: The following frameworks are available in the iOS SDK but not in the visionOS SDK.    ActivityKit AdSupport AppClip AutomatedDeviceEnrollment BusinessChat CarKey CarPlay CoreLocationUI CoreNFC CoreTelephony DeviceActivity DockKit DriverKit ExposureNotification FamilyControls ManagedSettings ManagedSettingsUI Messages MLCompute OpenAL ProximityReader RoomPlan SafetyKit SCSIPeripheralsDriverKit SensorKit ServiceManagement Social Twitter WidgetKit  Some frameworks have behavioral changes that impact your app in visionOS, and some frameworks disable features when the required hardware is unavailable. To help you avoid using APIs for missing features, many frameworks offer APIs to check the availability of those features. Continue to use those APIs and take appropriate actions when the features aren’t available. In other cases, be prepared for the framework code to do nothing or to generate errors when you use it. ARKit. This framework requires you to use different APIs for iOS and visionOS. For more information, see Bringing your ARKit app to visionOS. AutomaticAssessmentConfiguration. The framework returns an error if you try to start a test in visionOS. AVFoundation. Capture interfaces aren’t available in visionOS. Use availability checks to determine which services are present. CallKit. You may continue to offer Voice-over-IP (VoIP) services, but phone number verification, call-blocking, and other cellular-related services are unavailable. ClockKit. The APIs of this framework do nothing in visionOS. CoreHaptics. visionOS plays audio feedback instead of haptic feedback. CoreLocation. You can request someone’s location using the standard location service, but most other services are unavailable. Use availability checks to determine which services are present. The Always authorization level is unavailable and automatically becomes When in Use authorization. CoreMotion. Barometer data is unavailable, but most other sensors are available. Use availability checks to determine which sensors you can use. HealthKit and HealthKitUI. Health data is unavailable. Use availability checks to determine when information is available. MapKit. User-tracking features that involve heading information aren’t available. MediaPlayer. Some APIs are unavailable in visionOS. MetricKit. You can gather on-device diagnostic logs and generate reports, but you can’t gather metrics. MusicKit. Some APIs are unavailable in visionOS. NearbyInteraction. The framework does nothing in visionOS. Use availability checks to determine when services are present. PushToTalk. Push to Talk services are unavailable. Check for errors when creating a PTChannelManager. SafariServices. A link that presents a SFSafariViewController now opens a new scene in the Safari app. ScreenTime. The APIs of this framework do nothing in visionOS. SensorKit. The APIs of this framework do nothing in visionOS. UIKit. The system reports a maximum of two simultaneous touch inputs — one for each of the person’s hands. All system gesture recognizers handle these inputs correctly, including for zoom and rotation gestures that require multiple fingers. If you have custom gesture recognizers that require more than two fingers, update them to support only one or two touches in visionOS. VisionKit. The DataScannerViewController APIs are unavailable, but other features are still available. WatchConnectivity. The framework supports connections only between an iPhone and Apple Watch. Use availability checks to determine when services are available. For additional information about how to isolate code to the iOS version of your app, see Running code on a specific platform or OS version. After your existing code runs correctly in visionOS, look for ways to improve the experience you offer on the platform. In visionOS, you can display content using more than just windows. Think about ways to incorporate the following elements into your interface: Depth. Many SwiftUI and UIKit views use visual effects to add depth. Look for similar ways to incorporate depth into your own custom views. 3D content. Think about where you might incorporate 3D models and shapes into your content. Use RealityKit to implement your content, and a RealityView to present that content from your app. See Adding 3D content to your app. Immersive experiences. Present a space to immerse someone in your app’s content. Spaces let you place content anywhere in a person’s surroundings. You can also create fully immersive experiences that display only your app’s content. See Creating fully immersive experiences in your app. Interactions with someone’s surroundings. Use ARKit to facilitate interactions between your content and the surroundings. For example, detect planar surfaces to use as anchor points for your content. See ARKit. If you built your interface using UIKit, you can still load iOS storyboards into your app, but you can’t customize your interface for visionOS or include 3D content. To include visionOS content in your app, programmatically add your SwiftUI views using UIHostingController or UIViewRepresentable. Alternatively, migrate the relevant parts of your interface to SwiftUI. Moving your interface to SwiftUI gives you less code to maintain and makes it easier to validate that your interface does what you want. For information about mixing SwiftUI and UIKit content, see UIKit integration in SwiftUI. For guidance on how best to incorporate depth and 3D elements in your interface, see Human Interface Guidelines. Add vector-based or high-resolution images to your project specifically to support visionOS. In visionOS, people can view your app’s content at different angles and different distances, so image pixels rarely line up with screen pixels. Vector-based images work best because they maintain their detail and crispness at any size. For bitmap-based images, use high-resolution images (@2x or better) to ensure your images retain detail at different sizes. For more information about designing images for your app, see Images in Human Interface Guidelines. In some cases, it might not make sense to port your app for visionOS. For example, don’t port the following types of apps: Apps that act as containers for app extensions. This includes apps where the primary purpose is to deliver custom keyboard extensions, device drivers, sticker packs, SMS and MMS message filtering extensions, call directory extensions, or widgets. Movement-based apps. This includes apps that follow a person’s location changes, such as apps that offer turn-by-turn directions or navigation. It also includes apps that track body movements. Selfie or photography apps. This includes apps where the primary purpose is to capture images or video from the device’s cameras. If your app uses an unsupported feature but can function without it, you can still bring your app to visionOS. Remove features that aren’t available and focus on bringing the rest of your content to the platform. For example, if you have an app that lets people write down notes and take pictures to include with those notes, disable the picture-taking ability in visionOS but let people add text and incorporate images they already have.       Accounts
Address Book
Address Book UI
Assets Library
iAd
Newsstand Kit
NotificationCenter
OpenGL ES ARKit. This framework requires you to use different APIs for iOS and visionOS. For more information, see Bringing your ARKit app to visionOS.
AutomaticAssessmentConfiguration. The framework returns an error if you try to start a test in visionOS.
AVFoundation. Capture interfaces aren’t available in visionOS. Use availability checks to determine which services are present.
CallKit. You may continue to offer Voice-over-IP (VoIP) services, but phone number verification, call-blocking, and other cellular-related services are unavailable.
ClockKit. The APIs of this framework do nothing in visionOS.
CoreHaptics. visionOS plays audio feedback instead of haptic feedback.
CoreLocation. You can request someone’s location using the standard location service, but most other services are unavailable. Use availability checks to determine which services are present. The Always authorization level is unavailable and automatically becomes When in Use authorization.
CoreMotion. Barometer data is unavailable, but most other sensors are available. Use availability checks to determine which sensors you can use.
HealthKit and HealthKitUI. Health data is unavailable. Use availability checks to determine when information is available.
MapKit. User-tracking features that involve heading information aren’t available.
MediaPlayer. Some APIs are unavailable in visionOS.
MetricKit. You can gather on-device diagnostic logs and generate reports, but you can’t gather metrics.
MusicKit. Some APIs are unavailable in visionOS.
NearbyInteraction. The framework does nothing in visionOS. Use availability checks to determine when services are present.
PushToTalk. Push to Talk services are unavailable. Check for errors when creating a PTChannelManager.
SafariServices. A link that presents a SFSafariViewController now opens a new scene in the Safari app.
ScreenTime. The APIs of this framework do nothing in visionOS.
SensorKit. The APIs of this framework do nothing in visionOS.
UIKit. The system reports a maximum of two simultaneous touch inputs — one for each of the person’s hands. All system gesture recognizers handle these inputs correctly, including for zoom and rotation gestures that require multiple fingers. If you have custom gesture recognizers that require more than two fingers, update them to support only one or two touches in visionOS.
VisionKit. The DataScannerViewController APIs are unavailable, but other features are still available.
WatchConnectivity. The framework supports connections only between an iPhone and Apple Watch. Use availability checks to determine when services are available. Depth. Many SwiftUI and UIKit views use visual effects to add depth. Look for similar ways to incorporate depth into your own custom views.
3D content. Think about where you might incorporate 3D models and shapes into your content. Use RealityKit to implement your content, and a RealityView to present that content from your app. See Adding 3D content to your app.
Immersive experiences. Present a space to immerse someone in your app’s content. Spaces let you place content anywhere in a person’s surroundings. You can also create fully immersive experiences that display only your app’s content. See Creating fully immersive experiences in your app.
Interactions with someone’s surroundings. Use ARKit to facilitate interactions between your content and the surroundings. For example, detect planar surfaces to use as anchor points for your content. See ARKit. Apps that act as containers for app extensions. This includes apps where the primary purpose is to deliver custom keyboard extensions, device drivers, sticker packs, SMS and MMS message filtering extensions, call directory extensions, or widgets.
Movement-based apps. This includes apps that follow a person’s location changes, such as apps that offer turn-by-turn directions or navigation. It also includes apps that track body movements.
Selfie or photography apps. This includes apps where the primary purpose is to capture images or video from the device’s cameras. true #if os(xrOS)
   // visionOS code
#elseif os(iOS)
   // iOS code
#endif PTChannelManager SFSafariViewController DataScannerViewController RealityView UIHostingController UIViewRepresentable @2x "
33,"Bringing your ARKit app to visionOS  Overview See Also          Adopt technologies available in both iOS and visionOS Convert 3D assets to the USDZ format Update your interface to support visionOS Replace your ARKit code Isolate ARKit features not available in visionOS iOS migration and compatibility                                          If you use ARKit to create an augmented reality experience on iPhone or iPad, you need to rethink your use of that technology when bringing your app to visionOS. ARKit plays a crucial role in delivering your content to the display in iPadOS and iOS. In visionOS, you use ARKit only to acquire data about the person’s surroundings, and you do so using a different set of APIs. In visionOS, you don’t need a special view to display an augmented reality interface. Build windows with your app’s content using SwiftUI or UIKit. When you display those windows, visionOS places them in the person’s surroundings for you. If you want to control the placement of any 2D or 3D content in the person’s surroundings, build your content using SwiftUI and RealityKit. When migrating your app to visionOS, reuse as much of your app’s existing content as you can. visionOS supports most of the same technologies as iOS, so you can reuse project assets, 3D models, and most custom views. Don’t reuse your app’s ARKit code or any code that relies on technologies visionOS doesn’t support. For general guidance on how to port apps to visionOS, see Bringing your existing apps to visionOS. To create a single app that runs in both iOS and visionOS, use technologies that are available on both platforms. While ARKit in iOS lets you create your interface using several different technologies, the preferred technologies in visionOS are SwiftUI and RealityKit. If you’re not currently using RealityKit for 3D content, consider switching to it before you start adding visionOS support. If you retain code that uses older technologies in your iOS app, you might need to re-create much of that code using RealityKit when migrating to visionOS. If you use Metal to draw your app’s content, you can bring your code to visionOS to create content for 2D views or to create fully immersive experiences. You can’t use Metal to create 3D content that integrates with the person’s surroundings. This restriction prevents apps from sampling pixels of the person’s surroundings, which might contain sensitive information. For information on how to create a fully immersive experience with Metal, see Drawing fully immersive content using Metal. The recommended format for 3D assets in iOS and visionOS is USDZ. This format offers a compact single file for everything, including your models, textures, behaviors, physics, anchoring, and more. If you have assets that don’t use this format, use the Reality Converter tool that comes with Xcode to convert them for your project. When building 3D scenes for visionOS, use Reality Composer Pro to create your scenes that incorporate your USDZ assets. With Reality Composer Pro, you can import your USD files and edit them in place, nondestructively. If your iOS app applies custom materials to your assets, convert those materials to shader graphs in the app. Although you can bring models and materials to your project using USDZ files, you can’t bring custom shaders you wrote using Metal. Replace any custom shader code with MaterialX shaders. Many digital content creation tools support the MaterialX standard, and let you create dynamic shaders and save them with your USDZ files. Reality Composer Pro and RealityKit support MaterialX shaders, and incorporate them with your other USDZ asset content. For more information about MaterialX, see https://materialx.org. In visionOS, you manage your app’s content, and the system handles the integration of that content with the person’s surroundings. This approach differs from iOS, where you use a special ARKit view to blend your content and the live camera content. Bringing your interface to visionOS therefore means you need to remove this special ARKit view and focus only on your content. If you can display your app’s content using SwiftUI or UIKit views, build a window with those views and present it from your visionOS app. If you use other technologies to incorporate 2D or 3D content into the person’s surroundings, make the following substitutions in the visionOS version of your app. If you create your AR experience using: Update to: RealityKit and ARView RealityKit and RealityView SceneKit and ARSCNView RealityKit and RealityView SpriteKit and ARSKView RealityKit or SwiftUI A RealityView is a SwiftUI view that manages the content and animations you create using RealityKit and Reality Composer Pro. You can add a RealityView to any of your app’s windows to display 2D or 3D content. You can also add the view to an doc://com.apple.documentation/documentation/swiftui/immersivespace scene, which you use to integrate your RealityKit content into the person’s surroundings. Note You can load iOS storyboards into a visionOS app, but you can’t customize your interface for visionOS or include 3D content. If you want to share interface files between iOS and visionOS, adopt SwiftUI views or create your interface programmatically. For more information about how to use RealityView and respond to interactions with your content, see Adding 3D content to your app. ARKit provides different APIs for iOS and visionOS, and the way you use ARKit services on the platforms is also different. In iOS, you must use ARKit to put your content onscreen, and you can also use it to manage interactions between your content and a person’s surroundings. In visionOS, the system puts your content onscreen, so you only use ARKit to manage interactions with the surroundings. Because of this more limited usage, some apps don’t need ARKit at all in visionOS. The only time you use ARKit in visionOS is when you need one of the following services: Plane detection Image tracking Scene reconstruction Hand tracking World tracking and device-pose prediction Use plane detection, image tracking, and scene reconstruction to facilitate interactions between your app’s virtual content and real-world items. For example, use plane detection to detect a tabletop on which to place your content. Use world tracking to record anchors that you want to persist between launches of your app. Use hand tracking if your app requires custom hands-based input. To start ARKit services in your app, create an ARKitSession object and run it with the data providers for each service. Unlike ARKit in iOS, services in visionOS are independent of one another, and you can start and stop each one at any time. The following example shows how to detect horizontal and vertical planes. Data providers deliver new information using an asynchronous sequence. If you use the world-tracking data provider in visionOS, ARKit automatically persists the anchors you add to your app’s content. You don’t need to persist these anchors yourself. For more information about how to use ARKit, see ARKit. If your app uses ARKit features that aren’t present in visionOS, isolate that code to the iOS version of your app. The following features are available in iOS, but don’t have an equivalent in visionOS: Face tracking Body tracking Geotracking and placing anchors using a latitude and longitude Object detection App Clip Code detection Video frame post-processing Although whole body tracking isn’t available in visionOS, you can track the hands of the person wearing the device. Hand gestures are an important way of interacting with content in visionOS. SwiftUI handles common types of interactions like taps and drags, but you can use custom hand tracking for more complex gestures your app supports. If you use ARKit raycasting in iOS to detect interactions with objects in the person’s surroundings, you might not need that code in visionOS. SwiftUI and RealityKit handle both direct and indirect interactions with your app’s content in 3D space, eliminating the need for raycasting in many situations. In other situations, you can use the features of ARKit and RealityKit to manage interactions with your content. For example, you might use ARKit hand tracking to determine where someone is pointing in the scene, and use scene reconstruction to build a mesh you can integrate into your RealityKit content.       Plane detection
Image tracking
Scene reconstruction
Hand tracking
World tracking and device-pose prediction Face tracking
Body tracking
Geotracking and placing anchors using a latitude and longitude
Object detection
App Clip Code detection
Video frame post-processing ARView RealityView ARSCNView RealityView ARSKView RealityView RealityView RealityView ARKitSession let session = ARKitSession()
let planeData = PlaneDetectionProvider(alignments: [.horizontal, .vertical])


Task {
    try await session.run([planeData])
    
    for await update in planeData.anchorUpdates {
        switch update.event {
        case .added, .updated:
            // Update plane representation.
            print(""Updated planes."")
        case .removed:
            // Indicate plane removal.
            print(""Removed plane."")
        }
    }
} "
34,"Checking whether your existing app is compatible with visionOS  Overview See Also          Determine whether a missing feature impacts your app Test specific scenarios before uploading your app iOS migration and compatibility                                          visionOS runs compatible iPad and iPhone apps to provide continuous access to existing content right away. visionOS supports most of the same technologies as iOS, so many apps built to run on iPad or iPhone can run unmodified on visionOS devices. When a compatible app runs in visionOS, it retains the same appearance it had in iPadOS or iOS, and its content appears in a window in the person’s surroundings. If you have an app in the iOS App Store, try downloading it and running it on Apple Vision Pro. If you run into issues, use Xcode to identify and fix them. If you built your app using the iOS SDK, Xcode 15 and later automatically adds a Designed for iPad runtime destination to your project. Use this destination to run your app and test its compatibility in visionOS. You can test most of your app’s core functionality in Simulator, but some features are available only on a device. visionOS contains most of the same technologies as iPadOS and iOS, but there are some differences. In some cases, a feature you use in your app might not be available because of hardware differences or because of differences in how people use a visionOS device. As part of your testing, consider the impact of any missing features on your app’s overall experience. Whenever possible, work around missing features by disabling them or providing alternate ways to access the same content. The following features aren’t available in compatible iPad and iPhone apps in visionOS. Use framework APIs to determine when the features are available. Core Motion services Barometer and magnetometer data All location services except the standard service HealthKit data Video or still-photo capture Camera features like auto-focus or flash Rear-facing (selfie) cameras In some cases, a framework or feature behaves differently when your app runs in visionOS. Be prepared to handle these differences when your app runs in visionOS. AirPlay. visionOS hides AirPlay sharing buttons in system interfaces, and you can’t use AirPlay features from compatible apps. App extensions. visionOS doesn’t load App Clips, device drivers, device activity monitors, keyboard extensions, Messages app extensions, photo-editing app extensions, SMS and call-reporting extensions, or widgets. Apple Watch features. visionOS ignores watchOS apps and WatchKit extensions in your iOS or iPadOS app. The Watch Connectivity framework is unavailable. Face sharing in ClockKit does nothing in visionOS. Audio and video. visionOS doesn’t support Picture in Picture or AV routing features. Check the availability of video features before using them. Be prepared for audio playback to stop automatically when your app moves to the background. Classroom features. Starting a test with Automatic Assessment Configuration reports an error. Cellular telephony. Cellular services are unavailable. You can still implement Voice-over-IP (VoIP) services using CallKit and Core Telephony. Device management. Calls to the ManagedSettings and ManagedSettingsUI frameworks do nothing. Game controllers. visionOS delivers game controller events only when someone is looking at the app. Handoff. visionOS doesn’t attempt to hand off user activities to other devices. Haptics. visionOS plays sounds instead of haptics. HomeKit. You can’t add accessories using a QR code from a visionOS device. Metrics. You can gather on-device diagnostic logs and generate reports, but you can’t gather metrics. Multi-Touch. The system reports a maximum of two simultaneous touch inputs — one for each of the person’s hands. All system gesture recognizers handle these inputs correctly, including for zoom and rotation gestures that require multiple fingers. If you have custom gesture recognizers that require more than two points of interaction, update them to support only one or two touches in visionOS. Parental controls. Calls to the FamilyControls framework do nothing. PencilKit. visionOS doesn’t report touches of type UITouch.TouchType.pencil, but it does report other types of touches. Push to Talk. Calls to the Push to Talk framework do nothing. Safari Services. Links that present an SFSafariViewController open a Safari scene instead. ScreenTime. Calls to the Screen Time framework do nothing. Sensor-related features. Calls to the SensorKit framework do nothing. Social media. Calls to the Social framework do nothing. System interfaces. Authorization prompts, Sign in with Apple prompts, and other system-provided interfaces run asynchronously outside of your app’s process. Because these interfaces don’t run modally in your app, your app might not receive immediate responses. Vehicle features. The system doesn’t call your app’s CarPlay code. Calls you make using CarKey do nothing. Vision. Data scanners do nothing in VisionKit. The version of ARKit in iOS is incompatible with the one in visionOS and visionOS can’t display windows that contain ARKit views. For information about how to bring an ARKit app to visionOS, see Bringing your ARKit app to visionOS. For details about how to handle missing features in your code, see Making your existing app compatible with visionOS. The following App Store features for iOS continue to work when your app runs in visionOS: In-app purchases and subscriptions App capabilities and entitlements On-demand resources App thinning When you use app thinning to optimize your app for different devices and operating systems, the App Store selects the resources and content that offer the best fit for visionOS devices. It then removes any other resources to create a streamlined installation of your app. When you export your app from Xcode 15 or later, you can test the thinning support using the visionOS virtual thinning target. When you’re ready to distribute your app, create an archive and export it using the Ad-Hoc or Development distribution method. During the export process, Xcode creates an appropriately signed app for you to distribute to your testers. For more information, see Distributing your app to registered devices.       Core Motion services
Barometer and magnetometer data
All location services except the standard service
HealthKit data
Video or still-photo capture
Camera features like auto-focus or flash
Rear-facing (selfie) cameras AirPlay. visionOS hides AirPlay sharing buttons in system interfaces, and you can’t use AirPlay features from compatible apps.
App extensions. visionOS doesn’t load App Clips, device drivers, device activity monitors, keyboard extensions, Messages app extensions, photo-editing app extensions, SMS and call-reporting extensions, or widgets.
Apple Watch features. visionOS ignores watchOS apps and WatchKit extensions in your iOS or iPadOS app. The Watch Connectivity framework is unavailable. Face sharing in ClockKit does nothing in visionOS.
Audio and video. visionOS doesn’t support Picture in Picture or AV routing features. Check the availability of video features before using them. Be prepared for audio playback to stop automatically when your app moves to the background.
Classroom features. Starting a test with Automatic Assessment Configuration reports an error.
Cellular telephony. Cellular services are unavailable. You can still implement Voice-over-IP (VoIP) services using CallKit and Core Telephony.
Device management. Calls to the ManagedSettings and ManagedSettingsUI frameworks do nothing.
Game controllers. visionOS delivers game controller events only when someone is looking at the app.
Handoff. visionOS doesn’t attempt to hand off user activities to other devices.
Haptics. visionOS plays sounds instead of haptics.
HomeKit. You can’t add accessories using a QR code from a visionOS device.
Metrics. You can gather on-device diagnostic logs and generate reports, but you can’t gather metrics.
Multi-Touch. The system reports a maximum of two simultaneous touch inputs — one for each of the person’s hands. All system gesture recognizers handle these inputs correctly, including for zoom and rotation gestures that require multiple fingers. If you have custom gesture recognizers that require more than two points of interaction, update them to support only one or two touches in visionOS.
Parental controls. Calls to the FamilyControls framework do nothing.
PencilKit. visionOS doesn’t report touches of type UITouch.TouchType.pencil, but it does report other types of touches.
Push to Talk. Calls to the Push to Talk framework do nothing.
Safari Services. Links that present an SFSafariViewController open a Safari scene instead.
ScreenTime. Calls to the Screen Time framework do nothing.
Sensor-related features. Calls to the SensorKit framework do nothing.
Social media. Calls to the Social framework do nothing.
System interfaces. Authorization prompts, Sign in with Apple prompts, and other system-provided interfaces run asynchronously outside of your app’s process. Because these interfaces don’t run modally in your app, your app might not receive immediate responses.
Vehicle features. The system doesn’t call your app’s CarPlay code. Calls you make using CarKey do nothing.
Vision. Data scanners do nothing in VisionKit. In-app purchases and subscriptions
App capabilities and entitlements
On-demand resources
App thinning UITouch.TouchType.pencil SFSafariViewController "
35,"Making your existing app compatible with visionOS  Overview See Also          Perform availability and authorization checks before using features Handle environmental differences appropriately Audit your interface code Respond gracefully to missing features Remove code that uses deprecated APIs iOS migration and compatibility                                          A compatible iPadOS or iOS app links against the iOS SDK and runs in visionOS. Although visionOS provides a complete set of iOS frameworks for linking, some features of those frameworks might be unavailable due to hardware or usage differences. To ensure your app runs correctly in visionOS, handle any missing features gracefully and provide workarounds wherever possible. Some frameworks offer APIs to let you determine when framework features are available or whether your app is authorized to use them. Always check these APIs before you try to use the corresponding features, and don’t assume a feature is available because the necessary hardware is present. The device’s configuration also plays a role in determining the results of some availability and authorization checks, and features might not be present when your app runs in Simulator. If an availability or authorization check fails, don’t try to use the associated feature in your app. The following frameworks support availability or authorization checks: ActivityKit. Check the areActivitiesEnabled property of ActivityAuthorizationInfo to determine if Live Activities are authorized. ARKit. Check the isSupported property of your configuration object to determine availability of augmented reality features. In visionOS, ARKit views such as ARView are never available, so isolate interface code containing those views to the iOS version of your app. AVFoundation. Identify what cameras are available using the AVCaptureDevice.DiscoverySession class. Don’t assume the presence of specific cameras. Automatic Assessment Configuration. Check for error values when you configure an AEAssessmentSession object. Contacts. Use the CNContactStore class to determine your app’s authorization status. Core Bluetooth. Use the CBCentralManager and CBPeripheralManager classes to determine feature availability and your app’s authorization status. Core Haptics. Call the capabilitiesForHardware() method of the haptic engine to determine the available features. Core Location. Check the properties of CLLocationManager to determine the availability of location services. Core Motion. Check the properties of CMMotionManager to determine the availability of accelerometers, gyroscopes, magnetometers, and other hardware sensors. Core NFC. Check the readingAvailable property of your reader session to determine if NFC tag reading is available. EventKit. Use the EKEventStore class to determine your app’s authorization status. ExposureNotification. Use the ENManager class to determine your app’s authorization status. HealthKit. Use the HKHealthStore class to determine if health-related data is available. HomeKit. Check the properties of HMHomeManager to determine your app’s authorization status. Local Authentication. Use the LAContext class to determine the authentication policies you can use. Media Player. Use the MPMediaLibrary class to determine your app’s authorization status. Nearby Interaction. Check the deviceCapabilities property of your session to determine whether features are available. PhotoKit. Use the PHPhotoLibrary class to determine your app’s authorization status. ProximityReader. Check the isSupported property of the card reader object to determine if Tap to Pay on iPhone is available. ReplayKit. Check the isAvailable property of RPScreenRecorder to determine if screen recording support is available. RoomPlan. Check the isSupported property of the RoomCaptureSession object to determine if LiDAR scanning is available on the device. SensorKit. Use the SRSensorReader class to determine your app’s authorization status. Speech. Use the SFSpeechRecognizer class to determine if speech recognition is available. User Notifications. Use the getNotificationSettings(completionHandler:) method of UNUserNotificationCenter to determine your app’s authorization status. WatchConnectivity. Call the isSupported() method of the WCSession object to determine if the framework is available. Apple frameworks take a device-agnostic approach whenever possible to minimize issues when you use them on different device types. Apple devices come in a variety of shapes and sizes, and with different sets of features. Rather than build your app for a specific device, make sure it adapts to any device and can gracefully handle differences. During the design process, avoid assumptions that might break when a new device appears. In particular, don’t assume: The current device type or idiom is always iPhone, iPad, or iPod Touch. If you make decisions based on the current idiom, they might break when your app runs in visionOS. Eliminate decisions based on idiom, or provide reasonable defaults for unknown idioms. Device-specific hardware or features are available. Hardware and features can be unavailable for a variety of reasons. For example, a feature might be unavailable when your app runs in Simulator. Don’t assume a feature is available because your app supports a specific device type. Windows and views are a specific size, or at a specific location onscreen. Build your interface to adapt dynamically to any size using SwiftUI or Auto Layout. Assume the size of your app can change dynamically. The device has only one display. People can connect iPad and iPhone to an external display, and visionOS devices have two displays that work together to present the same content. You can identify a device based on the available frameworks or symbols. The presence or absence of frameworks or code symbols is an unreliable way to identify a device type, and can change in later software updates. Your app runs in the background. visionOS doesn’t support the location, external accessory, or bluetooth-peripheral background execution modes. Background apps are hidden. In visionOS, the windows of background apps remain onscreen, but are dimmed when no one looks at them. The only time app windows disappear is when one app presents an immersive space. When you make decisions using device details, your app might produce inconsistent or erroneous results on an unknown device type, or it might fail altogether. Find solutions that rely on environmental information, rather than the device type. For example, SwiftUI and UIKit start layout using the app’s window size, which isn’t necessarily the same size as the device’s display. Note Device-specific information is available when you absolutely need it, but validate the information you receive and provide reasonable default behavior for unexpected values. To minimize disruptions, visionOS runs your compatible iPad or iPhone app in an environment that matches an iPad as much as possible. Windows and views retain the same appearance that they have in iPadOS or iOS, and the system sizes your app’s window to fit an iPad whenever possible. When building your app’s interface, make choices that ensure your app runs well in visionOS too. Adopt the following best practices for your interface-related code: Support iPad and iPhone in the same app. Create one app that supports both device types, rather than separate apps for each device. SwiftUI and UIKit support adaptable interfaces, and Xcode provides tools to help you visualize your interface at different supported sizes. Organize your interface using scenes. Scenes are a fundamental tool for managing your app’s interface. Use the scene types in SwiftUI and UIKit to assemble and manage the views you display in windows. Adapt your interface to any size. Design your interface to adapt naturally to different sizes. For an introduction to SwiftUI views and layout, see Declaring a custom view. For information about laying out views in UIKit, see View layout. Don’t access screen details. visionOS provides reasonable values for UIScreen objects, but don’t use those values to make decisions. Specify the supported interface orientations. Add the UISupportedInterfaceOrientations key to your app’s Info.plist file to specify the interface orientations it supports. Support all interface orientations whenever possible. visionOS adds an interface rotation for your app button only when this key is present. Update hover effects in custom views. Hover effects convey the focused view or control in your interface. Standard system views apply hover effects as needed. For custom views and controls, verify that the hover effects look appropriate in visionOS. Add or update the content shape for your hover effects if needed. Adopt vector-based images when possible. Vector-based images scale well to different sizes while retaining a crisp appearance. If you use bitmap-based assets, make them the exact size you need. Don’t use oversized assets, which require extra work to display at the correct size. If you want visionOS to display your app’s interface in a particular orientation at launch, add the UIPreferredDefaultInterfaceOrientation key to your app’s Info.plist file. Set the value of the key to one of the values in your app’s UISupportedInterfaceOrientations key. For example, to specify a preference for a portrait orientation, set the value to UIInterfaceOrientationPortrait. Add ~ipad or ~iphone to the key name to specify device-specific orientation preferences. If your app relies on frameworks that behave differently in visionOS, update your code to handle those differences. Availability checks give you a clear indication when you can’t use a feature, but some frameworks might have more subtle behavior. Throughout your code, make sure you respond to unusual situations: Handle error conditions. If a function throws an exception or returns an error, handle the error. Use error information to adjust your app’s behavior or provide an explanation of why it can’t perform certain operations. Handle nil or empty values gracefully. Validate objects and return values before you try to use them. Update your interface. Provide appropriate messaging in your interface when a feature is missing, or remove feature-specific views entirely if you can do so cleanly. Don’t leave empty views where the feature was. For information about frameworks that behave differently in visionOS, see Checking whether your existing app is compatible with visionOS. If your app currently uses deprecated APIs or frameworks, update your code to use appropriate replacements. Deprecated symbols represent outdated features, and in some cases might not do anything when you call them. To prevent potential issues, replace them with modern equivalents to ensure your code behaves as expected. The following frameworks are deprecated in their entirety in iPadOS, iOS, and visionOS. If your app still uses these frameworks, move off of them immediately. The reference documentation for each framework includes information about how to update your code. Accounts Address Book Address Book UI Assets Library iAd Newsstand Kit NotificationCenter OpenGL ES       ActivityKit. Check the areActivitiesEnabled property of ActivityAuthorizationInfo to determine if Live Activities are authorized.
ARKit. Check the isSupported property of your configuration object to determine availability of augmented reality features. In visionOS, ARKit views such as ARView are never available, so isolate interface code containing those views to the iOS version of your app.
AVFoundation. Identify what cameras are available using the AVCaptureDevice.DiscoverySession class. Don’t assume the presence of specific cameras.
Automatic Assessment Configuration. Check for error values when you configure an AEAssessmentSession object.
Contacts. Use the CNContactStore class to determine your app’s authorization status.
Core Bluetooth. Use the CBCentralManager and CBPeripheralManager classes to determine feature availability and your app’s authorization status.
Core Haptics. Call the capabilitiesForHardware() method of the haptic engine to determine the available features.
Core Location. Check the properties of CLLocationManager to determine the availability of location services.
Core Motion. Check the properties of CMMotionManager to determine the availability of accelerometers, gyroscopes, magnetometers, and other hardware sensors.
Core NFC. Check the readingAvailable property of your reader session to determine if NFC tag reading is available.
EventKit. Use the EKEventStore class to determine your app’s authorization status.
ExposureNotification. Use the ENManager class to determine your app’s authorization status.
HealthKit. Use the HKHealthStore class to determine if health-related data is available.
HomeKit. Check the properties of HMHomeManager to determine your app’s authorization status.
Local Authentication. Use the LAContext class to determine the authentication policies you can use.
Media Player. Use the MPMediaLibrary class to determine your app’s authorization status.
Nearby Interaction. Check the deviceCapabilities property of your session to determine whether features are available.
PhotoKit. Use the PHPhotoLibrary class to determine your app’s authorization status.
ProximityReader. Check the isSupported property of the card reader object to determine if Tap to Pay on iPhone is available.
ReplayKit. Check the isAvailable property of RPScreenRecorder to determine if screen recording support is available.
RoomPlan. Check the isSupported property of the RoomCaptureSession object to determine if LiDAR scanning is available on the device.
SensorKit. Use the SRSensorReader class to determine your app’s authorization status.
Speech. Use the SFSpeechRecognizer class to determine if speech recognition is available.
User Notifications. Use the getNotificationSettings(completionHandler:) method of UNUserNotificationCenter to determine your app’s authorization status.
WatchConnectivity. Call the isSupported() method of the WCSession object to determine if the framework is available. The current device type or idiom is always iPhone, iPad, or iPod Touch. If you make decisions based on the current idiom, they might break when your app runs in visionOS. Eliminate decisions based on idiom, or provide reasonable defaults for unknown idioms.
Device-specific hardware or features are available. Hardware and features can be unavailable for a variety of reasons. For example, a feature might be unavailable when your app runs in Simulator. Don’t assume a feature is available because your app supports a specific device type.
Windows and views are a specific size, or at a specific location onscreen. Build your interface to adapt dynamically to any size using SwiftUI or Auto Layout. Assume the size of your app can change dynamically.
The device has only one display. People can connect iPad and iPhone to an external display, and visionOS devices have two displays that work together to present the same content.
You can identify a device based on the available frameworks or symbols. The presence or absence of frameworks or code symbols is an unreliable way to identify a device type, and can change in later software updates.
Your app runs in the background. visionOS doesn’t support the location, external accessory, or bluetooth-peripheral background execution modes.
Background apps are hidden. In visionOS, the windows of background apps remain onscreen, but are dimmed when no one looks at them. The only time app windows disappear is when one app presents an immersive space. Support iPad and iPhone in the same app. Create one app that supports both device types, rather than separate apps for each device. SwiftUI and UIKit support adaptable interfaces, and Xcode provides tools to help you visualize your interface at different supported sizes.
Organize your interface using scenes. Scenes are a fundamental tool for managing your app’s interface. Use the scene types in SwiftUI and UIKit to assemble and manage the views you display in windows.
Adapt your interface to any size. Design your interface to adapt naturally to different sizes. For an introduction to SwiftUI views and layout, see Declaring a custom view. For information about laying out views in UIKit, see View layout.
Don’t access screen details. visionOS provides reasonable values for UIScreen objects, but don’t use those values to make decisions.
Specify the supported interface orientations. Add the UISupportedInterfaceOrientations key to your app’s Info.plist file to specify the interface orientations it supports. Support all interface orientations whenever possible. visionOS adds an interface rotation for your app button only when this key is present.
Update hover effects in custom views. Hover effects convey the focused view or control in your interface. Standard system views apply hover effects as needed. For custom views and controls, verify that the hover effects look appropriate in visionOS. Add or update the content shape for your hover effects if needed.
Adopt vector-based images when possible. Vector-based images scale well to different sizes while retaining a crisp appearance. If you use bitmap-based assets, make them the exact size you need. Don’t use oversized assets, which require extra work to display at the correct size. Handle error conditions. If a function throws an exception or returns an error, handle the error. Use error information to adjust your app’s behavior or provide an explanation of why it can’t perform certain operations.
Handle nil or empty values gracefully. Validate objects and return values before you try to use them.
Update your interface. Provide appropriate messaging in your interface when a feature is missing, or remove feature-specific views entirely if you can do so cleanly. Don’t leave empty views where the feature was. Accounts
Address Book
Address Book UI
Assets Library
iAd
Newsstand Kit
NotificationCenter
OpenGL ES areActivitiesEnabled ActivityAuthorizationInfo isSupported ARView AVCaptureDevice.DiscoverySession AEAssessmentSession CNContactStore CBCentralManager CBPeripheralManager capabilitiesForHardware() CLLocationManager CMMotionManager readingAvailable EKEventStore ENManager HKHealthStore HMHomeManager LAContext MPMediaLibrary deviceCapabilities PHPhotoLibrary isSupported isAvailable RPScreenRecorder isSupported RoomCaptureSession SRSensorReader SFSpeechRecognizer getNotificationSettings(completionHandler:) UNUserNotificationCenter isSupported() WCSession UIScreen UISupportedInterfaceOrientations Info.plist UIPreferredDefaultInterfaceOrientation Info.plist UISupportedInterfaceOrientations UIInterfaceOrientationPortrait ~ipad ~iphone "
36,"Hello World  Overview See Also        Create an entry point into the app Present different modules using a navigation stack Display an interactive globe in a new scene Declare a volumetric window for the globe Open and dismiss the globe window Display objects that orbit the Earth Show Earth’s relationship to its satellites in an immersive space View the solar system from space using full immersion Related samples Related articles Related videos                                You can use visionOS scene types and styles to share information in fun and compelling ways. Features like volumes and immersive spaces let you put interactive virtual objects into people’s environments, or put people into a virtual environment. Hello World uses these tools to teach people about the Earth — the planet we call home. The app shows how the Earth’s tilt creates the seasons, how objects move as they orbit the Earth, and how Earth appears from space. The app uses SwiftUI to define its interface, including both 2D and 3D elements. To create, customize, and manage 3D models and effects, it also relies on the RealityKit framework and Reality Composer Pro. Hello World constructs the scene that it displays at launch — the first scene that appears in the WorldApp structure — using a WindowGroup: Like other platforms — for example, macOS and iOS — visionOS displays a window group as a familiar-looking window. In visionOS, people can resize and move windows around the Shared Space. Even if your app offers a sophisticated 3D experience, a window is a great starting point for an app because it eases people into the experience. It’s also a good place to provide instructions or controls. Tip This particular window group uses the doc://com.apple.documentation/documentation/SwiftUI/WindowStyle/plain window style to maintain control over the glass background effect that visionOS would otherwise automatically add. After you watch a brief introductory animation that shows the text Hello World typing in, the Modules view that defines the primary scene’s content presents options to explore different aspects of the world. This view contains a table of contents at the root of a NavigationStack: A visionOS navigation stack has the same behavior that it has in other platforms. When it first appears, the stack displays its root view. When someone chooses an embedded NavigationLink, the stack draws a new view and displays a back button in the toolbar. When someone taps the back button, the stack restores the previous view.  The trailing closure of the navigationDestination(for:destination:) view modifier in the code above displays a view when someone activates a link based on a module input that comes from the corresponding link’s initializer: The possible module values come from a custom Module enumeration: The globe module opens with a few facts about the Earth in the main window next to a decorative, flat image that supports the content. To help people understand even more, the module includes a button titled View Globe that opens a 3D interactive globe in a new window.  To be able to open multiple scene types, Hello World includes the UIApplicationSceneManifest key in its Information Property List file. The value for this key is a dictionary that includes the UIApplicationSupportsMultipleScenes key with a value of true: With the key in place, the app makes use of a second WindowGroup in its App declaration. This new window group uses the Globe view as its content: This window group creates a window that has arbitrary depth — great for displaying a 3D model in a bounded region that behaves like a transparent box — because Hello World uses the doc://com.apple.documentation/documentation/SwiftUI/WindowStyle/volumetric window style scene modifier. People can move this box around the Shared Space like any other window, and the content remains fixed inside. The doc://com.apple.documentation/documentation/SwiftUI/Scene/defaultSize(width:height:depth:in:) modifier specifies a size for the window in meters, including a depth dimension. The Globe view contains 3D content, but is still just a SwiftUI view. It contains two elements in a ZStack: a subview that draws a model of the Earth, and another that provides a control panel that people can use to configure the model’s appearance. The globe module presents a View Globe button that people can tap to display the volumetric window or dismiss the window, depending on the current state. Hello World achieves this behavior by creating a Toggle with the button style, and embedding it in a custom, reusable WindowToggle view.  When someone taps the toggle, the isShowing state changes, and the onChange(of:initial:_:) modifier calls the openWindow or dismissWindow action to open or dismiss the window, respectively. The view gets these actions from the environment and uses an identifier that matches the window’s identifier. You use windows in visionOS the same way you do in other platforms. But windows in visionOS provide a small amount of depth you can use to create 3D effects — like elements that appear in front of other elements. Hello World takes advantage of this depth to present small models inline with 2D content. The app’s second module, Objects in Orbit, provides information about objects that go around the Earth, like the Moon and artificial satellites. To give a sense of what these objects look like, the module displays 3D models of these items directly inside the window.  Hello World loads these models from the asset bundle using a Model3D structure inside a custom ItemView. The view scales and positions the model to fit the available space, and applies optional orientation adjustments: The app uses this ItemView once for each model, placing each in an overlay that only becomes visible based on the current selection. For example, the following overlay displays the satellite model with a small amount of tilt in the x-axis and z-axis: The VStack that contains the models also contains a Picker that people use to select a model to view: When you add 3D effects to a 2D window, keep this guidance in mind: Don’t overdo it. These kinds of effects add interest, but can unintentionally obscure important controls or information as people view the window from different directions. Ensure that elements don’t exceed the available depth. Excess depth causes elements to clip. Account for any position or orientation changes that might occur after initial placement. Avoid models intersecting with the backing glass. Again, account for potential movement after initial placement. People can visualize how satellites move around the Earth because the app’s orbit module displays the Earth, the Moon, and a communications satellite together as a single system. People can move the system anywhere in their environment or resize it using standard gestures. They can also move themselves around the system to get different perspectives.  Note To learn about designing with gestures in visionOS, read Gestures in Human Interface Guidelines. To create this visualization, the app displays the Orbit view — which contains a single RealityView that models the entire system — in an doc://com.apple.documentation/documentation/SwiftUI/ImmersiveSpace scene with the doc://com.apple.documentation/documentation/SwiftUI/ImmersionStyle/mixed immersion style: As with any secondary scene in a visionOS app, this scene depends on having the UIApplicationSupportsMultipleScenes key in the Information Property List file. The app also opens and closes the space using a generalized toggle view that resembles the one used for windows: There are a few key differences from the window equivalent of this toggle that appears in the section Open and dismiss the globe window: SpaceToggle uses doc://com.apple.documentation/documentation/SwiftUI/EnvironmentValues/openImmersiveSpace and doc://com.apple.documentation/documentation/SwiftUI/EnvironmentValues/dismissImmersiveSpace from the environment, rather than the window equivalents. The dismiss action in this case doesn’t require an identifier, because people can only open one space at a time, even across apps. The open and dismiss actions for spaces operate asynchronously, and so they appear inside a Task. The app’s final module gives people a sense of the Earth’s place in the solar system. Like other modules, this one includes information and a decorative image next to a button that leads to another visualization — in this case so people can experience Earth from space. When a person taps the button, the app takes over the entire display and shows stars in all directions, which you can see in the video at the right. The Earth appears directly in front, the Moon to the right, and the Sun to the left. The main window also shows a small control panel that people can use to exit the fully immersive experience.  Tip People can exit full immersion by pressing the device’s Digital Crown, but it’s typically useful when you provide a built-in mechanism to maintain control of the experience within your app. The app uses another immersive space scene for this module, but here with the doc://com.apple.documentation/documentation/SwiftUI/ImmersionStyle/full immersion style that turns off the passthrough video: This scene depends on the same UIApplicationSupportsMultipleScenes key that other secondary scenes do, and is activated by the same custom SpaceToggle that the previous section describes, but uses the Module.solar.id scene identifier in this case. To reuse the main window for the solar system controls, Hello World places both the navigation stack and the controls in a ZStack, and then sets the opacity of each to ensure that only one appears at a time:       Don’t overdo it. These kinds of effects add interest, but can unintentionally obscure important controls or information as people view the window from different directions.
Ensure that elements don’t exceed the available depth. Excess depth causes elements to clip. Account for any position or orientation changes that might occur after initial placement.
Avoid models intersecting with the backing glass. Again, account for potential movement after initial placement. SpaceToggle uses doc://com.apple.documentation/documentation/SwiftUI/EnvironmentValues/openImmersiveSpace and doc://com.apple.documentation/documentation/SwiftUI/EnvironmentValues/dismissImmersiveSpace from the environment, rather than the window equivalents.
The dismiss action in this case doesn’t require an identifier, because people can only open one space at a time, even across apps.
The open and dismiss actions for spaces operate asynchronously, and so they appear inside a Task. WorldApp WindowGroup WindowGroup(""Hello World"", id: ""modules"") {
    Modules()
        .environment(model)
}
.windowStyle(.plain) Modules NavigationStack NavigationStack(path: $model.navigationPath) {
    TableOfContents()
        .navigationDestination(for: Module.self) { module in
            ModuleDetail(module: module)
                .navigationTitle(module.eyebrow)
        }
} NavigationLink navigationDestination(for:destination:) module NavigationLink(value: module) { /* The link's label. */ } module Module enum Module: String, Identifiable, CaseIterable, Equatable {
    case globe, orbit, solar
    // ...
} globe UIApplicationSupportsMultipleScenes true <key>UIApplicationSceneManifest</key>
<dict>
    <key>UIApplicationSupportsMultipleScenes</key>
    <true/>
    <key>UISceneConfigurations</key>
    <dict/>
</dict> WindowGroup App Globe WindowGroup(id: Module.globe.name) {
    Globe()
        .environment(model)
}
.windowStyle(.volumetric)
.defaultSize(width: 0.6, height: 0.6, depth: 0.6, in: .meters) Globe ZStack Toggle WindowToggle private struct WindowToggle: View {
    var title: String
    var id: String
    @Binding var isShowing: Bool


    @Environment(\.openWindow) private var openWindow
    @Environment(\.dismissWindow) private var dismissWindow


    var body: some View {
        Toggle(title, isOn: $isShowing)
            .onChange(of: isShowing) { wasShowing, isShowing in
                if isShowing {
                    openWindow(id: id)
                } else {
                    dismissWindow(id: id)
                }
            }
            .toggleStyle(.button)
    }
} isShowing onChange(of:initial:_:) openWindow dismissWindow Model3D ItemView private struct ItemView: View {
    var item: Item
    var orientation: SIMD3<Double> = .zero


    var body: some View {
        Model3D(named: item.name, bundle: worldAssetsBundle) { model in
            model.resizable()
                .scaledToFit()
                .rotation3DEffect(
                    Rotation3D(
                        eulerAngles: .init(angles: orientation, order: .xyz)
                    )
                )
                .frame(depth: modelDepth)
                .offset(z: -modelDepth / 2)
        } placeholder: {
            ProgressView()
                .offset(z: -modelDepth * 0.75)
        }
    }
} ItemView .overlay {
    ItemView(item: .satellite, orientation: [0.15, 0, 0.15])
        .opacity(selection == .satellite ? 1 : 0)
} VStack Picker Picker(""Satellite"", selection: $selection) {
    ForEach(Item.allCases) { item in
        Text(item.name)
    }
}
.pickerStyle(.segmented) Orbit RealityView ImmersiveSpace(id: Module.orbit.name) {
    Orbit()
        .environment(model)
}
.immersionStyle(selection: $orbitImmersionStyle, in: .mixed) UIApplicationSupportsMultipleScenes private struct SpaceToggle: View {
    var title: String
    var id: String
    @Binding var isShowing: Bool


    @Environment(\.openImmersiveSpace) private var openImmersiveSpace
    @Environment(\.dismissImmersiveSpace) private var dismissImmersiveSpace


    var body: some View {
        Toggle(title, isOn: $isShowing)
            .onChange(of: isShowing) { wasShowing, isShowing in
                Task {
                    if isShowing {
                        await openImmersiveSpace(id: id)
                    } else {
                        await dismissImmersiveSpace()
                    }
                }
            }
            .toggleStyle(.button)
    }
} SpaceToggle Task ImmersiveSpace(id: Module.solar.name) {
    SolarSystem()
        .environment(model)
}
.immersionStyle(selection: $solarImmersionStyle, in: .full) UIApplicationSupportsMultipleScenes SpaceToggle Module.solar.id ZStack ZStack {
    SolarSystemControls()
        .opacity(model.isShowingSolar ? 1 : 0)


    NavigationStack(path: $model.navigationPath) {
        // ...
    }
    .opacity(model.isShowingSolar ? 0 : 1)
}
.animation(.default, value: model.isShowingSolar) "
37,"Destination Video  Overview See Also        Play video in an inline player Play video in a fullscreen player Configure the spatial audio experience Present an immersive space Provide a shared viewing experience Related samples Related articles Related videos                                Destination Video is a multiplatform video-playback app for visionOS, iOS, and tvOS. People get a familiar media-browsing experience navigating the libraryʼs content and playing videos they find interesting. The app provides a similar experience on supported platforms, but leverages unique features of visionOS to create a novel, immersive playback experience. When you select a video in the library, Destination Video presents a view that displays additional details about the item. The view presents controls to play the video and specify whether to include it in your Up Next list. In visionOS, it also displays a video poster along its leading edge. Tapping the view’s Preview button displays an inline preview of the video. When you present an AVPlayerViewController object’s interface as a child of another view, inline controls display, for example, pause, skip, and seek. Showing standard playback controls in your app provides a familiar UI that automatically adapts its appearance to fit each platform, and is the recommended choice in most cases. Destination Video uses a simple UI for the inline player view: a single button that toggles state of playback, and replays the content when it reaches the item’s end. AVPlayerViewController doesn’t provide this controls style, but the app uses it to display the video content without controls by setting the value of its showsPlaybackControls property to false. It then overlays the custom playback controls it requires. See the Destination Video’s InlinePlayerView type for details on how you can implement this. Note AVPlayerViewController only supports displaying 2D content when embedded inline. Present the player fullscreen to play 3D video. One of the most exciting features of visionOS is its ability to play 3D video along with spatial audio, which adds a deeper level of immersion to the viewing experience. Playing 3D content in your app requires that you display AVPlayerViewController in its full screen presentation. When you present the player fullscreen, the system automatically docks it into the ideal viewing position, and presents streamlined playback controls that keep the person’s focus on the content. Note In iOS or tvOS, you typically present video in a full-screen presentation using the fullScreenCover(isPresented:onDismiss:content:) modifier. You can present the player this way in visionOS, but the recommended way to present it fullscreen is to set it as the root view of your app’s window group. Destination Video’s ContentView displays the library view by default. It observes changes to the player model’s presentation property, which indicates whether the app requests inline or fullscreen playback. When the presentation state changes to fullScreen, the view redraws the UI to display the player view in place of the library: When someone selects the Play Video button on the detail screen, this calls the player model’s loadVideo(_: presentation:) method requesting the fullScreen presentation option: After the player model successfully loads the video content for playback, it updates its presentation value to fullScreen, which causes the app to replace the library with PlayerView. To dismiss the fullscreen player in visionOS, people tap the back button in the player UI. To handle this action, the app’s FullScreenPlayerView type defines an AVPlayerViewControllerDelegate object that handles the dismissal: When the delegate receives this call, it clears the media from the player model and resets the presentation state back to its default value, which results in the Destination Video app redisplaying the library view. Media playback apps require common configuration of their capabilities and audio session. In addition to performing the steps outlined in Configuring your app for media playback, Destination Video also adopts new AVAudioSession API to customize the a person’s spatial audio experience. After the app successfully loads a video for playback, it configures the spatial audio experience for the current presentation. For the inline player view, it sets the experience to a small front-focused sound stage where the audio comes from a person’s perception of front. When displaying a video full screen, it specifies automatic settings that let the system optimize the experience to best fit the video presentation. Building video playback apps for visionOS provides new opportunities to enhance the viewing experience beyond the bounds of the player window. To add a greater level of immersion, the sample presents an immersive space that displays a scene around a person as they watch the video. It defines the immersive space in the DestinationVideo app structure: The immersive space presents an instance of DestinationView, which maps a texture to the inside of a sphere that it displays around a person. The app presents it using the .progressive immersion style, which lets someone change their amount of immersion by turning the Digital Crown on the device. The Destination Video app automatically presents the immersive space when a person navigates to a video’s detail view, and dismisses it when they return to the library. To monitor these events, the app observes its navigation path to determine when a navigation event occurs so it can show or dismiss the space: One of the best ways to enhance your app’s playback experience is to make that experience shareable with others. You can use the AVFoundation and the GroupActivities frameworks to build SharePlay experiences that bring people together even when they can’t be in the same location. The Destination Video app creates an experience where people can watch videos with others across devices and platforms. It defines a group activity called VideoWatchingActivity that adopts the GroupActivity protocol. When people have a FaceTime call active and they play a video in the fullscreen player, it becomes eligible for playback for everyone on the call. The app’s VideoWatchingCoordinator actor manages Destination Video’s SharePlay functionality. It observes the activation of new VideoWatchingActivity sessions and when one starts, it sets the GroupSession instance on the player object’s AVPlaybackCoordinator: With the player configured to use the group session, when the app loads new videos, they become eligible to share with people in the FaceTime call.       AVPlayerViewController AVPlayerViewController showsPlaybackControls false InlinePlayerView AVPlayerViewController AVPlayerViewController fullScreenCover(isPresented:onDismiss:content:) ContentView presentation fullScreen struct ContentView: View {
    
    /// The library's selection path.
    @State private var selectionPath = [Video]()
    @EnvironmentObject private var player: PlayerModel
    
    var body: some View {
        switch player.presentation {
        case .fullScreen:
            PlayerView()
                .onAppear {
                    player.play()
                }
        default:
            LibraryView(path: $selectionPath)
                // Set a specific frame size in case the user resizes the video-player window.
                .frame(width: 960, height: 540)
        }
    }
} loadVideo(_: presentation:) fullScreen Button {
    /// Load the media item for full-screen presentation.
    player.loadVideo(video, presentation: .fullScreen)
} label: {
    Label(""Play Video"", systemImage: ""play.fill"")
} presentation fullScreen PlayerView FullScreenPlayerView AVPlayerViewControllerDelegate func playerViewController(_ playerViewController: AVPlayerViewController,
                          willEndFullScreenPresentationWithAnimationCoordinator coordinator: UIViewControllerTransitionCoordinator) {
  // Resets the player state, which dismisses the player view.
  player.reset()
} AVAudioSession /// Configures people's intended spatial audio experience to best fit the presentation.
func configureSpatialExperience(for presentation: Presentation) {
    do {
        let experience: AVAudioSessionSpatialExperience
        switch presentation {
        case .inline:
            // Set a small, front-focused experience when watching trailers.
            experience = .headTracked(soundStageSize: .small, anchoringStrategy: .front)
        case .fullScreen:
            // Set a large sound stage size when viewing fullscreen.
            experience = .headTracked(soundStageSize: .automatic, anchoringStrategy: .automatic)
        }
        try AVAudioSession.sharedInstance().setIntendedSpatialExperience(experience)
    } catch {
        logger.error(""Unable to set the intended spatial experience. \(error.localizedDescription)"")
    }
} DestinationVideo struct DestinationVideo: App {
    
    var body: some Scene {
        // The app's primary window.
        WindowGroup {
            ContentView()
        }


        // Defines an immersive space to present a destination in which to watch the video.
        ImmersiveSpace(for: Destination.self) { $destination in
            if let destination {
                DestinationView(destination)
            }
        }
        // Set the immersion style to progressive, so the person can use the Digital Crown to dial in their experience.
        .immersionStyle(selection: .constant(.progressive), in: .progressive)
    }
} DestinationView .progressive .onChange(of: navigationPath) {
    Task {
        // The selection path becomes empty when the person returns to the main library window.
        if navigationPath.isEmpty {
            if isSpaceOpen {
                // Dismiss the space and return the person to their real-world space.
                await dismissSpace()
                isSpaceOpen = false
            }
        } else {
            // The navigationPath has one video, or is empty.
            guard let video = navigationPath.first else { fatalError() }
            // Await the request to open the destination and set the state accordingly.
            switch await openSpace(value: video.destination) {
            case .opened: isSpaceOpen = true
            default: isSpaceOpen = false
            }
        }
    }
} VideoWatchingActivity GroupActivity VideoWatchingCoordinator VideoWatchingActivity GroupSession AVPlaybackCoordinator private var groupSession: GroupSession<VideoWatchingActivity>? {
    didSet {
        guard let groupSession else { return }
        // Set the group session on the AVPlayer object's's playback coordinator.
        // so it can synchronize playback with other devices.
        playbackCoordinator.coordinateWithSession(groupSession)
    }
} "
38,"Diorama  Overview See Also       Import assets for building the scene Create scenes containing the app’s entities Add assets to your scenes Add components to entities Use transforms to mark locations Load a scene at runtime Create the floating view Create attachments for points of interest Display point of interest attachments Create custom materials with Shader Graph Update the Shader Graph material at runtime Related samples Related articles Related videos                                Use Reality Composer Pro to compose, edit, and preview RealityKit content for your visionOS app. In your Reality Composer Pro project, you can create one or more scenes, each of which contains a hierarchy of virtual objects called entities that your app can efficiently load and display. In addition to helping you compose entity hierarchies, Reality Composer Pro also gives you the ability to add and configure components — even custom components that you’ve written — to the entities in your scenes. You can also design the visual appearance of entities using Shader Graph, a node-based visual tool for creating RealityKit materials. Shader Graph gives you a tremendous amount of control over the surface details and shape of entities. You can even create animated materials and dynamic materials that change based on the state of your app or user input. Diorama demonstrates many of RealityKit and Reality Composer Pro’s features. It displays an interactive, virtual topographical trail map, much like the real-world dioramas you find at trailheads and ranger stations in national parks. This virtual map has points of interest you can tap to bring up more detailed information. You can also smoothly transition between two trail maps: Yosemite and Catalina Island. Your Reality Composer Pro project must contain assets, which you use to compose scenes for your app. Diorama’s project has several assets, including 3D models like the diorama table, trail map, some birds and clouds that fly over the map, and a number of sounds and images. Reality Composer Pro provides a library of 3D models you can use. Access the library by clicking the Add (+) button on the right side of the toolbar. Selecting objects from the library imports them into your project.  Diorama uses custom assets instead of the available library assets. To use custom assets in your own Reality Composer Pro scenes, import them into your project in one of three ways: by dragging them to Reality Composer Pro’s project browser, using File > Import from the File menu, or copying the assets into the .rkassets bundle inside your project’s Swift package.  Note Although you can still load USDZ files and other assets directly in visionOS, RealityKit compiles assets in your Reality Composer Pro project into a binary format that loads considerably faster than loading from individual files. A single Reality Composer Pro project can have multiple scenes. A scene is an entity hierarchy stored in the project as a .usda file that you can load and display in a RealityView. You can use Reality Composer’s scenes to build an entire RealityKit scene, or to store reusable entity hierarchies that you can use as building block for composing scenes at runtime — the approach Diorama uses. You can add as many different scenes to your project as you need by selecting File > New > Scene, or pressing ⌘N. At the top of the Reality Composer Pro window, there’s a separate tab for every scene that’s currently open. To open a scene, double-click the scene’s .usda file in the project browser. To edit a scene, select its tab, and make changes using the hierarchy viewer, the 3D view, and the inspector.  RealityKit can only include entities in a scene, but it can’t use every type of asset that Reality Composer Pro supports as an entity. Reality Composer Pro automatically turns some assets, like 3D models, into an entity when you place them in a scene. It uses other assets indirectly. It uses image files, for example, primarily to define the surface details of model entities. Diorama uses multiple scenes to group assets together and then, at runtime, combines those scenes into a single immersive experience. For example, the diorama table has its own scene that includes the table, the map surface, and the trail lines. There are separate scenes for the birds that flock over the table, and for the clouds that float above it.  To add entities to a scene, drag assets from the project browser to the scene’s hierarchy view or 3D view. If the asset you drag is a type that can be represented as an entity, Reality Composer Pro adds it to your scene. You can select any asset in the scene hierarchy or the 3D view and change its location, rotation, and scale using the inspector on the right side of the window or the manipulator in the 3D view. RealityKit follows a design pattern called Entity Component System (ECS). In an ECS app, you store additional data on an entity using components and can implement entity behavior by writing systems that use the data from those components. You can add and configure components to entities in Reality Composer Pro, including both shipped components like PhysicsBodyComponent, and custom components that you write and place in the Sources folder of your Reality Composer Pro Swift package. You can even create new components in Reality Composer Pro and then edit them in Xcode. For more information about ECS, see Understanding RealityKit’s modular architecture. Diorama uses custom components to identify which transforms are points of interest, to mark the birds so the app can make sure they flock together, and to control the opacity of entities that are specific to just one of the two maps. To add a component to an entity, select that entity in the hierarchy view or 3D view. At the bottom right of the inspector window, click on the Add Component button. A list of available components appears and the first item in that list is New Component. This item creates a new component class, and optionally a new system class, and adds the component to the selected entity. If you look at the list of components, you see the PointOfInterestComponent that Diorama uses to indicate which transforms are points of interest. If the selected entity doesn’t already contain a PointOfInterestComponent, selecting that adds it to the selected entity. Each entity can only have one component of a specific type. You can edit the values of the existing component in the inspector, which changes what shows up when you tap that point of interest in the app.  In Reality Composer Pro, a transform is an empty entity that marks a point in space. A transform contains a location, rotation, and scale, and its child entities inherit those. But, transforms have no visual representation and do nothing by themselves. Use transforms to mark locations in your scene or organize your entity hierarchy. For example, you might make several entities that need to move together into child entities of the same transform, so you can move them together by moving the parent transform. Diorama uses transforms with a PointOfInterestComponent to indicate points of interest on the map. When the app runs, those transforms mark the location of the floating placards with the name of the location. Tapping on a placard expands it to show more detailed information. To turn transforms into an interactive view, the app looks for a specific component on transforms called a PointOfInterestComponent. Because a transform contains no data other than location, orientation, and scale, it uses this component to hold the data the app needs to display on the placards. If you open the DioramaAssembled scene in Reality Composer Pro and click on the transform called Cathedral_Rocks, you see the PointOfInterestComponent in the inspector.  To load a Reality Composer Pro scene, use load(named:in:), passing the name of the scene you want to load and the project’s bundle. Reality Composer Pro Swift packages define a constant that provides ready access to its bundle. The constant is the name of the Reality Composer Pro project with “Bundle” appended to the end. In this case, the project is called RealityKitContent, so the constant is called RealityKitContentBundle. Here’s how Diorama loads the map table in the RealityView initializer: The load(named:in:) function is asynchronous when called from an asynchronous context. Because the content closure of the RealityView initializer is asynchronous, it automatically uses the async version to load the scene. Note that when using it asynchronously, you must call it using the await keyword. Diorama adds a PointOfInterestComponent to a transform to display details about interesting places. Every point of interest’s name appears in a view that floats above its location on the map. When you tap the floating view, it expands to show detailed information, which the app pulls from the PointOfInterestComponent. The app shows these details by creating a SwiftUI view for each point of interest and querying for all entities that have a PointOfInterestComponent using this query declared in ImmersiveView.swift: In the RealityView initializer, Diorama queries to retrieve the points of interest entities and passes them to a function called createLearnMoreView(for:), which creates the view and saves it for display when it’s tapped. Diorama displays the information added to a PointOfInterestComponent in a LearnMoreView, which it stores as an attachment. Attachments are SwiftUI views that are also RealityKit entities and that you can place into a RealityKit scene at a specific location. Diorama uses attachments to position the view that floats above each point of interest. The app first checks to see if the entity has a component called PointOfInterestRuntimeComponent. If it doesn’t, it creates a new one and adds it to the entity. This new component contains a value you only use at runtime that you don’t need to edit in Reality Composer Pro. By putting this value into a separate component and adding it to entities at runtime, Reality Composer Pro never displays it in the inspector. The PointOfInterestRuntimeComponent stores an identifier called an attachment tag, which uniquely identifies an attachment so the app can retrieve and display it at the appropriate time. Next, Diorama creates a SwiftUI view called a LearnMoreView with the information from the PointOfInterestComponent, tags that view, and stores the tag in the PointOfInterestRuntimeComponent. Finally, it stores the view in an AttachmentProvider, which is a custom class that maintains references to the attachment views so they don’t get deallocated when they’re not in a scene. Assigning a view to an attachment provider doesn’t actually display that view in the scene. The initializer for RealityView has an optional view builder called attachments that’s used to specify the attachments. In the update closure of the initializer, which RealityKit calls when the contents of the view change, the app queries for entities with a PointOfInterestRuntimeComponent, uses the tag from that component to retrieve the correct attachment for it, and then adds that attachment and places it above its location on the map. To switch between the two different topographical maps, Diorama shows a slider that morphs the map between the two locations. To accomplish this, and to draw elevation lines on the map, the FlatTerrain entity in the DioramaAssembled scene uses a Shader Graph material. Shader Graph is a node-based material editor that’s built into Reality Composer Pro. Shader Graph gives you the ability to create dynamic materials that you can change at runtime. Prior to Reality Composer Pro, the only way to implement a dynamic material like this was to create a CustomMaterial and write Metal shaders to implement the necessary logic. Diorama’s DynamicTerrainMaterialEnhanced does two things. It draws contour lines on the map based on height data stored in displacement map images, and it also offsets the vertices of the flat disk based on the same data. By interpolating between two different height maps, the app achieves a smooth transition between the two different sets of height data. When you build Shader Graph materials, you can give them input parameters called promoted inputs that you set from Swift code. This allows you to implement logic that previously required writing a Metal shader. The materials you build in the editor can affect both the look of an entity using the custom surface output node, which equates to writing Metal code in a fragment shader, or the position of vertices using the geometry modifier output, which equates to Metal code running in a vertex shader.  Node graphs can contain subgraphs, which are similar to functions. They contain reusable sets of nodes with inputs and outputs. Subgraphs contain the logic to draw the contour lines and the logic to offset the vertices. Double-click a subgraph to edit it. For more information about building materials using Shader Graph, see Explore Materials in Reality Composer Pro. To change the map, DynamicTerrainMaterialEnhanced has a promoted input called Progress. If that parameter is set to 1.0, it displays Catalina Island. If it’s set to 0, it displays Yosemite. Any other number shows a state in transition between the two. When someone manipulates the slider, the app updates that input parameter based on the slider’s value. Important Shader Graph material parameters are case-sensitive. If the capitalization is wrong, your code won’t actually update the material. The app sets the value of the input parameter in a function called handleMaterial() that the slider’s .onChanged closure calls. That function retrieves the ShaderGraphMaterial from the terrain entity and calls setParameter(name:value:) on it.       .rkassets .usda RealityView .usda PhysicsBodyComponent PointOfInterestComponent PointOfInterestComponent PointOfInterestComponent PointOfInterestComponent DioramaAssembled Cathedral_Rocks PointOfInterestComponent load(named:in:) RealityKitContent RealityKitContentBundle RealityView let entity = try await Entity.load(named: ""DioramaAssembled"", 
                                   in: RealityKitContent.RealityKitContentBundle) load(named:in:) RealityView async await PointOfInterestComponent PointOfInterestComponent PointOfInterestComponent ImmersiveView.swift static let markersQuery = EntityQuery(where: .has(PointOfInterestComponent.self)) RealityView createLearnMoreView(for:) subscriptions.append(content.subscribe(to: ComponentEvents.DidAdd.self, componentType: PointOfInterestComponent.self, { event in
    createLearnMoreView(for: event.entity)
})) PointOfInterestComponent LearnMoreView PointOfInterestRuntimeComponent PointOfInterestRuntimeComponent struct PointOfInterestRuntimeComponent: Component {
    let attachmentTag: ObjectIdentifier
} LearnMoreView PointOfInterestComponent PointOfInterestRuntimeComponent AttachmentProvider let tag: ObjectIdentifier = entity.id


let view = LearnMoreView(name: pointOfInterest.name,
                         description: pointOfInterest.description ?? """",
                         imageNames: pointOfInterest.imageNames,
                         trail: trailEntity,
                         viewModel: viewModel)
    .tag(tag)
entity.components[PointOfInterestRuntimeComponent.self] = PointOfInterestRuntimeComponent(attachmentTag: tag)


attachmentsProvider.attachments[tag] = AnyView(view) RealityView attachments ForEach(attachmentsProvider.sortedTagViewPairs, id: \.tag) { pair in
    pair.view
} update PointOfInterestRuntimeComponent viewModel.rootEntity?.scene?.performQuery(Self.runtimeQuery).forEach { entity in


    guard let attachmentEntity = attachments.entity(for: component.attachmentTag) else { return }
    
    if let pointOfInterestComponent = entity.components[PointOfInterestComponent.self] {
        attachmentEntity.components.set(RegionSpecificComponent(region: pointOfInterestComponent.region))
        attachmentEntity.components.set(OpacityComponent(opacity: 0))
    }
    
    viewModel.rootEntity?.addChild(attachmentEntity)
    attachmentEntity.setPosition([0, 0.2, 0], relativeTo: entity)
} FlatTerrain DioramaAssembled CustomMaterial DynamicTerrainMaterialEnhanced DynamicTerrainMaterialEnhanced Progress 1.0 0 handleMaterial() .onChanged ShaderGraphMaterial setParameter(name:value:) private func handleMaterial() {
    guard let terrain = viewModel.rootEntity?.terrain,
            let terrainMaterial = terrainMaterial else { return }
    do {
        var material = terrainMaterial
        try material.setParameter(name: materialParameterName, value: .float(viewModel.sliderValue))
        
        if var component = terrain.modelComponent {
            component.materials = [material]
            terrain.components.set(component)
        }
        
        try terrain.update(shaderGraphMaterial: terrainMaterial, { m in
            try m.setParameter(name: materialParameterName, value: .float(viewModel.sliderValue))
        })
    } catch {
        print(""problem: \(error)"")
    }
} "
39,"Happy Beam  Overview Design the game interface in SwiftUI Detect a heart gesture with ARKit Support several kinds of input Display 3D content with RealityKit Add SharePlay support for multiplayer gaming experiences See Also       Related samples Related articles Related videos                                In visionOS, you can create fun, dynamic games and apps using several different frameworks to create new kinds of spatial experiences: RealityKit, ARKit, SwiftUI, and Group Activities. This sample introduces Happy Beam, a game where you and your friends can hop on a FaceTime call and play together. You’ll learn the mechanics of the game where grumpy clouds float around in the space, and people play by making a heart shape with their hands to project a beam. People aim the beam at the clouds to cheer them up, and a score counter keeps track of how well each player does cheering up the clouds. Most apps in visionOS launch as a window that opens different scene types depending on the needs of the app. Here you see how Happy Beam presents a fun interface to people by using several SwiftUI views that display a welcome screen, a coaching screen that gives instructions, a scoreboard, and a game-ending screen.     The following shows you the primary view in the app that displays each phase of gameplay: When 3D content starts to appear, the game opens an immersive space to present content outside of the main window and in a person’s surroundings. The HappyBeam container view declares a dependency on openImmersiveSpace: It later uses that dependency to open the space from the app’s declaration when it’s time to start showing 3D content: The Happy Beam app recognizes the central heart-shaped hands gesture using ARKit’s support for 3D hand tracking in visionOS. Using hand tracking requires a running session and authorization from the wearer. It uses the NSHandsTrackingUsageDescription user info key to explain to players why the app requests permission for hand tracking.  Hand-tracking data isn’t available when your app is only displaying a window or volume. Instead, it’s available when you present an immersive space, as in the previous example. You can detect gestures using ARKit data with a level of accuracy that depends on your use case and intended experience. For example, Happy Beam could require strict positioning of finger joints to closely resemble a heart shape. Instead, however, it prompts people to make a heart shape and uses a heuristic to indicate when the gesture is close enough. The following checks whether a person’s thumbs and index fingers are almost touching: To support accessibility features and general user preferences, include multiple kinds of input in an app that uses hand tracking as one form of input. Happy Beam supports several kinds of input: Interactive hands input from ARKit with the custom heart gesture. Drag gesture input to rotate the stationary beam on its platform. Accessibility components from RealityKit to support custom actions for cheering up the clouds. Game Controller support to make control over the beam more interactive from Switch Control. The 3D content in the app comes in the form of assets that you can export from Reality Composer Pro. You place each asset in the RealityView that represents your immersive space. The following shows how Happy Beam generates clouds when the game starts, as well as materials for the floor-based beam projector. Because the game uses collision detection to keep score — the beam cheers up grumpy clouds when they collide — you make collision shapes for each model that might be involved. You use the Group Activities framework in visionOS to support interactive experiences in a shared activity during a FaceTime call. Happy Beam uses Group Activities to sync the score, active players list, and the position of each player’s projected beam. Use a reliable channel to send information that’s important to be correct, even if it can be slightly delayed as a result. The following shows how Happy Beam updates the game model’s score state in response to a score message: Use an unreliable messenger for sending data with low-latency requirements. Because the delivery mode is unreliable, some messages might not make it. Happy Beam uses the unreliable mode to send live updates to the position of the beam when each participant in the call chooses the Spatial option in FaceTime. The following shows how Happy Beam serializes beam data for each message:        struct HappyBeam: View {
    @Environment(\.openImmersiveSpace) private var openImmersiveSpace
    @EnvironmentObject var gameModel: GameModel
    
    @State var session: GroupSession<HeartProjection>? = nil
    @State var timer = Timer.publish(every: 1, on: .main, in: .common).autoconnect()
    @State var subscriptions = Set<AnyCancellable>()
    
    var body: some View {
        let gameState = GameScreen.from(state: gameModel)
        VStack {
            Spacer()
            Group {
                switch gameState {
                case .start:
                    Start()
                case .soloPlay:
                    SoloPlay()
                case .lobby:
                    Lobby()
                case .soloScore:
                    SoloScore()
                case .multiPlay:
                    MultiPlay()
                case .multiScore:
                    MultiScore()
                }
            }
            .glassBackgroundEffect(
                in: RoundedRectangle(
                    cornerRadius: 32,
                    style: .continuous
                )
            )
        }
    }
} @main
struct HappyBeamApp: App {
    @StateObject var gameModel = GameModel()
    @State var immersionState: ImmersionStyle = .mixed
    
    var body: some SwiftUI.Scene {
        WindowGroup(""HappyBeam"", id: ""happyBeamApp"") {
            HappyBeam()
                .environmentObject(gameModel)
        }
        .windowStyle(.plain)
        
        ImmersiveSpace(id: ""happyBeam"") {
            HappyBeamSpace(gestureModel: HeartGestureModelContainer.heartGestureModel)
                .environmentObject(gameModel)
        }
        .immersionStyle(selection: $immersionState, in: .mixed)
    }
} HappyBeam openImmersiveSpace @Environment(\.openImmersiveSpace) private var openImmersiveSpace if gameModel.countDown == 0 {
    Task {
        await openImmersiveSpace(id: ""happyBeam"")
    }
} NSHandsTrackingUsageDescription Task {
    do {
        try await session.run([handTrackingProvider])
    } catch {
        print(""ARKitSession error:"", error)
    }
} func computeTransformOfUserPerformedHeartGesture() -> simd_float4x4? {
    // Get the latest hand anchors and return false if either of them isn't tracked.
    guard let leftHandAnchor = latestHandTracking.left,
          let rightHandAnchor = latestHandTracking.right,
          leftHandAnchor.isTracked, rightHandAnchor.isTracked else {
        return nil
    }
    
    // Get all the required joints and check whether they're tracked.
    let leftHandThumbKnuckle = leftHandAnchor.skeleton.joint(named: .handThumbKnuckle)
    let leftHandThumbTipPosition = leftHandAnchor.skeleton.joint(named: .handThumbTip)
    let leftHandIndexFingerTip = leftHandAnchor.skeleton.joint(named: .handIndexFingerTip)
    let rightHandThumbKnuckle = rightHandAnchor.skeleton.joint(named: .handThumbKnuckle)
    let rightHandThumbTipPosition = rightHandAnchor.skeleton.joint(named: .handThumbTip)
    let rightHandIndexFingerTip = rightHandAnchor.skeleton.joint(named: .handIndexFingerTip)
    
    guard leftHandIndexFingerTip.isTracked && leftHandThumbTipPosition.isTracked &&
            rightHandIndexFingerTip.isTracked && rightHandThumbTipPosition.isTracked &&
            leftHandThumbKnuckle.isTracked &&  rightHandThumbKnuckle.isTracked else {
        return nil
    }
    
    // Get the position of all joints in world coordinates.
    let leftHandThumbKnuckleWorldPosition = matrix_multiply(leftHandAnchor.transform, leftHandThumbKnuckle.rootTransform).columns.3.xyz
    let leftHandThumbTipWorldPosition = matrix_multiply(leftHandAnchor.transform, leftHandThumbTipPosition.rootTransform).columns.3.xyz
    let leftHandIndexFingerTipWorldPosition = matrix_multiply(leftHandAnchor.transform, leftHandIndexFingerTip.rootTransform).columns.3.xyz
    let rightHandThumbKnuckleWorldPosition = matrix_multiply(rightHandAnchor.transform, rightHandThumbKnuckle.rootTransform).columns.3.xyz
    let rightHandThumbTipWorldPosition = matrix_multiply(rightHandAnchor.transform, rightHandThumbTipPosition.rootTransform).columns.3.xyz
    let rightHandIndexFingerTipWorldPosition = matrix_multiply(rightHandAnchor.transform, rightHandIndexFingerTip.rootTransform).columns.3.xyz
    
    let indexFingersDistance = distance(leftHandIndexFingerTipWorldPosition, rightHandIndexFingerTipWorldPosition)
    let thumbsDistance = distance(leftHandThumbTipWorldPosition, rightHandThumbTipWorldPosition)
    
    // Heart gesture detection is true when the distance between the index fingertips' centers
    // and the distance between the thumb tips' centers is each less than four centimeters.
    let isHeartShapeGesture = indexFingersDistance < 0.04 && thumbsDistance < 0.04
    if !isHeartShapeGesture {
        return nil
    }
    
    // Computes a position in the middle of the heart gesture.
    let halfway = (rightHandIndexFingerTipWorldPosition - leftHandThumbTipWorldPosition)/2
    let heartMidpoint = rightHandIndexFingerTipWorldPosition - halfway
    
    // Computes the vector from the left thumb knuckle to the right thumb knuckle and normalizes (x-axis).
    let xAxis = normalize(rightHandThumbKnuckleWorldPosition - leftHandThumbKnuckleWorldPosition)
    
    // Computes the vector from the right thumb tip to the right index fingertip and normalizes (y-axis).
    let yAxis = normalize(rightHandIndexFingerTipWorldPosition - rightHandThumbTipWorldPosition)
    
    let zAxis = normalize(cross(xAxis, yAxis))
    
    // Creates the final transform for the heart gesture from the three axes and midpoint vector.
    let heartMidpointWorldTransform = simd_matrix(SIMD4(xAxis.x, xAxis.y, xAxis.z, 0), SIMD4(yAxis.x, yAxis.y, yAxis.z, 0), SIMD4(zAxis.x, zAxis.y, zAxis.z, 0), SIMD4(heartMidpoint.x, heartMidpoint.y, heartMidpoint.z, 1))
    return heartMidpointWorldTransform
} RealityView @MainActor
func placeCloud(start: Point3D, end: Point3D, speed: Double) async throws -> Entity {
    let cloud = await loadFromRealityComposerPro(
        named: BundleAssets.cloudEntity,
        fromSceneNamed: BundleAssets.cloudScene
    )!
        .clone(recursive: true)
    
    cloud.generateCollisionShapes(recursive: true)
    cloud.components[PhysicsBodyComponent.self] = PhysicsBodyComponent()
    
    var accessibilityComponent = AccessibilityComponent()
    accessibilityComponent.label = ""Cloud""
    accessibilityComponent.value = ""Grumpy""
    accessibilityComponent.isAccessibilityElement = true
    accessibilityComponent.traits = [.button, .playsSound]
    accessibilityComponent.systemActions = [.activate]
    cloud.components[AccessibilityComponent.self] = accessibilityComponent
    
    let animation = cloudMovementAnimations[cloudPathsIndex]
    
    cloud.playAnimation(animation, transitionDuration: 1.0, startsPaused: false)
    cloudAnimate(cloud, kind: .sadBlink, shouldRepeat: false)
    spaceOrigin.addChild(cloud)
    
    return cloud
} sessionInfo.reliableMessenger = GroupSessionMessenger(session: newSession, deliveryMode: .reliable)


Task {
    for await (message, sender) in sessionInfo!.reliableMessenger!.messages(of: ScoreMessage.self) {
        gameModel.clouds[message.cloudID].isHappy = true
        gameModel
            .players
            .filter { $0.name == sender.source.id.asPlayerName }
            .first!
            .score += 1
    }
} sessionInfo.messenger = GroupSessionMessenger(session: newSession, deliveryMode: .unreliable) // Send each player's beam data during FaceTime calls where players have selected the Spatial option.
func sendBeamPositionUpdate(_ pose: Pose3D) {
    if let sessionInfo = sessionInfo, let session = sessionInfo.session, let messenger = sessionInfo.messenger {
        let everyoneElse = session.activeParticipants.subtracting([session.localParticipant])
        
        if isShowingBeam, gameModel.isSpatial {
            messenger.send(BeamMessage(pose: pose), to: .only(everyoneElse)) { error in
                if let error = error { print(""Message failure:"", error) }
            }
        }
    }
} "
